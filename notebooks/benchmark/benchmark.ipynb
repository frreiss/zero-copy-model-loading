{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8270a16d",
   "metadata": {},
   "source": [
    "# benchmark.ipynb\n",
    "\n",
    "This notebook contains the text and code for the next blog post in the zero-copy model series, \n",
    "title TBD.\n",
    "\n",
    "The first post explained how to load PyTorch models for inference extremely fast by leveraging the Plasma object store's ability to load numeric data directly from shared memory.\n",
    "\n",
    "In this post, we talk in more concrete terms about how to use this zero-copy model loading for model serving. We put together a simple model serving system, then set up a microbenchmark that simulates a heavy-tailed traffic pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbf7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import zerocopy\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69fefd",
   "metadata": {},
   "source": [
    "# Title of new blog post goes here\n",
    "\n",
    "*Recap of previous blog post goes here.*\n",
    "\n",
    "In a [previous post](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c), we introduced the concept of *zero-copy model loading*. Zero-copy model loading involves keeping the weights of a deep learning model in shared memory, so that different processes can \"load\" the model for inference without \n",
    "\n",
    "showed that \n",
    "The Plasma object store integrated into Ray makes it easy to do zero-copy model loading\n",
    "\n",
    "and that implementing this technique on Ray can accelerate model loading by several orders of magnitude.\n",
    "\n",
    "If you'd like to find out more about these\n",
    "\n",
    "follow [this link](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c) to view the previous post.\n",
    "\n",
    "\n",
    "Today's post goes into the details of how to use zero-copy model loading for production deployments of large natural language processing (NLP) models. We introduce `zerocopy`, a Python package that makes it extra simple to apply zero-copy model loading to PyTorch models. We show how easy it is to deploy modern NLP models using with `zerocopy` and Ray Serve. Finally, we present an end-to-end model serving benchmark that shows how we can serve 12 state-of-the-art NLP models with a single cloud VM and achieve (**TODO: final numbers**)x better scalability.\n",
    "  \n",
    "But before we get into these details, we need to give some background about the end-to-end scenario we'll be targeting with our benchmark and the NLP models that use to cover this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eccd9",
   "metadata": {},
   "source": [
    "## The Scenario\n",
    "\n",
    "The end-to-end scenario for our benchmark involves supporting an AI chatbot.\n",
    "The chatbot's conversational AI runs off of a conversation tree. Some of the \n",
    "nodes of this tree invoke models.\n",
    "\n",
    "\n",
    "> **TODO:** Cartoon block diagram of the end-to-end scenario. \n",
    "> Diagram should show a user interacting with a chatbot. The chatbot runs off of a conversation tree. \n",
    "> Some of the nodes of the conversation tree have question answering models hanging off of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc5ddc-35bc-4c70-b3c8-c9e0debbc8bb",
   "metadata": {},
   "source": [
    "Our benchmark will cover the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs four different types of models:\n",
    "* *Intent detection* models that determine what is the user's goal.\n",
    "* *Sentiment analysis* models that monitor the user's mood.\n",
    "* *Question answering* models that provide the answers to specific factual questions.\n",
    "* *Natural language generation* models that give the chatbot's responses a less scripted flavor.\n",
    "\n",
    "Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models.\n",
    "\n",
    "In a real application, you would want to train custom versions of each type\n",
    "of model for the topics your chatbot covers.\n",
    "Since we're only interested in modeling throughput and latency, we skipped that customization\n",
    "step and just used the most popular pretrained model from each category from the \n",
    "[Huggingface model marketplace](https://huggingface.co/models).\n",
    "\n",
    "Each of these models uses a [Transformer](https://arxiv.org/abs/1706.03762)-based neural network,\n",
    "with a *language model* and a task specific *head*, tuned over \n",
    "a domain-specific training set.  The table below summarizes the four models that we used.\n",
    "\n",
    "\n",
    "| Task                 | Model Name                                   | Language Model  |  Pre/post Processing\n",
    "| -----------          | -----------                                  | ------------    | ---------------\n",
    "| Intent Detection     | `mrm8488/t5-base-finetuned-e2m-intent`       | T5              | Reference code\n",
    "| Sentiment Analysis   | `cardiffnlp/twitter-roberta-base-sentiment`  | RoBERTa         | Reference code\n",
    "| Question Answering   | `deepset/roberta-base-squad2`                | RoBERTa         | Pipeline\n",
    "| Text Generation      | `gpt2`                                       | GPT-2           | Pipeline\n",
    "\n",
    "\n",
    "Although all four models came from the same marketplace, they are quite diverse. The models use three different core language models: [Text-to-Text Transfer Transformer](https://arxiv.org/pdf/1910.10683.pdf) (T5) from Google Research, \n",
    "[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) from Facebook AI, and [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) from OpenAI. \n",
    "\n",
    "NLP models generally require *preprocessing* to convert natural language text into a format amenable to \n",
    "model inference and *postprocessing* to convert the answer into a format that a person can understand. All four of our models come with code for these crucial steps, but models use two very different ways to package this code. The intent and sentiment models provide small blocks of reference Python code, with the intent being that the user will adapt this reference code to the specific circumstances of the end-to-end appliction.\n",
    "\n",
    "For example, the code block below loads and runs the intent model using code adapted from the provided example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55f2dd0-169e-461e-b4c0-1b5c532214df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to eat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "\n",
    "# Load model and tokenizer\n",
    "intent_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "intent_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    INTENT_MODEL_NAME)\n",
    "\n",
    "# Example input\n",
    "INTENT_INPUT = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and beat you up, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference\n",
    "output = intent_model.generate(**features)\n",
    "\n",
    "# Postprocessing\n",
    "result_string = intent_tokenizer.decode(output[0])\n",
    "result_string = result_string.replace('<pad>', '')\n",
    "result_string = result_string[len(' '):-len('</s>')]\n",
    "\n",
    "result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb04ad3-dee3-4578-9e12-6a850f734452",
   "metadata": {},
   "source": [
    "The question answering and text generation models both use the Huggingface `tokenizers` library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines) to package their preprocessing and postprocesing code.\n",
    "The following code snippet demonstrates the process of loading and running the question answering model using this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17af4998-357f-420c-ab1a-61b82cfef905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "\n",
    "# Loading the model and associated resources\n",
    "qa_pipeline = transformers.pipeline('question-answering',\n",
    "                                    model=QA_MODEL_NAME)\n",
    "\n",
    "# Example input\n",
    "# TODO: For the blog post, truncate the context string.\n",
    "QA_INPUT = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "# Preprocessing, inference, and postprocessing all happen in\n",
    "# the Python object's the __call__() method.\n",
    "qa_result = qa_pipeline(**QA_INPUT)\n",
    "\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1dca8a-a2ac-4d5f-b535-8aac6c0ca2c1",
   "metadata": {},
   "source": [
    "Unlike the \"example reference code\" approach, the Pipelines API's end-to-end inference code is intended for direct production use. It includes support for model retraining, as well as performance optimizations like batching and GPU acceleration, plus code for handling corner cases like long input strings. This prepackaged code can save a lot of time, provided that your application is structured in a way that can easily accomodate a large block of non-modifiable third-party Python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab98d8e-40a0-4c1b-a994-4fdb88756185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introducing `zerocopy`\n",
    "\n",
    "We've created a Python package, `zerocopy`, with the model rewrite code from our previous post (TODO: Publish the package to PyPI).\n",
    "\n",
    "To use that package, you'll need to install it with `pip`, then import it into your script.\n",
    "\n",
    "```python\n",
    "import zerocopy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82b4a9-5d9a-4b35-b818-627f2e7ea4fb",
   "metadata": {},
   "source": [
    "(TODO: Insert description of how `zero_copy` strips off the weights of a model\n",
    "and provides a way to reconsitute them from Plasma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a5406-f37d-4848-a0ca-27cd83360ff8",
   "metadata": {},
   "source": [
    "The low-level process works for all of our example models. Here it is in action with the intent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7add2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 11:14:33,034\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=6014)\u001b[0m 2022-03-02 11:14:37,418\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=6014)\u001b[0m 2022-03-02 11:14:37,527\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:PqJsBM:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-02 11:14:37,966\tINFO api.py:475 -- Started Serve instance in namespace '920c26c7-98b1-420a-91f5-c65831bfef20'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6015)\u001b[0m INFO:     Started server process [6015]\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog post.\n",
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()\n",
    "\n",
    "# Wait a moment to make sure that all log output goes to this cell\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ee503d-812c-4f37-bc1c-f677f15133d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result without zero-copy loading: tensor([[   0,   12,    3, 1544,    1]])\n",
      " Result *with* zero-copy loading: tensor([[   0,   12,    3, 1544,    1]])\n"
     ]
    }
   ],
   "source": [
    "# Recall that we loaded `intent_model` in an earlier cell.\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference without zero-copy loading\n",
    "print('Result without zero-copy loading: '\n",
    "      + str(intent_model.generate(**features)))\n",
    "\n",
    "# Inference with zero-copy loading\n",
    "intent_model_ref = ray.put(zerocopy.extract_tensors(intent_model))\n",
    "print(' Result *with* zero-copy loading: ' +\n",
    "      str(ray.get(zerocopy.call_model.remote(\n",
    "          intent_model_ref, [], features, 'generate'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b99817-8c69-4059-a4ee-743090798eab",
   "metadata": {
    "tags": []
   },
   "source": [
    "The time to invoke the rewritten model once is almost the same as running the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36429095-4c64-42bb-af7c-f58b5939a2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time to run locally: 275 ms ± 5.15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Time to run with zero-copy: 280 ms ± 3.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit intent_model.generate(**features)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(intent_model_ref, [], features, 'generate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815d4b4-a22f-41b0-85db-4564c7531272",
   "metadata": {},
   "source": [
    "If we run inference multiple times, `zero_copy.call_model()` can send those inference requests to separate Ray tasks that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fec81f-f1c0-434e-abb7-2f4aacf5449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 50 times with zero-copy: 6.28 s ± 99.3 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "       Time to run 50 times locally: 13.9 s ± 215 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        intent_model.generate(**features)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [\n",
    "        zerocopy.call_model.remote(intent_model_ref, [], features, 'generate')\n",
    "        for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 50\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a09990-4188-47af-9c70-a57069cc7297",
   "metadata": {
    "tags": []
   },
   "source": [
    "The question answering and text generation models in our benchmark come packaged as `transformers` model pipelines. For convenience, the `zerocopy` library includes a function `rewrite_pipeline` that transforms any models embedded into Python object into Ray tasks that use zero-copy model loading to load weights. If we apply this function to a pipeline, the resulting rewritten pipeline faithfully performs all the preprocessing and postprocessing that the original pipeline performed. However, this rewritten pipeline runs the embedded PyTorch model in remote Ray tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10fd36e6-347b-422c-b558-bea0a0e9c5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before rewrite: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      " After rewrite: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n"
     ]
    }
   ],
   "source": [
    "zero_copy_qa = zerocopy.rewrite_pipeline(qa_pipeline)\n",
    "print(f\"Before rewrite: {qa_pipeline(**QA_INPUT)}\")\n",
    "print(f\" After rewrite: {zero_copy_qa(**QA_INPUT)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38195230-a2d2-4d77-8d41-090ecceef5c1",
   "metadata": {},
   "source": [
    "## Deploying Models with `zerocopy` and Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312b6318-ee14-4eef-b576-de4efe4f78de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 11:16:51,788\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=6116)\u001b[0m 2022-03-02 11:16:55,988\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=6116)\u001b[0m 2022-03-02 11:16:56,099\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:KRWRcF:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-02 11:16:56,513\tINFO api.py:475 -- Started Serve instance in namespace '49755261-3f6f-4b7b-80b7-96cf71f63cd3'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fd5d1c56670>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a7ef8-cda9-4ac2-bf97-b1d69985948d",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a379ad1-4902-4a33-8136-5343fccea287",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class Intent:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    INTENT_MODEL_NAME)\n",
    "        \n",
    "        # Extract weights and load them onto the Plasma object store\n",
    "        self._model_ref = ray.put(zerocopy.extract_tensors(model))\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "        \n",
    "        # Preprocessing\n",
    "        input_text = f'{json_request[\"context\"]} </s>'\n",
    "        features = self._tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        output = await zerocopy.call_model.remote(\n",
    "            self._model_ref, [], features, 'generate')\n",
    "\n",
    "        # Postprocessing\n",
    "        result_string = self._tokenizer.decode(output[0])\n",
    "        result_string = result_string[len('<pad> '):-len('</s>')]\n",
    "        return {\n",
    "            'intent': result_string\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee7aa8-3640-4795-b32d-c7c447014182",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8db55e-0384-4973-bf2d-351e9d61015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class QA:\n",
    "    def __init__(self):\n",
    "        # Load the pipeline and move the model's weights onto the\n",
    "        # Plasma object store.\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            transformers.pipeline('question-answering', \n",
    "                                  model=QA_MODEL_NAME))\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # The original `transformers` code is not async-aware, so we\n",
    "        # call it from `run_in_executor()`\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(**json_request))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cde0b-fbd8-4df5-8bf1-5ea74a8fafe8",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de42944d-e1d0-4c56-bc77-583a2c3137fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 11:16:56,549\tINFO api.py:249 -- Updating deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6118)\u001b[0m INFO:     Started server process [6118]\n",
      "\u001b[2m\u001b[36m(ServeController pid=6116)\u001b[0m 2022-03-02 11:16:56,638\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(intent_en pid=6119)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "2022-03-02 11:17:07,138\tINFO api.py:261 -- Deployment 'intent_en' is ready at `http://127.0.0.1:8000/intent_en`. component=serve deployment=intent_en\n",
      "2022-03-02 11:17:07,149\tINFO api.py:249 -- Updating deployment 'qa_en'. component=serve deployment=qa_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6116)\u001b[0m 2022-03-02 11:17:07,241\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_en'. component=serve deployment=qa_en\n",
      "2022-03-02 11:17:16,808\tINFO api.py:261 -- Deployment 'qa_en' is ready at `http://127.0.0.1:8000/qa_en`. component=serve deployment=qa_en\n"
     ]
    }
   ],
   "source": [
    "Intent.options(name='intent_en', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "QA.options(name='qa_en', ray_actor_options={\"num_cpus\": 0.1}).deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add17b9-3994-4c81-8b4d-9fe16bf80317",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf8d4ef7-4dab-4511-bc77-05231a1725bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat'}\n",
      "Question answering result: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n"
     ]
    }
   ],
   "source": [
    "intent_result = requests.put(\n",
    "    'http://127.0.0.1:8000/intent_en',\n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    'http://127.0.0.1:8000/qa_en',\n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4614552-0203-4ecc-a7c3-986c78c8c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=6116)\u001b[0m 2022-03-02 11:17:35,924\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6116)\u001b[0m 2022-03-02 11:17:35,928\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_en'. component=serve deployment=qa_en\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Stop this notebook's copy of Ray so as not to interfere with the\n",
    "# copy in `ray_deploy.ipynb`\n",
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b1aa",
   "metadata": {},
   "source": [
    "## Baseline implementation\n",
    "\n",
    "Our baseline implementation of the model serving backend for our benchmark emulates running each model in a separate container. We used [TorchServe](https://pytorch.org/serve/) as our model serving framework for the baseline deployment. By configuing TorchServe to use a pool of processes, we were able to simulate running each model in a separate container without having to set up a dedicated Kubernetes cluster. See [this notebook](./torchserve.ipynb) for details of the TorchServe deployment.\n",
    "\n",
    "*Note that earlier versions of this notebook implemented the baseline model deployment with a pool of Ray actors. That older version is preserved in [a separate notebook](./ray_baseline.ipynb).*\n",
    "\n",
    "With TorchServe running in the background, we can invoke our models via their REST APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3025582b-6588-42c8-ba47-545903b3511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'modelName': 'generate_en', 'modelUrl': 'generate.mar'}, {'modelName': 'generate_es', 'modelUrl': 'generate.mar'}, {'modelName': 'generate_zh', 'modelUrl': 'generate.mar'}, {'modelName': 'intent_en', 'modelUrl': 'intent.mar'}, {'modelName': 'intent_es', 'modelUrl': 'intent.mar'}, {'modelName': 'intent_zh', 'modelUrl': 'intent.mar'}, {'modelName': 'qa_en', 'modelUrl': 'qa.mar'}, {'modelName': 'qa_es', 'modelUrl': 'qa.mar'}, {'modelName': 'qa_zh', 'modelUrl': 'qa.mar'}, {'modelName': 'sentiment_en', 'modelUrl': 'sentiment.mar'}, {'modelName': 'sentiment_es', 'modelUrl': 'sentiment.mar'}, {'modelName': 'sentiment_zh', 'modelUrl': 'sentiment.mar'}]}\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the management API to verify that TorchServe is running.\n",
    "try:\n",
    "    print(requests.get('http://127.0.0.1:8081/models').json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Stop notebook execution\n",
    "    raise ValueError('TorchServe does not appear to be running. Please start TorchServe.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c011ecef-438d-458d-a542-90118b7d36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat chips'}\n",
      "Sentiment result: {'positive': 0.5419477820396423, 'neutral': 0.38251084089279175, 'negative': 0.07554134726524353}\n",
      "Question answering result: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      "Natural language generation result: [{'generated_text': \"All your base are ready for those.\\n\\n2. Once you've added the base of each element, click on the first one to display the current layout.\\n\\n3. After clicking Add Add, there will now be an option to display\"}]\n"
     ]
    }
   ],
   "source": [
    "TORCHSERVE_PORT = 8080\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}\n",
    "\n",
    "intent_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/intent_en',\n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/sentiment_en',\n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/qa_en',\n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/generate_en',\n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4316-86af-481f-8cc0-887a88c30c0d",
   "metadata": {},
   "source": [
    "## The Benchmark\n",
    "\n",
    "Now that we have deployed each of our models with a web service front end, we can define a benchmark that sends inference traffic to these web service endpoints and measures response time.\n",
    "\n",
    "We start by wrapping all the web services in a single callback function that calls a model, retrieves the result, verifies the result, and returns elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079ad698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we have a single canned input for each model type.\n",
    "MODEL_INPUTS = {\n",
    "    'intent': INTENT_INPUT,\n",
    "    'sentiment': SENTIMENT_INPUT,\n",
    "    'qa': QA_INPUT,\n",
    "    'generate': GENERATE_INPUT\n",
    "}\n",
    "\n",
    "LANGUAGES = ['en', 'es', 'zh']\n",
    "\n",
    "\n",
    "MODEL_TYPES = list(MODEL_INPUTS.keys())\n",
    "\n",
    "\n",
    "def call_model(model_type: str, language: str, port: int,\n",
    "               timeout_sec: float = 20.0) \\\n",
    "        -> Tuple[int, float, float]:\n",
    "    '''\n",
    "    Callack function that calls the model deployment, retrieves and\n",
    "    validates the result, and returns elapsed time.\n",
    "\n",
    "    :param model_type: Type of model to call; must be one of\n",
    "                       'intent', 'sentiment', 'qa', or 'generate'\n",
    "    :param language: Two-letter language code; must be one of\n",
    "                     'en', 'es', 'zh'\n",
    "    :param port: Port on which the local REST API is listening.\n",
    "    :param timeout_sec: Request timeout, in seconds.\n",
    "\n",
    "    :returns: Tuple of HTTP result code and start and end times \n",
    "              of the web service call. If a client-side timeout\n",
    "              happens, the result code will be 408 (request timeout)\n",
    "    '''\n",
    "    if model_type not in MODEL_TYPES:\n",
    "        raise ValueError(f'Unexpected model type \"{model_type}\" '\n",
    "                         f'(expected {MODEL_TYPES}')\n",
    "    if language not in LANGUAGES:\n",
    "        raise ValueError(f'Unexpected language code \"{language}\" '\n",
    "                         f'(expected {LANGUAGES}')\n",
    "\n",
    "    # For now, use the same input every time\n",
    "    model_input = MODEL_INPUTS[model_type]\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = requests.put(\n",
    "            f'http://127.0.0.1:{port}/predictions/{model_type}_{language}',\n",
    "            json.dumps(model_input),\n",
    "            timeout=timeout_sec)\n",
    "        end_time = time.time()\n",
    "        status_code = result.status_code\n",
    "    except requests.exceptions.Timeout:\n",
    "        end_time = start_time + timeout_sec\n",
    "        status_code = 408  # HTTP/408 Request Timeout\n",
    "\n",
    "    return (status_code, start_time, end_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe47e03-83d6-416a-a71f-92358202e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"intent\" model takes 0.328 seconds.\n",
      "The \"sentiment\" model takes 0.088 seconds.\n",
      "The \"qa\" model takes 0.537 seconds.\n",
      "The \"generate\" model takes 1.840 seconds.\n"
     ]
    }
   ],
   "source": [
    "TORCHSERVE_PORT = 8080\n",
    "\n",
    "# Test with each model type\n",
    "for model_type in MODEL_INPUTS.keys():\n",
    "    times = call_model(model_type, 'en', TORCHSERVE_PORT)\n",
    "    print(f'The \"{model_type}\" model takes {times[2] - times[1]:1.3f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92a1f5-3624-4f22-8a95-0b9d9c2f32d6",
   "metadata": {},
   "source": [
    "Our benchmark generates a trace of requests, then plays back the trace and measures the \n",
    "latency of each request. \n",
    "\n",
    "The request rate changes each second, with the rate of a particular 1-second window drawn from the Poisson\n",
    "distribution. Here's the code to generate the start times for the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "110ace1f-9c9d-4b55-a50b-9c20d2d9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_start_times(requests_per_sec: float, num_sec: int,\n",
    "                    seed: int) -> np.ndarray:\n",
    "    '''\n",
    "    Generate a trace of inference request start times. Divides the trace\n",
    "    into 1-second intervals. Each interval gets a number of requests drawn\n",
    "    from a Poissson distribution. These requests are evenly spread through the\n",
    "    interval.\n",
    "\n",
    "    :param requests_per_sec: Average requests per second overall\n",
    "    :param num_sec: Number of seconds of trace to generate\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Numpy array of timestamps (starting from 0) for the requests\n",
    "     in the trace\n",
    "    '''\n",
    "    trace = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Compute the number of requests in each 1-second window.\n",
    "    req_per_window = rng.poisson(requests_per_sec, size=num_sec)\n",
    "\n",
    "    for window_num in range(num_sec):\n",
    "        num_requests = req_per_window[window_num]\n",
    "        if num_requests > 0:\n",
    "            request_interval = 1.0 / num_requests\n",
    "            for i in range(num_requests):\n",
    "                trace.append(window_num + request_interval * i)\n",
    "\n",
    "    return np.array(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c681f9-e06e-4da8-a6e8-08ae5e54fa85",
   "metadata": {},
   "source": [
    "Each request goes to a randomly-selected model. The choice of models is\n",
    "weighted according to a truncated Poisson distribution. Here's the code to generate\n",
    "the list of model IDs for the requests in the trace. When we play back the trace,\n",
    "we'll map each integer model ID to a combination of a language code and a model type\n",
    "--- for example, `('en', 'sentiment')` for the English sentiment model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "451f1281-6c46-45e9-b397-5d8898717e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en', 'intent'),\n",
       " ('en', 'sentiment'),\n",
       " ('en', 'qa'),\n",
       " ('en', 'generate'),\n",
       " ('es', 'intent'),\n",
       " ('es', 'sentiment'),\n",
       " ('es', 'qa'),\n",
       " ('es', 'generate'),\n",
       " ('zh', 'intent'),\n",
       " ('zh', 'sentiment'),\n",
       " ('zh', 'qa'),\n",
       " ('zh', 'generate')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_model_ids(lambda_: float, num_models: int, num_points: int,\n",
    "                  seed: int) -> np.ndarray:\n",
    "    '''\n",
    "    Draw integer model IDs at random from a truncated Poisson distribution.\n",
    "\n",
    "    :param lambda_: Primary parameter of the distribution, which also happens to \n",
    "     be the mean value of the (untruncated) distribution.\n",
    "    :param num_models: Number of models; generated IDs will range from 0 to\n",
    "                       `num_models - 1`, inclusive.\n",
    "    :param num_points: Number of random model IDs to return.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Randomly generated model IDs for a series of requests, as a\n",
    "     1D Numpy array of integers.\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Draw integers from a truncated Poisson distribution. Start with a \n",
    "    # non-truncated distribution, then resample for\n",
    "    # any values that went over the limit.\n",
    "    int_ids = rng.poisson(lambda_, size=num_points)\n",
    "    while np.any(int_ids >= num_models):\n",
    "        new_values = rng.poisson(lambda_, size=np.sum(int_ids >= num_models))\n",
    "        int_ids[int_ids >= num_models] = new_values\n",
    "    return int_ids\n",
    "\n",
    "\n",
    "# Map the integer model IDs from the trace to pairs of language code and\n",
    "# model type.\n",
    "MODEL_ID_TO_PARAMS = [\n",
    "    (lang_code, model_name)\n",
    "    for lang_code in LANGUAGES\n",
    "    for model_name in MODEL_TYPES\n",
    "]\n",
    "\n",
    "\n",
    "MODEL_ID_TO_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61453629-426e-4b09-8430-4b2c11b71a64",
   "metadata": {},
   "source": [
    "The benchmark itself generates and then plays back the trace, measuring the end-to-end latency of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfa55ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>model_type</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>result_code</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.093103</td>\n",
       "      <td>200</td>\n",
       "      <td>0.089085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125097</td>\n",
       "      <td>0.211028</td>\n",
       "      <td>200</td>\n",
       "      <td>0.085931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250112</td>\n",
       "      <td>0.578213</td>\n",
       "      <td>200</td>\n",
       "      <td>0.328101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.377058</td>\n",
       "      <td>0.903518</td>\n",
       "      <td>200</td>\n",
       "      <td>0.526460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.501596</td>\n",
       "      <td>0.895607</td>\n",
       "      <td>200</td>\n",
       "      <td>0.394011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625474</td>\n",
       "      <td>1.229426</td>\n",
       "      <td>200</td>\n",
       "      <td>0.603952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.755350</td>\n",
       "      <td>1.548097</td>\n",
       "      <td>200</td>\n",
       "      <td>0.792747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.877534</td>\n",
       "      <td>0.965960</td>\n",
       "      <td>200</td>\n",
       "      <td>0.088426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.001118</td>\n",
       "      <td>1.094315</td>\n",
       "      <td>200</td>\n",
       "      <td>0.093197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.126424</td>\n",
       "      <td>1.887494</td>\n",
       "      <td>200</td>\n",
       "      <td>0.761070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.254388</td>\n",
       "      <td>2.215109</td>\n",
       "      <td>200</td>\n",
       "      <td>0.960721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.376975</td>\n",
       "      <td>2.533208</td>\n",
       "      <td>200</td>\n",
       "      <td>1.156233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.501634</td>\n",
       "      <td>1.584714</td>\n",
       "      <td>200</td>\n",
       "      <td>0.083080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.627374</td>\n",
       "      <td>1.712055</td>\n",
       "      <td>200</td>\n",
       "      <td>0.084681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.751333</td>\n",
       "      <td>2.281436</td>\n",
       "      <td>200</td>\n",
       "      <td>0.530103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.876292</td>\n",
       "      <td>2.843521</td>\n",
       "      <td>200</td>\n",
       "      <td>0.967229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.003596</td>\n",
       "      <td>3.191591</td>\n",
       "      <td>200</td>\n",
       "      <td>1.187995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>2.111463</td>\n",
       "      <td>3.503997</td>\n",
       "      <td>200</td>\n",
       "      <td>1.392534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>2.224466</td>\n",
       "      <td>3.822897</td>\n",
       "      <td>200</td>\n",
       "      <td>1.598431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.334976</td>\n",
       "      <td>4.149760</td>\n",
       "      <td>200</td>\n",
       "      <td>1.814784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>2.446154</td>\n",
       "      <td>2.528694</td>\n",
       "      <td>200</td>\n",
       "      <td>0.082540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>2.556066</td>\n",
       "      <td>4.464265</td>\n",
       "      <td>200</td>\n",
       "      <td>1.908199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.671780</td>\n",
       "      <td>4.776895</td>\n",
       "      <td>200</td>\n",
       "      <td>2.105115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>2.778353</td>\n",
       "      <td>5.100220</td>\n",
       "      <td>200</td>\n",
       "      <td>2.321867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>2.890876</td>\n",
       "      <td>5.424154</td>\n",
       "      <td>200</td>\n",
       "      <td>2.533278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.004255</td>\n",
       "      <td>5.730242</td>\n",
       "      <td>200</td>\n",
       "      <td>2.725987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>3.144704</td>\n",
       "      <td>6.047008</td>\n",
       "      <td>200</td>\n",
       "      <td>2.902304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>3.288920</td>\n",
       "      <td>6.381157</td>\n",
       "      <td>200</td>\n",
       "      <td>3.092237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>3.432689</td>\n",
       "      <td>6.702036</td>\n",
       "      <td>200</td>\n",
       "      <td>3.269347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>3.574686</td>\n",
       "      <td>7.037221</td>\n",
       "      <td>200</td>\n",
       "      <td>3.462535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.715971</td>\n",
       "      <td>3.798963</td>\n",
       "      <td>200</td>\n",
       "      <td>0.082992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857562</td>\n",
       "      <td>7.373729</td>\n",
       "      <td>200</td>\n",
       "      <td>3.516167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.001290</td>\n",
       "      <td>4.093341</td>\n",
       "      <td>200</td>\n",
       "      <td>0.092051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.167401</td>\n",
       "      <td>7.684968</td>\n",
       "      <td>200</td>\n",
       "      <td>3.517567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.334398</td>\n",
       "      <td>8.004573</td>\n",
       "      <td>200</td>\n",
       "      <td>3.670175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500392</td>\n",
       "      <td>8.317927</td>\n",
       "      <td>200</td>\n",
       "      <td>3.817535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.668474</td>\n",
       "      <td>8.633385</td>\n",
       "      <td>200</td>\n",
       "      <td>3.964911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>4.835481</td>\n",
       "      <td>8.942523</td>\n",
       "      <td>200</td>\n",
       "      <td>4.107042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    request_id  model_num lang_code model_type  desired_start  actual_start  \\\n",
       "0            0          1        en  sentiment       0.000000      0.004018   \n",
       "1            1          1        en  sentiment       0.125000      0.125097   \n",
       "2            2          0        en     intent       0.250000      0.250112   \n",
       "3            3          2        en         qa       0.375000      0.377058   \n",
       "4            4          0        en     intent       0.500000      0.501596   \n",
       "5            5          0        en     intent       0.625000      0.625474   \n",
       "6            6          0        en     intent       0.750000      0.755350   \n",
       "7            7          1        en  sentiment       0.875000      0.877534   \n",
       "8            8          1        en  sentiment       1.000000      1.001118   \n",
       "9            9          0        en     intent       1.125000      1.126424   \n",
       "10          10          0        en     intent       1.250000      1.254388   \n",
       "11          11          0        en     intent       1.375000      1.376975   \n",
       "12          12          1        en  sentiment       1.500000      1.501634   \n",
       "13          13          1        en  sentiment       1.625000      1.627374   \n",
       "14          14          2        en         qa       1.750000      1.751333   \n",
       "15          15          0        en     intent       1.875000      1.876292   \n",
       "16          16          0        en     intent       2.000000      2.003596   \n",
       "17          17          0        en     intent       2.111111      2.111463   \n",
       "18          18          0        en     intent       2.222222      2.224466   \n",
       "19          19          0        en     intent       2.333333      2.334976   \n",
       "20          20          1        en  sentiment       2.444444      2.446154   \n",
       "21          21          0        en     intent       2.555556      2.556066   \n",
       "22          22          0        en     intent       2.666667      2.671780   \n",
       "23          23          0        en     intent       2.777778      2.778353   \n",
       "24          24          0        en     intent       2.888889      2.890876   \n",
       "25          25          0        en     intent       3.000000      3.004255   \n",
       "26          26          0        en     intent       3.142857      3.144704   \n",
       "27          27          0        en     intent       3.285714      3.288920   \n",
       "28          28          0        en     intent       3.428571      3.432689   \n",
       "29          29          0        en     intent       3.571429      3.574686   \n",
       "30          30          1        en  sentiment       3.714286      3.715971   \n",
       "31          31          0        en     intent       3.857143      3.857562   \n",
       "32          32          1        en  sentiment       4.000000      4.001290   \n",
       "33          33          0        en     intent       4.166667      4.167401   \n",
       "34          34          0        en     intent       4.333333      4.334398   \n",
       "35          35          0        en     intent       4.500000      4.500392   \n",
       "36          36          0        en     intent       4.666667      4.668474   \n",
       "37          37          0        en     intent       4.833333      4.835481   \n",
       "\n",
       "         end  result_code   latency  \n",
       "0   0.093103          200  0.089085  \n",
       "1   0.211028          200  0.085931  \n",
       "2   0.578213          200  0.328101  \n",
       "3   0.903518          200  0.526460  \n",
       "4   0.895607          200  0.394011  \n",
       "5   1.229426          200  0.603952  \n",
       "6   1.548097          200  0.792747  \n",
       "7   0.965960          200  0.088426  \n",
       "8   1.094315          200  0.093197  \n",
       "9   1.887494          200  0.761070  \n",
       "10  2.215109          200  0.960721  \n",
       "11  2.533208          200  1.156233  \n",
       "12  1.584714          200  0.083080  \n",
       "13  1.712055          200  0.084681  \n",
       "14  2.281436          200  0.530103  \n",
       "15  2.843521          200  0.967229  \n",
       "16  3.191591          200  1.187995  \n",
       "17  3.503997          200  1.392534  \n",
       "18  3.822897          200  1.598431  \n",
       "19  4.149760          200  1.814784  \n",
       "20  2.528694          200  0.082540  \n",
       "21  4.464265          200  1.908199  \n",
       "22  4.776895          200  2.105115  \n",
       "23  5.100220          200  2.321867  \n",
       "24  5.424154          200  2.533278  \n",
       "25  5.730242          200  2.725987  \n",
       "26  6.047008          200  2.902304  \n",
       "27  6.381157          200  3.092237  \n",
       "28  6.702036          200  3.269347  \n",
       "29  7.037221          200  3.462535  \n",
       "30  3.798963          200  0.082992  \n",
       "31  7.373729          200  3.516167  \n",
       "32  4.093341          200  0.092051  \n",
       "33  7.684968          200  3.517567  \n",
       "34  8.004573          200  3.670175  \n",
       "35  8.317927          200  3.817535  \n",
       "36  8.633385          200  3.964911  \n",
       "37  8.942523          200  4.107042  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_single_benchmark(\n",
    "        model_callback: Callable, \n",
    "        requests_per_sec: float,\n",
    "        num_sec: int,\n",
    "        model_id_to_params: List[Tuple[str, str]],\n",
    "        model_lambda: float = 0.3,\n",
    "        seed: int = 42) -> pd.DataFrame:\n",
    "    '''\n",
    "    A single run of the benchmark.\n",
    "\n",
    "    Sends a stream of requests to multiple models, with the rate varying\n",
    "    according to a Poisson distribution and division of traffic among models\n",
    "    following a truncated Poisson distribution.\n",
    "\n",
    "    :param model_callback: Thread-safe callback function that makes a \n",
    "                           single request and returns a tuple of\n",
    "                           ``(result code, start time, end time)``.\n",
    "                           Should have the signature\n",
    "                           `f(model_type: str, language: str)`\n",
    "    :param request_per_sec: Mean of the Poisson distribution that determines\n",
    "     the number of requests in each 1-second window.\n",
    "    :param num_sec: Seconds of traffic to generate at the requested rate.\n",
    "     The actual session will extend past this window until all open requests\n",
    "     have finished.\n",
    "    :param model_lambda: Primary parameter of the truncated Poisson\n",
    "     distribution used to split requests among models. Approximately\n",
    "     equal to the mean of the distribution. The default value of 0.3 sends\n",
    "     70% of traffic to model 0.\n",
    "    :param model_id_to_params: List that maps integer model ID to a tuple of \n",
    "     (language code, model name) for each of the models.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: DataFrame of benchmark results at per-request granularity\n",
    "    '''\n",
    "    # Preallocate the trace as a set of lists.\n",
    "    benchmark_start_time = time.time()\n",
    "    desired_start_times = (\n",
    "        gen_start_times(requests_per_sec, num_sec, seed)\n",
    "        + benchmark_start_time)\n",
    "    num_requests = desired_start_times.shape[0]\n",
    "    model_nums = gen_model_ids(model_lambda, len(model_id_to_params),\n",
    "                               num_requests, seed)\n",
    "    language_codes = [model_id_to_params[num][0] for num in model_nums]\n",
    "    model_types = [model_id_to_params[num][1] for num in model_nums]\n",
    "    actual_start_times = [None] * num_requests\n",
    "    end_times = [None] * num_requests\n",
    "    result_codes = [None] * num_requests\n",
    "\n",
    "    # Because some notebook servers (i.e. VSCode) don't play well with\n",
    "    # asyncio, we use threads to manage concurrent requests.\n",
    "    thread_pool = concurrent.futures.ThreadPoolExecutor(1000)\n",
    "\n",
    "    # Map from request object to request number\n",
    "    active_requests = {}  # type: Dict[concurrent.futures.Future, int]\n",
    "\n",
    "    # Main event loop: Spawn background requests, get their responses.\n",
    "    request_num = 0\n",
    "    while request_num < num_requests or len(active_requests) > 0:\n",
    "        sec_to_next = (\n",
    "            1.0 if request_num >= num_requests\n",
    "            else desired_start_times[request_num] - time.time()\n",
    "        )\n",
    "        if sec_to_next <= 0:\n",
    "            # Time to send the next request\n",
    "            lang_code = language_codes[request_num]\n",
    "            model_type = model_types[request_num]\n",
    "            future = thread_pool.submit(\n",
    "                model_callback, model_type, lang_code)\n",
    "            active_requests[future] = request_num\n",
    "            request_num += 1\n",
    "        else:\n",
    "            # Block until it's time to send the next request or a previous\n",
    "            # request is done.\n",
    "            ready_set, _ = concurrent.futures.wait(\n",
    "                list(active_requests.keys()),\n",
    "                timeout=sec_to_next)\n",
    "\n",
    "            # Record timings from any open requests that have completed.\n",
    "            for future in ready_set:\n",
    "                request_id = active_requests.pop(future)\n",
    "                result_code, start_time, end_time = future.result()\n",
    "                actual_start_times[request_id] = start_time\n",
    "                end_times[request_id] = end_time\n",
    "                result_codes[request_id] = result_code\n",
    "\n",
    "    # Collate results as a DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        'request_id': range(num_requests),\n",
    "        'model_num': model_nums,\n",
    "        'lang_code': language_codes,\n",
    "        'model_type': model_types,\n",
    "        'desired_start': desired_start_times,\n",
    "        'actual_start': actual_start_times,\n",
    "        'end': end_times,\n",
    "        'result_code': result_codes\n",
    "    })\n",
    "\n",
    "    # Make all times relative to start of the trace\n",
    "    for key in (\"desired_start\", \"actual_start\", \"end\"):\n",
    "        result[key] -= benchmark_start_time\n",
    "    result[\"latency\"] = result[\"end\"] - result[\"actual_start\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def call_torchserve(model_type: str, language: str):\n",
    "    return call_model(model_type, language, TORCHSERVE_PORT)\n",
    "\n",
    "\n",
    "# Quick test run\n",
    "run_single_benchmark(call_torchserve, 6, 5, MODEL_ID_TO_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5047d",
   "metadata": {},
   "source": [
    "Now we can define a top-level function that runs the entire benchmark, gradually ramping up the average request rate of the bursty traffic until requests start timing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccb1d361-41b6-481c-a770-6f802d1b782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FAILURES = 0\n",
    "RUNNING_TIME_SEC = 60\n",
    "\n",
    "def run_benchmarks(\n",
    "        model_callback: Callable, \n",
    "        num_sec: int = 60,\n",
    "        min_request_rate: int = 2,\n",
    "        request_rate_step: int = 1,\n",
    "        max_failures: int = 0) -> pd.DataFrame:\n",
    "    '''\n",
    "    Perform multiple runs of the benchmark, increasing the request\n",
    "    rate gradually until requests start returning errors.\n",
    "\n",
    "    :param num_sec: Seconds of traffic to generate for each run.\n",
    "                    The actual session will extend past this window \n",
    "                    until all open requests have finished.\n",
    "    :param min_request_rate: Mean request rate for the first run of the\n",
    "                             benchmark.\n",
    "                             The actual request rate will follow a Poisson \n",
    "                             distribution with this mean.\n",
    "    :param request_rate_step: Amount by which the request rate increases\n",
    "                              with each subsequent run of the benchmark,\n",
    "                              in requests per second.\n",
    "    :param max_failures: How many failed web service calls the benchmark\n",
    "                         will tolerate per run before stopping the overall\n",
    "                         process.\n",
    "\n",
    "    :returns: A Pandas DataFrame of detailed timings for all web service\n",
    "              requests. The column ``request_rate`` tells which run of the\n",
    "              benchmark each request belongs to.\n",
    "    '''\n",
    "    to_concat = []\n",
    "    request_rate = min_request_rate\n",
    "    num_failures = 0\n",
    "\n",
    "    while num_failures <= max_failures:\n",
    "        print(f'Running at {request_rate} requests/sec.')\n",
    "        times = run_single_benchmark(model_callback, \n",
    "                                     request_rate, num_sec,\n",
    "                                     MODEL_ID_TO_PARAMS)\n",
    "        times.insert(0, 'request_rate', request_rate)\n",
    "        to_concat.append(times)\n",
    "        num_failures = sum(times['result_code'] != 200)\n",
    "        request_rate += request_rate_step\n",
    "\n",
    "    print(f'Stopping due to number of failures ({num_failures}) '\n",
    "          f'exceeding allowable limit ({max_failures})')\n",
    "    return pd.concat(to_concat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403c8b3-7dda-4c32-86ec-1ba0eee6be34",
   "metadata": {},
   "source": [
    "### Baseline benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "510f535f-12a8-4104-a9c1-ea4c0a24936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at 2 requests/sec.\n",
      "Running at 3 requests/sec.\n",
      "Running at 4 requests/sec.\n",
      "Running at 5 requests/sec.\n",
      "Running at 6 requests/sec.\n",
      "Stopping due to number of failures (70) exceeding allowable limit (0)\n"
     ]
    }
   ],
   "source": [
    "results = run_benchmarks(call_torchserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e96396d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_rate</th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>model_type</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>result_code</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.077841</td>\n",
       "      <td>200</td>\n",
       "      <td>0.076408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166783</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>200</td>\n",
       "      <td>0.079717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333452</td>\n",
       "      <td>0.680786</td>\n",
       "      <td>200</td>\n",
       "      <td>0.347334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500829</td>\n",
       "      <td>1.025693</td>\n",
       "      <td>200</td>\n",
       "      <td>0.524864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.667457</td>\n",
       "      <td>1.005116</td>\n",
       "      <td>200</td>\n",
       "      <td>0.337659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.837799</td>\n",
       "      <td>1.322753</td>\n",
       "      <td>200</td>\n",
       "      <td>0.484954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.003644</td>\n",
       "      <td>1.651029</td>\n",
       "      <td>200</td>\n",
       "      <td>0.647385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.334034</td>\n",
       "      <td>1.416483</td>\n",
       "      <td>200</td>\n",
       "      <td>0.082449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.666769</td>\n",
       "      <td>1.742849</td>\n",
       "      <td>200</td>\n",
       "      <td>0.076080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000116</td>\n",
       "      <td>2.328511</td>\n",
       "      <td>200</td>\n",
       "      <td>0.328395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   request_rate  request_id  model_num lang_code model_type  desired_start  \\\n",
       "0             4           0          1        en  sentiment       0.000000   \n",
       "1             4           1          1        en  sentiment       0.166667   \n",
       "2             4           2          0        en     intent       0.333333   \n",
       "3             4           3          2        en         qa       0.500000   \n",
       "4             4           4          0        en     intent       0.666667   \n",
       "5             4           5          0        en     intent       0.833333   \n",
       "6             4           6          0        en     intent       1.000000   \n",
       "7             4           7          1        en  sentiment       1.333333   \n",
       "8             4           8          1        en  sentiment       1.666667   \n",
       "9             4           9          0        en     intent       2.000000   \n",
       "\n",
       "   actual_start       end  result_code   latency  \n",
       "0      0.001433  0.077841          200  0.076408  \n",
       "1      0.166783  0.246500          200  0.079717  \n",
       "2      0.333452  0.680786          200  0.347334  \n",
       "3      0.500829  1.025693          200  0.524864  \n",
       "4      0.667457  1.005116          200  0.337659  \n",
       "5      0.837799  1.322753          200  0.484954  \n",
       "6      1.003644  1.651029          200  0.647385  \n",
       "7      1.334034  1.416483          200  0.082449  \n",
       "8      1.666769  1.742849          200  0.076080  \n",
       "9      2.000116  2.328511          200  0.328395  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some example outputs\n",
    "results[results['request_rate'] == 4].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41097a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">latency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request_rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333326</td>\n",
       "      <td>0.327635</td>\n",
       "      <td>0.697247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.397609</td>\n",
       "      <td>0.364708</td>\n",
       "      <td>1.128865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.787526</td>\n",
       "      <td>0.723415</td>\n",
       "      <td>2.220640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.069852</td>\n",
       "      <td>6.777993</td>\n",
       "      <td>14.300960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               latency                     \n",
       "                  mean    median        max\n",
       "request_rate                               \n",
       "2             0.333326  0.327635   0.697247\n",
       "3             0.397609  0.364708   1.128865\n",
       "4             0.787526  0.723415   2.220640\n",
       "5             6.069852  6.777993  14.300960"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate the results from the runs that didn't experience timeouts.\n",
    "timeout_rate = results['request_rate'].max()\n",
    "agg_results = (\n",
    "    results[results['request_rate'] < timeout_rate]\n",
    "    .groupby(\"request_rate\")\n",
    "    .aggregate({\"latency\": [\"mean\", \"median\", \"max\"]}))\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9496843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average Latency (sec)')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partial plot. Don't include this plot in the blog.\n",
    "plt.scatter(agg_results.index, agg_results[\"latency\", \"mean\"])\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Average Latency (sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c948be2-7759-4ee1-866c-ef52df1cd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Make sure that TorchServe is shut down before we continue.\n",
    "torchserve_is_running = True\n",
    "try:\n",
    "    requests.get('http://127.0.0.1:8081/models').json()\n",
    "except requests.exceptions.ConnectionError:\n",
    "    torchserve_is_running = False\n",
    "if torchserve_is_running:\n",
    "    raise ValueError('Please shut down TorchServe before continuing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87735cb-441e-46c2-95d4-89179e84a1d1",
   "metadata": {},
   "source": [
    "### Benchmark run with zero-copy model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f71e2ec-77c9-4ef1-abea-dc3fad338794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the Ray models are up\n",
    "try:\n",
    "    requests.put(\n",
    "        'http://127.0.0.1:8000/predictions/intent_en', \n",
    "        json.dumps(INTENT_INPUT)).json()\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    raise ValueError('Please start up the zero-copy model deployment before continuing.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3f6ede3-e8e4-4ea9-8788-bbaf07187af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at 2 requests/sec.\n",
      "Running at 3 requests/sec.\n",
      "Running at 4 requests/sec.\n",
      "Running at 5 requests/sec.\n",
      "Running at 6 requests/sec.\n",
      "Running at 7 requests/sec.\n",
      "Running at 8 requests/sec.\n",
      "Running at 9 requests/sec.\n",
      "Running at 10 requests/sec.\n",
      "Running at 11 requests/sec.\n",
      "Stopping due to number of failures (9) exceeding allowable limit (0)\n"
     ]
    }
   ],
   "source": [
    "# Same benchmark, but pointed at our zero-copy implementation.\n",
    "def call_ray(model_type: str, language: str):\n",
    "    return call_model(model_type, language, 8000)\n",
    "\n",
    "results_zerocopy = run_benchmarks(call_ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3606a607-ef0c-4bbd-8093-05099c24f833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">latency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request_rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.344801</td>\n",
       "      <td>0.294816</td>\n",
       "      <td>2.941666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.458517</td>\n",
       "      <td>0.306698</td>\n",
       "      <td>3.183783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.231333</td>\n",
       "      <td>0.545989</td>\n",
       "      <td>5.528882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.532119</td>\n",
       "      <td>1.095748</td>\n",
       "      <td>6.054023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.634467</td>\n",
       "      <td>1.026585</td>\n",
       "      <td>8.508689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.754565</td>\n",
       "      <td>1.159115</td>\n",
       "      <td>7.509175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.680933</td>\n",
       "      <td>1.281266</td>\n",
       "      <td>6.736536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.165534</td>\n",
       "      <td>1.753185</td>\n",
       "      <td>17.289803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.278934</td>\n",
       "      <td>1.792043</td>\n",
       "      <td>10.409607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               latency                     \n",
       "                  mean    median        max\n",
       "request_rate                               \n",
       "2             0.344801  0.294816   2.941666\n",
       "3             0.458517  0.306698   3.183783\n",
       "4             1.231333  0.545989   5.528882\n",
       "5             1.532119  1.095748   6.054023\n",
       "6             1.634467  1.026585   8.508689\n",
       "7             1.754565  1.159115   7.509175\n",
       "8             1.680933  1.281266   6.736536\n",
       "9             2.165534  1.753185  17.289803\n",
       "10            2.278934  1.792043  10.409607"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeout_rate = results_zerocopy['request_rate'].max()\n",
    "agg_results_zerocopy = (\n",
    "    results_zerocopy[results_zerocopy['request_rate'] < timeout_rate]\n",
    "    .groupby(\"request_rate\")\n",
    "    .aggregate({\"latency\": [\"mean\", \"median\", \"max\"]}))\n",
    "agg_results_zerocopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41323031-2b33-4835-a40f-cb01e503388a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd5896e3190>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the two sets of results against each other.\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(agg_results.index, agg_results[\"latency\", \"mean\"],\n",
    "         \"-o\", label=\"Without zero-copy loading\")\n",
    "plt.plot(agg_results_zerocopy.index, \n",
    "         agg_results_zerocopy[\"latency\", \"mean\"],\n",
    "         \"-o\", label=\"With zero-copy loading\")\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Average Request Latency (sec)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91dea0-2356-4dee-ae4f-bbfd54cb8561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afa7e0f34d224467fd24b0cfa9c212efa127bdf53fe1c4e3ddf54198f34a39e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
