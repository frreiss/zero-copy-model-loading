{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0de8e77-2ee4-464c-b2da-d4983168dfbc",
   "metadata": {},
   "source": [
    "# Title of new blog post goes here\n",
    "\n",
    "In a [previous post](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c), we introduced the concept of *zero-copy model loading*. Zero-copy model loading involves keeping the weights of a machine learning model in shared memory, so that different processes can load the model for inference instantly without copying data.\n",
    "\n",
    "We showed that the Plasma object store integrated into [Ray](https://www.ray.io/) makes it easy to do zero-copy model loading, and that implementing this technique on Ray can accelerate model loading by several orders of magnitude. If you'd like to find out more about the details of zero-copy model loading, follow [this link](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c) to view the previous post.\n",
    "\n",
    "In this post, we focus on how zero-copy model loading lets you deploy models in production with fewer resources, fewer knobs to tune, and better performance. We introduce `zerocopy`, a Python package that makes it extra simple to apply the technique. We show how to deploy models using with `zerocopy` and Ray Serve. Finally, we present an end-to-end model serving benchmark that shows how we can serve multiple large NLP models with a single cloud VM and achieve (**TODO: final numbers**)x better scalability with no tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edbf7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import zerocopy\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7add2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 11:28:47,963\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=4570)\u001b[0m 2022-03-07 11:28:52,095\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=4570)\u001b[0m 2022-03-07 11:28:52,203\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:nhrXXr:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-07 11:28:52,625\tINFO api.py:475 -- Started Serve instance in namespace '0487c778-3b08-4b11-9092-6eca28563756'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=4572)\u001b[0m INFO:     Started server process [4572]\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog post.\n",
    "# Fire up Ray\n",
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()\n",
    "\n",
    "# Wait a moment to make sure that all log output goes to this cell\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cedcb5-62a4-401b-ac58-d2fe41044dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introducing `zerocopy`\n",
    "\n",
    "Our previous post included code snippets that show how to rewrite a PyTorch model to use zero-copy model loading. \n",
    "We've recently released a Python package, `zerocopy`, that lets you apply this technique to your models without having to copy and paste Python code. \n",
    "This package is part of IBM's [Project Codeflare](https://github.com/project-codeflare), a framework to simplify the integration, scaling and acceleration of complex multi-step analytics and machine learning pipelines.\n",
    "\n",
    "**(TODO: Publish the package to PyPI and insert installation instructions here)**\n",
    "\n",
    "Using the `zerocopy` package is a three-step process:\n",
    "1. Import the package.\n",
    "2. Move your model's weights onto the Plasma object store.\n",
    "3. Run your model in an asynchronous Ray task.\n",
    "\n",
    "Let's show these three steps in action with the [BERT language model](https://arxiv.org/abs/1810.04805) as implemented in the [Transformers](https://huggingface.co/docs/transformers/index) library.\n",
    "\n",
    "Step 1 is just a Python `import` statement:\n",
    "```python \n",
    "import zerocopy\n",
    "```\n",
    "\n",
    "Then it's on to step 2: Moving your model's weights onto Plasma. You will of course need a PyTorch model to do this step. Here we use the Transformers library's `from_pretrained()` function to load a copy of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c168f1-faa9-4dbb-aeea-64940ec5f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "727b62eb-186b-49a5-a612-c554282323c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  1005,  1055,  2178,  2773,  2005,  1005,  1996, 22244,\n",
       "          1005,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog\n",
    "# Load a tokenizer to go with the model\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased')\n",
    "text = \"What's another word for 'thesaurus'?\"\n",
    "bert_input = bert_tokenizer(text, return_tensors='pt')\n",
    "bert_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c449584-9ad4-484f-957e-162bf40f8781",
   "metadata": {},
   "source": [
    "To move this model's weights onto Plasma, you first need to pass the model through `zerocopy.extract_tensors()`, which separates the weights from the model's Python code. This function returns a copy of the model without any weights and a separate Python dictionary containing the weights. Then you need to copy the model and its weights to Plasma using the function `ray.put()`. You can do both of these operations with a single line of Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e30e8b0e-d6c2-4ec2-a3fe-369ab675f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ref = ray.put(zerocopy.extract_tensors(bert_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b456a-f5aa-48b1-81f6-c14e53835015",
   "metadata": {},
   "source": [
    "The return value from `ray.put()` is a Ray [object reference](https://arrow.apache.org/docs/python/plasma.html#object-ids). This object reference lets you load the model almost instantly from any location on your Ray cluster. This capability is what enables step 3: Running your model in an asynchronous Ray task.\n",
    "\n",
    "In our previous post, we showed how you can define a stateless Ray task that loads the model, runs inference over an input, and returns the result. The `zerocopy` package includes a built-in function `call_model()` that lets you do all these steps in one line of Python code. You just pass in the object reference, the name of the method on the model you want to invoke, and the arguments to that method. `call_model.remote()` takes care of the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2edae889-4937-450a-9813-b3a44dceb68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ref = zerocopy.call_model.remote(bert_ref, [], bert_input, '__call__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c8211-6dbe-46a4-b9b6-f0bfc0dc1fe1",
   "metadata": {},
   "source": [
    "As with any other Ray task, `call_model.remote()` returns a [future](https://docs.ray.io/en/latest/ray-overview/index.html#parallelizing-python-java-functions-with-ray-tasks) --- a Ray object reference to the place where the result will appear once the task has completed. You can retrieve this result with `ray.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65a1e427-fc22-4685-9f49-02dcc6145744",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ray.get(result_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23599bf9-9e86-46eb-8f5d-e95a21732bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The time to invoke the rewritten model is almost the same as running the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c162f92a-93f0-4732-9abc-b7d2db7f84fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time to run locally: 77.5 ms ± 951 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Time to run with zero-copy: 87.1 ms ± 2.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit bert_model(**bert_input)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(bert_ref, [], bert_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748fc9e-bb77-4697-9557-e96e1b788c26",
   "metadata": {},
   "source": [
    "If you run inference multiple times, `zero_copy.call_model()` can send those inference requests to separate Ray tasks that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93e1c83a-585e-4b13-ad18-7a5dd937a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 200 times with zero-copy: 4.02 s ± 134 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "       Time to run 200 times locally: 15.9 s ± 38.5 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use output from running this cell on a large machine for the blog post\n",
    "def run_local(num_repeats: int):\n",
    "    return [bert_model(**bert_input)\n",
    "            for _ in range(num_repeats)]\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    return ray.get([zerocopy.call_model.remote(bert_ref, [], bert_input)\n",
    "                    for _ in range(num_repeats)])\n",
    "\n",
    "\n",
    "NUM_REPEATS = 200\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334ff0f-d0ad-4d8d-9b58-9f9fbf5bada5",
   "metadata": {},
   "source": [
    "## Model inference pipelines\n",
    "\n",
    "With the `zerocopy` library's `extract_tensors()` and `call_model()` functions, you can apply zero-copy model loading to a Pytorch model with two lines of Python. But what about the end-to-end program that this model came from? \n",
    "\n",
    "Most machine learning models require additional code to apply them in a meaningful way. NLP models in particular require *preprocessing* to convert natural language text into a format the model can understand and *postprocessing* to convert the model's answer into a format that a person can understand. \n",
    "\n",
    "It's common to package the model as *pipeline* that includes preprocessing, inference, and postprocessing bundled together in a single Python object. For example, the Transformers library's BERT model that we have been using in our examples so far comes with a pipeline that performs the end-to-end task of *masked language modeling*: Identifying the most likely word to fill in a blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0938883f-cd5f-4ed6-b2f7-d195652859ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6013057231903076,\n",
       "  'token': 2033,\n",
       "  'token_str': 'me',\n",
       "  'sequence': 'all your base are belong to me.'},\n",
       " {'score': 0.08610764890909195,\n",
       "  'token': 2032,\n",
       "  'token_str': 'him',\n",
       "  'sequence': 'all your base are belong to him.'},\n",
       " {'score': 0.054536789655685425,\n",
       "  'token': 2017,\n",
       "  'token_str': 'you',\n",
       "  'sequence': 'all your base are belong to you.'},\n",
       " {'score': 0.04619930312037468,\n",
       "  'token': 2149,\n",
       "  'token_str': 'us',\n",
       "  'sequence': 'all your base are belong to us.'},\n",
       " {'score': 0.03938837721943855,\n",
       "  'token': 2068,\n",
       "  'token_str': 'them',\n",
       "  'sequence': 'all your base are belong to them.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pipeline = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "bert_pipeline('All your base are belong to [MASK].')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb709640-def3-415a-b0e0-8ef49968f7b3",
   "metadata": {},
   "source": [
    "The `zerocopy` library includes a function `rewrite_pipeline` that transforms any models embedded into Python object into Ray tasks that use zero-copy model loading to load weights. If we apply this function to a pipeline, the resulting rewritten pipeline faithfully performs all the preprocessing and postprocessing that the original pipeline performed. However, this rewritten pipeline runs the embedded PyTorch model in remote Ray tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2add9f8b-b401-4cf4-accb-c753bfe6a26c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heap memory used before rewrite: 715664\n",
      " Heap memory used after rewrite: 6848\n",
      "Output before rewrite: 0.6013057231903076\n",
      " Output after rewrite: 0.6013057231903076\n"
     ]
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "\n",
    "zero_copy_bert_pipeline = zerocopy.rewrite_pipeline(bert_pipeline)\n",
    "print(f\"Heap memory used before rewrite: {asizeof.asizeof(bert_pipeline)}\")\n",
    "print(f\" Heap memory used after rewrite: {asizeof.asizeof(zero_copy_bert_pipeline)}\")\n",
    "\n",
    "print(f\"Output before rewrite: {bert_pipeline('All your base are belong to [MASK].')[0]['score']}\")\n",
    "print(f\" Output after rewrite: {zero_copy_bert_pipeline('All your base are belong to [MASK].')[0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e89618-037d-4440-883d-9bcf45f9afce",
   "metadata": {},
   "source": [
    "## A better way to deploy models\n",
    "\n",
    "In a world without zero-copy model loading, anyone who runs deep learning models in production runs into a set of difficult design constraints. If you want to run inference on a model, you need to have the model's weights loaded into a process's memory. But loading the model from storage is orders of magnitude more expensive than running inference. So you need to keep the model perpetually loaded in memory. That means that you need to keep a process around all the time, with the model loaded. This process in turn needs to have enough CPU and memory resources reserved for it so that it can run inference when it is called upon to do so.\n",
    "\n",
    "These constraints are why systems like [TorchServe](https://github.com/pytorch/serve/blob/master/docs/management_api.md), [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/serving_kubernetes), and [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/overview.html#e2e-serving-with-model-servers) run each model in its own container, or equivalently in a UNIX process with reserved CPU and memory resources.\n",
    "\n",
    "**TODO: Find and insert an appropriate block diagram here**\n",
    "\n",
    "It is possible to achieve good response times and low costs with this design, but you will need to tune many parameters to arrive at that point. How much CPU and memory capacity should you reserve for each model's containers? How many replicas of each container should you keep up at all times? How quickly should you spin up additional replicas in response to changing loads? How quickly should you spin down replicas when they aren't being used? How do you even measure the current load? All of these important decisions take time and energy.\n",
    "\n",
    "Zero-copy model loading with Ray turns model inference into a stateless process. As long as the weights are in the local segment of the Plasma object store, your code can load a copy of the model instantly, run a single inference request, and then unload the model. This statelessness removes the design constraints that underlie conventional model serving systems. With zero-copy loading, there's no need to manage a pool of containers. Just load all the model weights onto Plasma and let Ray take care of replicating objects to nodes of the cluster as needed.\n",
    "\n",
    "A small Ray cluster can keep the weights for hundreds of models in shared memory. It can instantly dedicate the entire cluster's CPU resources to a single model. Then it can just as quickly retask all of those resources to a different model. You get an instant, optimal response to whatever workload the application sends your way. And no manual tuning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4075d9-2771-42cf-b308-40e9e5a8ffde",
   "metadata": {},
   "source": [
    "The best part about this approach is that deploying models with Ray and zero-copy is actually *easier* than deploying them with a traditional model serving framework.\n",
    "\n",
    "*TODO: Intro to Ray Serve*\n",
    "\n",
    "Here's how the code to deploy our example BERT model to Ray Serve with zero-copy model loading looks.\n",
    "\n",
    "First you define an endpoint by creating a small Python class. Then you call wrap an instance of that class in a Ray Serve *deployment*. The code looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c25fe785-18c4-4561-886d-be5f9ac8ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "\n",
    "\n",
    "# Step 1: Define a Ray Serve deployment\n",
    "@serve.deployment\n",
    "class MyDeployment:\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "\n",
    "        # Load the entire pipeline\n",
    "        self._pipeline = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "        # Move the model weights to Plasma\n",
    "        self._pipeline.model = zerocopy.extract_tensors(self._pipeline.model)\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        '''\n",
    "        Web service entry point.\n",
    "\n",
    "        Args:\n",
    "            request: HTTP request object for a REST web service call\n",
    "                     in the form:\n",
    "                     { \"input\": \"<input text with [MASK]>\" }\n",
    "        '''\n",
    "        # Parse JSON. A real deployment would also sanitize the input.\n",
    "        json_request = await request.json()\n",
    "        input_ = json_request['input']\n",
    "\n",
    "        # Preprocessing\n",
    "        features = self._pipeline.preprocess(input_)\n",
    "\n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        raw_output = await zerocopy.call_model.remote(\n",
    "            self._pipeline.model, [], features)\n",
    "\n",
    "        # Postprocessing\n",
    "        raw_output[\"input_ids\"] = features[\"input_ids\"]\n",
    "        return self._pipeline.postprocess(raw_output)\n",
    "    \n",
    "\n",
    "# Step 2: Attach the deployment to an HTTP endpoint\n",
    "MyDeployment.options(name='my_model', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "# There is no step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130c9b6-ae1e-4990-b7ad-13a54abbb957",
   "metadata": {},
   "source": [
    "Let's walk through the listing \n",
    "\n",
    "\n",
    "Ray Serve automatically handles incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "720bdb27-f993-4213-aa8c-6878c4d69263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.6013057231903076,\n",
      "    \"token\": 2033,\n",
      "    \"token_str\": \"me\",\n",
      "    \"sequence\": \"all your base are belong to me.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08610764890909195,\n",
      "    \"token\": 2032,\n",
      "    \"token_str\": \"him\",\n",
      "    \"sequence\": \"all your base are belong to him.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.054536789655685425,\n",
      "    \"token\": 2017,\n",
      "    \"token_str\": \"you\",\n",
      "    \"sequence\": \"all your base are belong to you.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.04619930312037468,\n",
      "    \"token\": 2149,\n",
      "    \"token_str\": \"us\",\n",
      "    \"sequence\": \"all your base are belong to us.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.03938837721943855,\n",
      "    \"token\": 2068,\n",
      "    \"token_str\": \"them\",\n",
      "    \"sequence\": \"all your base are belong to them.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(requests.put('http://127.0.0.1:8000/my_model',\n",
    "      '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33920dd8-32ff-47cb-976e-13251819318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 16:41:03,978\tINFO api.py:249 -- Updating deployment 'my_model2'. component=serve deployment=my_model2\n",
      "\u001b[2m\u001b[36m(ServeController pid=4570)\u001b[0m 2022-03-07 16:41:04,059\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'my_model2'. component=serve deployment=my_model2\n",
      "2022-03-07 16:41:04,610\tINFO api.py:261 -- Deployment 'my_model2' is ready at `http://127.0.0.1:8000/my_model2`. component=serve deployment=my_model2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.6013057231903076,\n",
      "    \"token\": 2033,\n",
      "    \"token_str\": \"me\",\n",
      "    \"sequence\": \"all your base are belong to me.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08610764890909195,\n",
      "    \"token\": 2032,\n",
      "    \"token_str\": \"him\",\n",
      "    \"sequence\": \"all your base are belong to him.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.054536789655685425,\n",
      "    \"token\": 2017,\n",
      "    \"token_str\": \"you\",\n",
      "    \"sequence\": \"all your base are belong to you.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.04619930312037468,\n",
      "    \"token\": 2149,\n",
      "    \"token_str\": \"us\",\n",
      "    \"sequence\": \"all your base are belong to us.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.03938837721943855,\n",
      "    \"token\": 2068,\n",
      "    \"token_str\": \"them\",\n",
      "    \"sequence\": \"all your base are belong to them.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Alternate version uses a multithreaded actor to run the pipeline via\n",
    "# its __call__ method.\n",
    "\n",
    "@ray.remote\n",
    "class PipelineActor:\n",
    "    '''\n",
    "    Threaded Ray actor\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        pipeline_tmp = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(pipeline_tmp)\n",
    "\n",
    "    def run(self, input_: str):\n",
    "        # Model inference calls inside this pipeline will happen in remote\n",
    "        # Ray tasks.\n",
    "        return self._pipeline(input_)\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class MyDeployment2:\n",
    "    def __init__(self):\n",
    "        self._pipeline_actor = PipelineActor.options(max_concurrency=100,\n",
    "                                                     num_cpus=0.1).remote()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "        input_ = json_request['input']\n",
    "\n",
    "        result = await self._pipeline_actor.run.remote(input_)\n",
    "        return result\n",
    "    \n",
    "MyDeployment2.options(name='my_model2', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "print(requests.put('http://127.0.0.1:8000/my_model2', \n",
    "      '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95790e6e-c8a4-4419-8b88-b50b8ead26e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 16:42:17,167\tINFO api.py:249 -- Updating deployment 'my_model3'. component=serve deployment=my_model3\n",
      "\u001b[2m\u001b[36m(ServeController pid=4570)\u001b[0m 2022-03-07 16:42:17,241\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'my_model3'. component=serve deployment=my_model3\n",
      "2022-03-07 16:42:24,727\tINFO api.py:261 -- Deployment 'my_model3' is ready at `http://127.0.0.1:8000/my_model3`. component=serve deployment=my_model3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.6013057231903076,\n",
      "    \"token\": 2033,\n",
      "    \"token_str\": \"me\",\n",
      "    \"sequence\": \"all your base are belong to me.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08610764890909195,\n",
      "    \"token\": 2032,\n",
      "    \"token_str\": \"him\",\n",
      "    \"sequence\": \"all your base are belong to him.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.054536789655685425,\n",
      "    \"token\": 2017,\n",
      "    \"token_str\": \"you\",\n",
      "    \"sequence\": \"all your base are belong to you.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.04619930312037468,\n",
      "    \"token\": 2149,\n",
      "    \"token_str\": \"us\",\n",
      "    \"sequence\": \"all your base are belong to us.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.03938837721943855,\n",
      "    \"token\": 2068,\n",
      "    \"token_str\": \"them\",\n",
      "    \"sequence\": \"all your base are belong to them.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Alternate version uses a thread pool and acyncio's run_in_executor()\n",
    "# method run the pipeline via its __call__ method.\n",
    "\n",
    "@serve.deployment\n",
    "class MyDeployment3:\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        pipeline_tmp = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(pipeline_tmp)\n",
    "\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        '''\n",
    "        Web service entry point.\n",
    "\n",
    "        Args:\n",
    "            request: HTTP request object for a REST web service call\n",
    "                     in the form:\n",
    "                     { \"input\": \"<input text with [MASK]>\" }\n",
    "        '''\n",
    "        # Parse JSON. A real deployment would also sanitize the input.\n",
    "        json_request = await request.json()\n",
    "        masked_string = json_request['input']\n",
    "\n",
    "        # The original `transformers` code is not async-aware, so we\n",
    "        # call it from `run_in_executor()`.\n",
    "        # Preprocessing and postprocessing code will happen inside this\n",
    "        # process, but model inference will occur in a remote Ray task.\n",
    "        # While that task is running, the local thread will block on\n",
    "        # a call to `ray.get()`\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(masked_string))\n",
    "        return result\n",
    "\n",
    "MyDeployment3.options(name='my_model3', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "print(requests.put('http://127.0.0.1:8000/my_model3', \n",
    "      '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4614552-0203-4ecc-a7c3-986c78c8c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=4570)\u001b[0m 2022-03-07 16:43:20,911\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'my_model2'. component=serve deployment=my_model2\n",
      "\u001b[2m\u001b[36m(ServeController pid=4570)\u001b[0m 2022-03-07 16:43:20,915\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'my_model3'. component=serve deployment=my_model3\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Stop this notebook's copy of Ray so as not to interfere with the\n",
    "# copy in `ray_deploy.ipynb`\n",
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eccd9",
   "metadata": {},
   "source": [
    "## A Simple Benchmark\n",
    "\n",
    "\n",
    "\n",
    "## The Scenario\n",
    "\n",
    "The end-to-end scenario for our benchmark involves supporting an AI chatbot.\n",
    "The chatbot's conversational AI runs off of a conversation tree. Some of the \n",
    "nodes of this tree invoke models.\n",
    "\n",
    "\n",
    "> **TODO:** Cartoon block diagram of the end-to-end scenario. \n",
    "> Diagram should show a user interacting with a chatbot. The chatbot runs off of a conversation tree. \n",
    "> Some of the nodes of the conversation tree have question answering models hanging off of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc5ddc-35bc-4c70-b3c8-c9e0debbc8bb",
   "metadata": {},
   "source": [
    "Our benchmark will cover the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs four different types of models:\n",
    "* *Intent detection* models that determine what is the user's goal.\n",
    "* *Sentiment analysis* models that monitor the user's mood.\n",
    "* *Question answering* models that provide the answers to specific factual questions.\n",
    "* *Natural language generation* models that give the chatbot's responses a less scripted flavor.\n",
    "\n",
    "Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models.\n",
    "\n",
    "In a real application, you would want to train custom versions of each type\n",
    "of model for the topics your chatbot covers.\n",
    "Since we're only interested in modeling throughput and latency, we skipped that customization\n",
    "step and just used the most popular pretrained model from each category from the \n",
    "[Huggingface model marketplace](https://huggingface.co/models).\n",
    "\n",
    "Each of these models uses a [Transformer](https://arxiv.org/abs/1706.03762)-based neural network,\n",
    "with a *language model* and a task specific *head*, tuned over \n",
    "a domain-specific training set.  The table below summarizes the four models that we used.\n",
    "\n",
    "\n",
    "| Task                 | Model Name                                   | Language Model  | Model Size (in memory) |  Pre/post Processing\n",
    "| -----------          | -----------                                  | ------------    | --------------------   |  ---------------\n",
    "| Intent Detection     | `mrm8488/t5-base-finetuned-e2m-intent`       | T5              | 1133 MiB               | Reference code\n",
    "| Sentiment Analysis   | `cardiffnlp/twitter-roberta-base-sentiment`  | RoBERTa         | 476 MiB                | Reference code\n",
    "| Question Answering   | `deepset/roberta-base-squad2`                | RoBERTa         | 474 MiB                | Pipeline\n",
    "| Text Generation      | `gpt2`                                       | GPT-2           | 634 MiB                | Pipeline\n",
    "\n",
    "\n",
    "Although all four models came from the same marketplace, they are quite diverse. The models use three different core language models: [Text-to-Text Transfer Transformer](https://arxiv.org/pdf/1910.10683.pdf) (T5) from Google Research, \n",
    "[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) from Facebook AI, and [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) from OpenAI. \n",
    "\n",
    "The models also use two very different ways to package their preprocessing and postprocessing code. The intent and sentiment models provide small blocks of reference Python code, with the intent being that the user will adapt this reference code to the specific circumstances of the end-to-end appliction. \n",
    "\n",
    "The question answering and text generation models both use the Transformers library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines) to package their preprocessing and postprocesing code. Unlike the \"example reference code\" approach, the Pipelines API's end-to-end inference code is intended for direct production use. It includes support for model retraining, as well as performance optimizations like batching and GPU acceleration, plus code for handling corner cases like long input strings. This prepackaged code can save a lot of time, provided that your application is structured in a way that can easily accomodate a large block of non-modifiable third-party Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b1aa",
   "metadata": {},
   "source": [
    "### Baseline implementation\n",
    "\n",
    "Our baseline implementation of the model serving backend for our benchmark emulates running each model in a separate container without doing extensive tuning. We used [TorchServe](https://pytorch.org/serve/) as our model serving framework for the baseline deployment. By configuing TorchServe to use a pool of processes, we were able to simulate running each model in a separate container without having to set up a dedicated Kubernetes cluster. The remaining cores on the 16-core [IBM Cloud VM](https://www.ibm.com/cloud/virtual-servers) ran the client portion of the benchmark and the TorchServe front end processes. \n",
    "\n",
    "See [this notebook](./torchserve.ipynb) for details of the TorchServe deployment.\n",
    "\n",
    "*Note (not to be included in the blog): earlier versions of this notebook implemented the baseline model deployment with a pool of Ray actors. That older version is preserved in [a separate notebook](./ray_baseline.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025582b-6588-42c8-ba47-545903b3511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the management API to verify that TorchServe is running.\n",
    "try:\n",
    "    print(requests.get('http://127.0.0.1:8081/models').json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Stop notebook execution\n",
    "    raise ValueError('TorchServe does not appear to be running. Please start TorchServe.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011ecef-438d-458d-a542-90118b7d36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the model deployments to verify that TorchServe is running.\n",
    "TORCHSERVE_PORT = 8080\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}\n",
    "\n",
    "intent_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/intent_en',\n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/sentiment_en',\n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/qa_en',\n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/generate_en',\n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6796f9-841b-4b8f-aaa7-8812dbbd097c",
   "metadata": {},
   "source": [
    "## Zero-copy implementation\n",
    "\n",
    "TODO: Describe how we deployed the four models, with reference to [this notebook](./ray_deploy.ipynb) that shows the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4316-86af-481f-8cc0-887a88c30c0d",
   "metadata": {},
   "source": [
    "## Running the Benchmark\n",
    "\n",
    "Now that we have deployed each of our models with a web service front end, we can define a benchmark that sends inference traffic to these web service endpoints and measures response time.\n",
    "\n",
    "[Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution)\n",
    "\n",
    "TODO: Cite a reference for Poisson being a realistic distribution of traffic for interactive services.\n",
    "\n",
    "Our benchmark generates a trace of requests, then plays back the trace and measures the latency of each request. The request rate changes each second, with the rate of a particular 1-second window drawn from the Poisson distribution. Each request goes to a randomly-selected model. The choice of models is weighted according to a truncated Poisson distribution.  \n",
    "\n",
    "The benchmark plays back the trace, measuring the end-to-end latency of each request. We repeat this process of generating and playing back the trace, gradually ramping up the average request rate of the bursty traffic until requests start timing out.\n",
    "\n",
    "Code is [here](./benchmark.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403c8b3-7dda-4c32-86ec-1ba0eee6be34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (not in blog) Baseline benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0e951-9592-48f2-94e4-07c328e2965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the management API to verify that TorchServe is running.\n",
    "try:\n",
    "    print(requests.get('http://127.0.0.1:8081/models').json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Stop notebook execution\n",
    "    raise ValueError('TorchServe does not appear to be running. Please start TorchServe.') from None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f99d8a4-c1b4-4529-a984-a1eb9cc47344",
   "metadata": {},
   "source": [
    "# This call may be disabled to avoid overwriting local results.\n",
    "# Toggle cell type to \"code\" to run.\n",
    "!python3 benchmark.py 8080 outputs/baseline.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c948be2-7759-4ee1-866c-ef52df1cd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Make sure that TorchServe is shut down before we continue.\n",
    "torchserve_is_running = True\n",
    "try:\n",
    "    requests.get('http://127.0.0.1:8081/models').json()\n",
    "except requests.exceptions.ConnectionError:\n",
    "    torchserve_is_running = False\n",
    "if torchserve_is_running:\n",
    "    raise ValueError('Please shut down TorchServe before continuing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06033854-2849-450f-b7e2-382f79b5654b",
   "metadata": {},
   "source": [
    "### (not in blog) Optimized benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f71e2ec-77c9-4ef1-abea-dc3fad338794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the Ray models are up\n",
    "try:\n",
    "    INTENT_INPUT = {\n",
    "        'context':\n",
    "            (\"I came here to eat chips and beat you up, \"\n",
    "             \"and I'm all out of chips.\")\n",
    "    }\n",
    "    requests.put(\n",
    "        'http://127.0.0.1:8000/predictions/intent_en', \n",
    "        json.dumps(INTENT_INPUT)).json()\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    raise ValueError('Please start up the zero-copy model deployment before continuing.') from None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ef7404d-354d-4a8e-aef6-b3906be6d0ad",
   "metadata": {},
   "source": [
    "# This call may be disabled to avoid overwriting local results.\n",
    "# Toggle cell type to \"code\" to run.\n",
    "!python3 benchmark.py 8000 outputs/zerocopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15aebd-46d9-41f1-9ae4-1947cb51b0b1",
   "metadata": {},
   "source": [
    "### (not in blog) Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4f8696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog\n",
    "\n",
    "# Aggregate benchmark results.\n",
    "def compute_stats(results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    timeout_results = results_df[results_df['result_code'] != 200]\n",
    "    success_results = results_df[results_df['result_code'] == 200]\n",
    "\n",
    "    timeout_counts = (\n",
    "        timeout_results\n",
    "        .groupby('num_users')\n",
    "        .aggregate({'request_id': 'count'})\n",
    "        .rename(columns={'request_id': 'timeouts'}))\n",
    "    stats = (\n",
    "        success_results\n",
    "        .groupby('num_users')\n",
    "        .aggregate({'latency': ['mean', 'median', 'max'],\n",
    "                    'request_id': 'count'}))\n",
    "\n",
    "    # Column names come out from the aggregations all messed up\n",
    "    stats.columns=['mean', 'median', 'max', 'successes']\n",
    "    stats = stats.join(timeout_counts).fillna(0)\n",
    "    stats['timeout_fraction'] = stats['timeouts'] / (stats['successes'] \n",
    "                                                     + stats['timeouts'])\n",
    "    stats['timeouts'] = stats['timeouts'].astype(int)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def maybe_generate_agg(prefix: str):\n",
    "    '''\n",
    "    Regenerate aggregate results for a benchmark run if a trace\n",
    "    is present.\n",
    "\n",
    "    :param prefix: Name of run, i.e. 'baseline' or 'zerocopy'\n",
    "    '''\n",
    "    if os.path.exists(f'outputs/{prefix}.csv'):\n",
    "        results = pd.read_csv(f'outputs/{prefix}.csv')\n",
    "        stats = compute_stats(results)\n",
    "        stats.to_csv(f'outputs/{prefix}_agg.csv')\n",
    "\n",
    "\n",
    "maybe_generate_agg('baseline')\n",
    "maybe_generate_agg('zerocopy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d9f5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_users</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>successes</th>\n",
       "      <th>timeouts</th>\n",
       "      <th>timeout_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.280791</td>\n",
       "      <td>0.310578</td>\n",
       "      <td>0.584357</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.302267</td>\n",
       "      <td>0.310957</td>\n",
       "      <td>0.620625</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.336864</td>\n",
       "      <td>0.313266</td>\n",
       "      <td>1.020234</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.429508</td>\n",
       "      <td>0.324988</td>\n",
       "      <td>1.753344</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.511645</td>\n",
       "      <td>0.349262</td>\n",
       "      <td>2.047470</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>0.614285</td>\n",
       "      <td>0.517198</td>\n",
       "      <td>2.941095</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>1.375264</td>\n",
       "      <td>1.543566</td>\n",
       "      <td>3.137735</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "      <td>2.124427</td>\n",
       "      <td>1.931852</td>\n",
       "      <td>5.003007</td>\n",
       "      <td>166</td>\n",
       "      <td>90</td>\n",
       "      <td>0.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>1.188452</td>\n",
       "      <td>0.309459</td>\n",
       "      <td>4.959908</td>\n",
       "      <td>121</td>\n",
       "      <td>165</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>0.972962</td>\n",
       "      <td>0.092938</td>\n",
       "      <td>4.835173</td>\n",
       "      <td>108</td>\n",
       "      <td>204</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_users      mean    median       max  successes  timeouts  \\\n",
       "0         10  0.280791  0.310578  0.584357         57         0   \n",
       "1         15  0.302267  0.310957  0.620625         88         0   \n",
       "2         20  0.336864  0.313266  1.020234        114         0   \n",
       "3         25  0.429508  0.324988  1.753344        141         0   \n",
       "4         30  0.511645  0.349262  2.047470        171         0   \n",
       "5         35  0.614285  0.517198  2.941095        199         0   \n",
       "6         40  1.375264  1.543566  3.137735        228         0   \n",
       "7         45  2.124427  1.931852  5.003007        166        90   \n",
       "8         50  1.188452  0.309459  4.959908        121       165   \n",
       "9         55  0.972962  0.092938  4.835173        108       204   \n",
       "\n",
       "   timeout_fraction  \n",
       "0          0.000000  \n",
       "1          0.000000  \n",
       "2          0.000000  \n",
       "3          0.000000  \n",
       "4          0.000000  \n",
       "5          0.000000  \n",
       "6          0.000000  \n",
       "7          0.351562  \n",
       "8          0.576923  \n",
       "9          0.653846  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog\n",
    "# Load up the baseline results\n",
    "baseline_stats = pd.read_csv('outputs/baseline_agg.csv')\n",
    "baseline_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5bb6f85-63ad-427d-875c-24ca86ad4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_users</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>successes</th>\n",
       "      <th>timeouts</th>\n",
       "      <th>timeout_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.264155</td>\n",
       "      <td>0.291887</td>\n",
       "      <td>0.728301</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.478131</td>\n",
       "      <td>0.290065</td>\n",
       "      <td>3.040544</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.516812</td>\n",
       "      <td>0.298340</td>\n",
       "      <td>3.108415</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.471256</td>\n",
       "      <td>0.295879</td>\n",
       "      <td>3.020606</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.895178</td>\n",
       "      <td>0.353172</td>\n",
       "      <td>4.205721</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>0.547787</td>\n",
       "      <td>0.304735</td>\n",
       "      <td>3.725431</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>1.225654</td>\n",
       "      <td>0.553401</td>\n",
       "      <td>4.702441</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "      <td>0.845444</td>\n",
       "      <td>0.357990</td>\n",
       "      <td>4.964735</td>\n",
       "      <td>254</td>\n",
       "      <td>2</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>1.184667</td>\n",
       "      <td>0.603067</td>\n",
       "      <td>4.861767</td>\n",
       "      <td>282</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>1.139660</td>\n",
       "      <td>0.638004</td>\n",
       "      <td>4.909761</td>\n",
       "      <td>304</td>\n",
       "      <td>8</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>1.406000</td>\n",
       "      <td>0.915088</td>\n",
       "      <td>4.869613</td>\n",
       "      <td>322</td>\n",
       "      <td>18</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>65</td>\n",
       "      <td>1.353651</td>\n",
       "      <td>0.929121</td>\n",
       "      <td>4.657467</td>\n",
       "      <td>351</td>\n",
       "      <td>19</td>\n",
       "      <td>0.051351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70</td>\n",
       "      <td>1.387002</td>\n",
       "      <td>1.013262</td>\n",
       "      <td>5.003786</td>\n",
       "      <td>379</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>75</td>\n",
       "      <td>1.934213</td>\n",
       "      <td>1.502840</td>\n",
       "      <td>4.988434</td>\n",
       "      <td>394</td>\n",
       "      <td>30</td>\n",
       "      <td>0.070755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>80</td>\n",
       "      <td>1.775195</td>\n",
       "      <td>1.368490</td>\n",
       "      <td>4.987363</td>\n",
       "      <td>426</td>\n",
       "      <td>23</td>\n",
       "      <td>0.051225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>85</td>\n",
       "      <td>1.697071</td>\n",
       "      <td>1.464466</td>\n",
       "      <td>4.943537</td>\n",
       "      <td>456</td>\n",
       "      <td>21</td>\n",
       "      <td>0.044025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>90</td>\n",
       "      <td>1.744525</td>\n",
       "      <td>1.791553</td>\n",
       "      <td>4.690907</td>\n",
       "      <td>484</td>\n",
       "      <td>20</td>\n",
       "      <td>0.039683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>95</td>\n",
       "      <td>1.767982</td>\n",
       "      <td>1.799572</td>\n",
       "      <td>4.922586</td>\n",
       "      <td>508</td>\n",
       "      <td>24</td>\n",
       "      <td>0.045113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>2.418646</td>\n",
       "      <td>2.803906</td>\n",
       "      <td>5.002402</td>\n",
       "      <td>494</td>\n",
       "      <td>67</td>\n",
       "      <td>0.119430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>105</td>\n",
       "      <td>2.615825</td>\n",
       "      <td>2.669764</td>\n",
       "      <td>5.009183</td>\n",
       "      <td>517</td>\n",
       "      <td>73</td>\n",
       "      <td>0.123729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>110</td>\n",
       "      <td>2.952222</td>\n",
       "      <td>4.079763</td>\n",
       "      <td>5.009276</td>\n",
       "      <td>352</td>\n",
       "      <td>263</td>\n",
       "      <td>0.427642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>115</td>\n",
       "      <td>1.457245</td>\n",
       "      <td>0.851736</td>\n",
       "      <td>4.911417</td>\n",
       "      <td>200</td>\n",
       "      <td>443</td>\n",
       "      <td>0.688958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_users      mean    median       max  successes  timeouts  \\\n",
       "0          10  0.264155  0.291887  0.728301         57         0   \n",
       "1          15  0.478131  0.290065  3.040544         88         0   \n",
       "2          20  0.516812  0.298340  3.108415        114         0   \n",
       "3          25  0.471256  0.295879  3.020606        141         0   \n",
       "4          30  0.895178  0.353172  4.205721        171         0   \n",
       "5          35  0.547787  0.304735  3.725431        199         0   \n",
       "6          40  1.225654  0.553401  4.702441        227         1   \n",
       "7          45  0.845444  0.357990  4.964735        254         2   \n",
       "8          50  1.184667  0.603067  4.861767        282         4   \n",
       "9          55  1.139660  0.638004  4.909761        304         8   \n",
       "10         60  1.406000  0.915088  4.869613        322        18   \n",
       "11         65  1.353651  0.929121  4.657467        351        19   \n",
       "12         70  1.387002  1.013262  5.003786        379        20   \n",
       "13         75  1.934213  1.502840  4.988434        394        30   \n",
       "14         80  1.775195  1.368490  4.987363        426        23   \n",
       "15         85  1.697071  1.464466  4.943537        456        21   \n",
       "16         90  1.744525  1.791553  4.690907        484        20   \n",
       "17         95  1.767982  1.799572  4.922586        508        24   \n",
       "18        100  2.418646  2.803906  5.002402        494        67   \n",
       "19        105  2.615825  2.669764  5.009183        517        73   \n",
       "20        110  2.952222  4.079763  5.009276        352       263   \n",
       "21        115  1.457245  0.851736  4.911417        200       443   \n",
       "\n",
       "    timeout_fraction  \n",
       "0           0.000000  \n",
       "1           0.000000  \n",
       "2           0.000000  \n",
       "3           0.000000  \n",
       "4           0.000000  \n",
       "5           0.000000  \n",
       "6           0.004386  \n",
       "7           0.007812  \n",
       "8           0.013986  \n",
       "9           0.025641  \n",
       "10          0.052941  \n",
       "11          0.051351  \n",
       "12          0.050125  \n",
       "13          0.070755  \n",
       "14          0.051225  \n",
       "15          0.044025  \n",
       "16          0.039683  \n",
       "17          0.045113  \n",
       "18          0.119430  \n",
       "19          0.123729  \n",
       "20          0.427642  \n",
       "21          0.688958  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog\n",
    "zerocopy_stats = pd.read_csv('outputs/zerocopy_agg.csv')\n",
    "zerocopy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41323031-2b33-4835-a40f-cb01e503388a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd2791123d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the two sets of results against each other.\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(baseline_stats['num_users'],\n",
    "         baseline_stats['timeout_fraction'],\n",
    "         '-o', label='Without zero-copy loading')\n",
    "plt.plot(zerocopy_stats['num_users'],\n",
    "         zerocopy_stats['timeout_fraction'],\n",
    "         '-o', label=\"With zero-copy loading\")\n",
    "plt.xlabel('Number of Users')\n",
    "plt.ylabel('Timeout Fraction\\n(lower is better)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ea8c9-3a23-4156-a0f0-477ba3f154dc",
   "metadata": {},
   "source": [
    "The baseline deployment can handle up to (TODO: Final number) users. Beyond that number, the CPU resources that are dedicated to the model with the highest traffic can no longer keep up with the increasingly large bursts of customer activity. Remember, the baseline deployment is stuck with a fixed allocation of CPU and memory because it hasn't been extensively tuned.\n",
    "\n",
    "Our Ray-based deployment with zero-copy model loading, on the other hand, scales to (TODO: Final number) without any tuning. It achieves this scalability because it can instantly retask the underlying hardware resources to whatever models are currently experiencing high traffic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bfbbb1e-0f5e-45b5-8462-6e2f9d526bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Old code that demos `zerocopy` with the intent model\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference without zero-copy loading\n",
    "print('Result without zero-copy loading: '\n",
    "      + str(intent_model.generate(**features)))\n",
    "\n",
    "# Inference with zero-copy loading\n",
    "intent_model_ref = ray.put(zerocopy.extract_tensors(intent_model))\n",
    "print(' Result *with* zero-copy loading: ' +\n",
    "      str(ray.get(zerocopy.call_model.remote(\n",
    "          intent_model_ref, [], features, 'generate'))))\n",
    "          \n",
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit intent_model.generate(**features)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(intent_model_ref, [], features, 'generate'))\n",
    "\n",
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        intent_model.generate(**features)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [\n",
    "        zerocopy.call_model.remote(intent_model_ref, [], features, 'generate')\n",
    "        for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 50\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58769647-0991-4da2-ac40-61960952b09c",
   "metadata": {},
   "source": [
    "# Old code that demos `zerocopy` on the QA model pipeline\n",
    "zero_copy_qa = zerocopy.rewrite_pipeline(qa_pipeline)\n",
    "print(f\"Before rewrite: {qa_pipeline(**QA_INPUT)}\")\n",
    "print(f\" After rewrite: {zero_copy_qa(**QA_INPUT)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afa7e0f34d224467fd24b0cfa9c212efa127bdf53fe1c4e3ddf54198f34a39e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
