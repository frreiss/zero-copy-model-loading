{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8270a16d",
   "metadata": {},
   "source": [
    "# benchmark.ipynb\n",
    "\n",
    "This notebook contains the text and code for the next blog post in the zero-copy model series, \n",
    "title TBD.\n",
    "\n",
    "The first post explained how to load PyTorch models for inference extremely fast by leveraging the Plasma object store's ability to load numeric data directly from shared memory.\n",
    "\n",
    "In this post, we talk in more concrete terms about how to use this zero-copy model loading for model serving. We put together a simple model serving system, then set up a microbenchmark that simulates a heavy-tailed traffic pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbf7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import zerocopy\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69fefd",
   "metadata": {},
   "source": [
    "# Title of new blog post goes here\n",
    "\n",
    "*Recap of previous blog post goes here.*\n",
    "\n",
    "In a [previous post](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c), we introduced the concept of *zero-copy model loading*. Zero-copy model loading involves keeping the weights of a deep learning model in shared memory, so that different processes can \"load\" the model for inference without \n",
    "\n",
    "showed that \n",
    "The Plasma object store integrated into Ray makes it easy to do zero-copy model loading\n",
    "\n",
    "and that implementing this technique on Ray can accelerate model loading by several orders of magnitude.\n",
    "\n",
    "If you'd like to find out more about these\n",
    "\n",
    "follow [this link](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c) to view the previous post.\n",
    "\n",
    "\n",
    "Today's post goes into the details of how to use zero-copy model loading for production deployments of large natural language processing (NLP) models. We introduce `zerocopy`, a Python package that makes it extra simple to apply zero-copy model loading to PyTorch models. We show how easy it is to deploy modern NLP models using with `zerocopy` and Ray Serve. Finally, we present an end-to-end model serving benchmark that shows how we can serve 12 state-of-the-art NLP models with a single cloud VM and achieve (**TODO: final numbers**)x better scalability.\n",
    "  \n",
    "But before we get into these details, we need to give some background about the end-to-end scenario we'll be targeting with our benchmark and the NLP models that use to cover this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eccd9",
   "metadata": {},
   "source": [
    "## The Scenario\n",
    "\n",
    "The end-to-end scenario for our benchmark involves supporting an AI chatbot.\n",
    "The chatbot's conversational AI runs off of a conversation tree. Some of the \n",
    "nodes of this tree invoke models.\n",
    "\n",
    "\n",
    "> **TODO:** Cartoon block diagram of the end-to-end scenario. \n",
    "> Diagram should show a user interacting with a chatbot. The chatbot runs off of a conversation tree. \n",
    "> Some of the nodes of the conversation tree have question answering models hanging off of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc5ddc-35bc-4c70-b3c8-c9e0debbc8bb",
   "metadata": {},
   "source": [
    "Our benchmark will cover the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs four different types of models:\n",
    "* *Intent detection* models that determine what is the user's goal.\n",
    "* *Sentiment analysis* models that monitor the user's mood.\n",
    "* *Question answering* models that provide the answers to specific factual questions.\n",
    "* *Natural language generation* models that give the chatbot's responses a less scripted flavor.\n",
    "\n",
    "Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models.\n",
    "\n",
    "In a real application, you would want to train custom versions of each type\n",
    "of model for the topics your chatbot covers.\n",
    "Since we're only interested in modeling throughput and latency, we skipped that customization\n",
    "step and just used the most popular pretrained model from each category from the \n",
    "[Huggingface model marketplace](https://huggingface.co/models).\n",
    "\n",
    "Each of these models uses a [Transformer](https://arxiv.org/abs/1706.03762)-based neural network,\n",
    "with a *language model* and a task specific *head*, tuned over \n",
    "a domain-specific training set.  The table below summarizes the four models that we used.\n",
    "\n",
    "\n",
    "| Task                 | Model Name                                   | Language Model  |  Pre/post Processing\n",
    "| -----------          | -----------                                  | ------------    | ---------------\n",
    "| Intent Detection     | `mrm8488/t5-base-finetuned-e2m-intent`       | T5              | Reference code\n",
    "| Sentiment Analysis   | `cardiffnlp/twitter-roberta-base-sentiment`  | RoBERTa         | Reference code\n",
    "| Question Answering   | `deepset/roberta-base-squad2`                | RoBERTa         | Pipeline\n",
    "| Text Generation      | `gpt2`                                       | GPT-2           | Pipeline\n",
    "\n",
    "\n",
    "Although all four models came from the same marketplace, they are quite diverse. The models use three different core language models: [Text-to-Text Transfer Transformer](https://arxiv.org/pdf/1910.10683.pdf) (T5) from Google Research, \n",
    "[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) from Facebook AI, and [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) from OpenAI. \n",
    "\n",
    "NLP models generally require *preprocessing* to convert natural language text into a format amenable to \n",
    "model inference and *postprocessing* to convert the answer into a format that a person can understand. All four of our models come with code for these crucial steps, but models use two very different ways to package this code. The intent and sentiment models provide small blocks of reference Python code, with the intent being that the user will adapt this reference code to the specific circumstances of the end-to-end appliction.\n",
    "\n",
    "For example, the code block below loads and runs the intent model using code adapted from the provided example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55f2dd0-169e-461e-b4c0-1b5c532214df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to eat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "\n",
    "# Load model and tokenizer\n",
    "intent_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "intent_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    INTENT_MODEL_NAME)\n",
    "\n",
    "# Example input\n",
    "INTENT_INPUT = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and beat you up, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference\n",
    "output = intent_model.generate(**features)\n",
    "\n",
    "# Postprocessing\n",
    "result_string = intent_tokenizer.decode(output[0])\n",
    "result_string = result_string.replace('<pad>', '')\n",
    "result_string = result_string[len(' '):-len('</s>')]\n",
    "\n",
    "result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb04ad3-dee3-4578-9e12-6a850f734452",
   "metadata": {},
   "source": [
    "The question answering and text generation models both use the Huggingface `tokenizers` library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines) to package their preprocessing and postprocesing code.\n",
    "The following code snippet demonstrates the process of loading and running the question answering model using this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17af4998-357f-420c-ab1a-61b82cfef905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278897904441692e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "\n",
    "# Loading the model and associated resources\n",
    "qa_pipeline = transformers.pipeline('question-answering',\n",
    "                                    model=QA_MODEL_NAME)\n",
    "\n",
    "# Example input\n",
    "# TODO: For the blog post, truncate the context string.\n",
    "QA_INPUT = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "# Preprocessing, inference, and postprocessing all happen in\n",
    "# the Python object's the __call__() method.\n",
    "qa_result = qa_pipeline(**QA_INPUT)\n",
    "\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1dca8a-a2ac-4d5f-b535-8aac6c0ca2c1",
   "metadata": {},
   "source": [
    "Unlike the \"example reference code\" approach, the Pipelines API's end-to-end inference code is intended for direct production use. It includes support for model retraining, as well as performance optimizations like batching and GPU acceleration, plus code for handling corner cases like long input strings. This prepackaged code can save a lot of time, provided that your application is structured in a way that can easily accomodate a large block of non-modifiable third-party Python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab98d8e-40a0-4c1b-a994-4fdb88756185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introducing `zerocopy`\n",
    "\n",
    "We've created a Python package, `zerocopy`, with the model rewrite code from our previous post (TODO: Publish the package to PyPI).\n",
    "\n",
    "To use that package, you'll need to install it with `pip`, then import it into your script.\n",
    "\n",
    "```python\n",
    "import zerocopy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82b4a9-5d9a-4b35-b818-627f2e7ea4fb",
   "metadata": {},
   "source": [
    "(TODO: Insert description of how `zero_copy` strips off the weights of a model\n",
    "and provides a way to reconsitute them from Plasma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a5406-f37d-4848-a0ca-27cd83360ff8",
   "metadata": {},
   "source": [
    "The low-level process works for all of our example models. Here it is in action with the intent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7add2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 13:02:19,376\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=18755)\u001b[0m 2022-03-03 13:02:20,995\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=18755)\u001b[0m 2022-03-03 13:02:21,099\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:yPcrNk:SERVE_PROXY_ACTOR-node:10.191.93.124-0' on node 'node:10.191.93.124-0' listening on '127.0.0.1:8000'\n",
      "2022-03-03 13:02:21,371\tINFO api.py:475 -- Started Serve instance in namespace 'fa2edb7e-a587-4a49-9704-9b6ee07aff5c'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=18740)\u001b[0m INFO:     Started server process [18740]\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog post.\n",
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()\n",
    "\n",
    "# Wait a moment to make sure that all log output goes to this cell\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ee503d-812c-4f37-bc1c-f677f15133d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result without zero-copy loading: tensor([[   0,   12,    3, 1544,    1]])\n",
      " Result *with* zero-copy loading: tensor([[   0,   12,    3, 1544,    1]])\n"
     ]
    }
   ],
   "source": [
    "# Recall that we loaded `intent_model` in an earlier cell.\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference without zero-copy loading\n",
    "print('Result without zero-copy loading: '\n",
    "      + str(intent_model.generate(**features)))\n",
    "\n",
    "# Inference with zero-copy loading\n",
    "intent_model_ref = ray.put(zerocopy.extract_tensors(intent_model))\n",
    "print(' Result *with* zero-copy loading: ' +\n",
    "      str(ray.get(zerocopy.call_model.remote(\n",
    "          intent_model_ref, [], features, 'generate'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b99817-8c69-4059-a4ee-743090798eab",
   "metadata": {
    "tags": []
   },
   "source": [
    "The time to invoke the rewritten model once is almost the same as running the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36429095-4c64-42bb-af7c-f58b5939a2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time to run locally: 384 ms ± 4.96 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Time to run with zero-copy: 402 ms ± 20.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit intent_model.generate(**features)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(intent_model_ref, [], features, 'generate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815d4b4-a22f-41b0-85db-4564c7531272",
   "metadata": {},
   "source": [
    "If we run inference multiple times, `zero_copy.call_model()` can send those inference requests to separate Ray tasks that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fec81f-f1c0-434e-abb7-2f4aacf5449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 50 times with zero-copy: 2.49 s ± 11.3 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "       Time to run 50 times locally: 19 s ± 65.1 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        intent_model.generate(**features)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [\n",
    "        zerocopy.call_model.remote(intent_model_ref, [], features, 'generate')\n",
    "        for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 50\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a09990-4188-47af-9c70-a57069cc7297",
   "metadata": {
    "tags": []
   },
   "source": [
    "The question answering and text generation models in our benchmark come packaged as `transformers` model pipelines. For convenience, the `zerocopy` library includes a function `rewrite_pipeline` that transforms any models embedded into Python object into Ray tasks that use zero-copy model loading to load weights. If we apply this function to a pipeline, the resulting rewritten pipeline faithfully performs all the preprocessing and postprocessing that the original pipeline performed. However, this rewritten pipeline runs the embedded PyTorch model in remote Ray tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10fd36e6-347b-422c-b558-bea0a0e9c5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before rewrite: {'score': 4.278897904441692e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      " After rewrite: {'score': 4.278897904441692e-06, 'start': 483, 'end': 484, 'answer': '5'}\n"
     ]
    }
   ],
   "source": [
    "zero_copy_qa = zerocopy.rewrite_pipeline(qa_pipeline)\n",
    "print(f\"Before rewrite: {qa_pipeline(**QA_INPUT)}\")\n",
    "print(f\" After rewrite: {zero_copy_qa(**QA_INPUT)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38195230-a2d2-4d77-8d41-090ecceef5c1",
   "metadata": {},
   "source": [
    "## Deploying Models with `zerocopy` and Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312b6318-ee14-4eef-b576-de4efe4f78de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 13:04:10,361\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=21101)\u001b[0m 2022-03-03 13:04:11,981\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=21101)\u001b[0m 2022-03-03 13:04:12,084\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:thnCLs:SERVE_PROXY_ACTOR-node:10.191.93.124-0' on node 'node:10.191.93.124-0' listening on '127.0.0.1:8000'\n",
      "2022-03-03 13:04:12,323\tINFO api.py:475 -- Started Serve instance in namespace 'd90e0fa1-488d-42c2-be53-98675d97da0f'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fe56c784f40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a7ef8-cda9-4ac2-bf97-b1d69985948d",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a379ad1-4902-4a33-8136-5343fccea287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=21098)\u001b[0m INFO:     Started server process [21098]\n"
     ]
    }
   ],
   "source": [
    "@serve.deployment\n",
    "class Intent:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    INTENT_MODEL_NAME)\n",
    "        \n",
    "        # Extract weights and load them onto the Plasma object store\n",
    "        self._model_ref = ray.put(zerocopy.extract_tensors(model))\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "        \n",
    "        # Preprocessing\n",
    "        input_text = f'{json_request[\"context\"]} </s>'\n",
    "        features = self._tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        output = await zerocopy.call_model.remote(\n",
    "            self._model_ref, [], features, 'generate')\n",
    "\n",
    "        # Postprocessing\n",
    "        result_string = self._tokenizer.decode(output[0])\n",
    "        result_string = result_string[len('<pad> '):-len('</s>')]\n",
    "        return {\n",
    "            'intent': result_string\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee7aa8-3640-4795-b32d-c7c447014182",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8db55e-0384-4973-bf2d-351e9d61015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class QA:\n",
    "    def __init__(self):\n",
    "        # Load the pipeline and move the model's weights onto the\n",
    "        # Plasma object store.\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            transformers.pipeline('question-answering', \n",
    "                                  model=QA_MODEL_NAME))\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # The original `transformers` code is not async-aware, so we\n",
    "        # call it from `run_in_executor()`\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(**json_request))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cde0b-fbd8-4df5-8bf1-5ea74a8fafe8",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de42944d-e1d0-4c56-bc77-583a2c3137fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 13:04:23,271\tINFO api.py:249 -- Updating deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=21101)\u001b[0m 2022-03-03 13:04:23,366\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(intent_en pid=21086)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "2022-03-03 13:04:37,901\tINFO api.py:261 -- Deployment 'intent_en' is ready at `http://127.0.0.1:8000/intent_en`. component=serve deployment=intent_en\n",
      "2022-03-03 13:04:37,910\tINFO api.py:249 -- Updating deployment 'qa_en'. component=serve deployment=qa_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=21101)\u001b[0m 2022-03-03 13:04:38,003\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_en'. component=serve deployment=qa_en\n",
      "2022-03-03 13:04:46,197\tINFO api.py:261 -- Deployment 'qa_en' is ready at `http://127.0.0.1:8000/qa_en`. component=serve deployment=qa_en\n"
     ]
    }
   ],
   "source": [
    "Intent.options(name='intent_en', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "QA.options(name='qa_en', ray_actor_options={\"num_cpus\": 0.1}).deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add17b9-3994-4c81-8b4d-9fe16bf80317",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf8d4ef7-4dab-4511-bc77-05231a1725bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat'}\n",
      "Question answering result: {'score': 4.278897904441692e-06, 'start': 483, 'end': 484, 'answer': '5'}\n"
     ]
    }
   ],
   "source": [
    "intent_result = requests.put(\n",
    "    'http://127.0.0.1:8000/intent_en',\n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    'http://127.0.0.1:8000/qa_en',\n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4614552-0203-4ecc-a7c3-986c78c8c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=21101)\u001b[0m 2022-03-03 13:04:50,708\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=21101)\u001b[0m 2022-03-03 13:04:50,711\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_en'. component=serve deployment=qa_en\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Stop this notebook's copy of Ray so as not to interfere with the\n",
    "# copy in `ray_deploy.ipynb`\n",
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b1aa",
   "metadata": {},
   "source": [
    "## Baseline implementation\n",
    "\n",
    "Our baseline implementation of the model serving backend for our benchmark emulates running each model in a separate container. We used [TorchServe](https://pytorch.org/serve/) as our model serving framework for the baseline deployment. By configuing TorchServe to use a pool of processes, we were able to simulate running each model in a separate container without having to set up a dedicated Kubernetes cluster. See [this notebook](./torchserve.ipynb) for details of the TorchServe deployment.\n",
    "\n",
    "*Note that earlier versions of this notebook implemented the baseline model deployment with a pool of Ray actors. That older version is preserved in [a separate notebook](./ray_baseline.ipynb).*\n",
    "\n",
    "With TorchServe running in the background, we can invoke our models via their REST APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3025582b-6588-42c8-ba47-545903b3511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'modelName': 'generate_en', 'modelUrl': 'generate.mar'}, {'modelName': 'generate_es', 'modelUrl': 'generate.mar'}, {'modelName': 'generate_zh', 'modelUrl': 'generate.mar'}, {'modelName': 'intent_en', 'modelUrl': 'intent.mar'}, {'modelName': 'intent_es', 'modelUrl': 'intent.mar'}, {'modelName': 'intent_zh', 'modelUrl': 'intent.mar'}, {'modelName': 'qa_en', 'modelUrl': 'qa.mar'}, {'modelName': 'qa_es', 'modelUrl': 'qa.mar'}, {'modelName': 'qa_zh', 'modelUrl': 'qa.mar'}, {'modelName': 'sentiment_en', 'modelUrl': 'sentiment.mar'}, {'modelName': 'sentiment_es', 'modelUrl': 'sentiment.mar'}, {'modelName': 'sentiment_zh', 'modelUrl': 'sentiment.mar'}]}\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the management API to verify that TorchServe is running.\n",
    "try:\n",
    "    print(requests.get('http://127.0.0.1:8081/models').json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Stop notebook execution\n",
    "    raise ValueError('TorchServe does not appear to be running. Please start TorchServe.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c011ecef-438d-458d-a542-90118b7d36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat chips'}\n",
      "Sentiment result: {'positive': 0.5419480204582214, 'neutral': 0.38251084089279175, 'negative': 0.07554131746292114}\n",
      "Question answering result: {'score': 4.278918822819833e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      "Natural language generation result: [{'generated_text': \"All your base are going to have to decide at what place. If your base is located in the southeast corner with some cliffs, just look out the window on your way down. If it's in the southwest corner of the map, wait until it\"}]\n"
     ]
    }
   ],
   "source": [
    "TORCHSERVE_PORT = 8080\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}\n",
    "\n",
    "intent_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/intent_en',\n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/sentiment_en',\n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/qa_en',\n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/generate_en',\n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4316-86af-481f-8cc0-887a88c30c0d",
   "metadata": {},
   "source": [
    "## The Benchmark\n",
    "\n",
    "Now that we have deployed each of our models with a web service front end, we can define a benchmark that sends inference traffic to these web service endpoints and measures response time.\n",
    "\n",
    "We start by wrapping all the web services in a single callback function that calls a model, retrieves the result, verifies the result, and returns elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "079ad698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we have a single canned input for each model type.\n",
    "MODEL_INPUTS = {\n",
    "    'intent': INTENT_INPUT,\n",
    "    'sentiment': SENTIMENT_INPUT,\n",
    "    'qa': QA_INPUT,\n",
    "    'generate': GENERATE_INPUT\n",
    "}\n",
    "\n",
    "LANGUAGES = ['en', 'es', 'zh']\n",
    "\n",
    "\n",
    "MODEL_TYPES = list(MODEL_INPUTS.keys())\n",
    "\n",
    "\n",
    "def call_model(model_type: str, language: str, port: int,\n",
    "               timeout_sec: float = 5.0) \\\n",
    "        -> Tuple[int, float, float]:\n",
    "    '''\n",
    "    Callack function that calls the model deployment, retrieves and\n",
    "    validates the result, and returns elapsed time.\n",
    "\n",
    "    :param model_type: Type of model to call; must be one of\n",
    "                       'intent', 'sentiment', 'qa', or 'generate'\n",
    "    :param language: Two-letter language code; must be one of\n",
    "                     'en', 'es', 'zh'\n",
    "    :param port: Port on which the local REST API is listening.\n",
    "    :param timeout_sec: Request timeout, in seconds.\n",
    "\n",
    "    :returns: Tuple of HTTP result code and start and end times \n",
    "              of the web service call. If a client-side timeout\n",
    "              happens, the result code will be 408 (request timeout)\n",
    "    '''\n",
    "    if model_type not in MODEL_TYPES:\n",
    "        raise ValueError(f'Unexpected model type \"{model_type}\" '\n",
    "                         f'(expected {MODEL_TYPES}')\n",
    "    if language not in LANGUAGES:\n",
    "        raise ValueError(f'Unexpected language code \"{language}\" '\n",
    "                         f'(expected {LANGUAGES}')\n",
    "\n",
    "    # For now, use the same input every time\n",
    "    model_input = MODEL_INPUTS[model_type]\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = requests.put(\n",
    "            f'http://127.0.0.1:{port}/predictions/{model_type}_{language}',\n",
    "            json.dumps(model_input),\n",
    "            timeout=timeout_sec)\n",
    "        end_time = time.time()\n",
    "        status_code = result.status_code\n",
    "    except requests.exceptions.Timeout:\n",
    "        end_time = start_time + timeout_sec\n",
    "        status_code = 408  # HTTP/408 Request Timeout\n",
    "\n",
    "    return (status_code, start_time, end_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afe47e03-83d6-416a-a71f-92358202e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"intent\" model takes 0.212 seconds.\n",
      "The \"sentiment\" model takes 0.044 seconds.\n",
      "The \"qa\" model takes 0.118 seconds.\n",
      "The \"generate\" model takes 1.446 seconds.\n"
     ]
    }
   ],
   "source": [
    "TORCHSERVE_PORT = 8080\n",
    "\n",
    "# Test with each model type\n",
    "for model_type in MODEL_INPUTS.keys():\n",
    "    times = call_model(model_type, 'en', TORCHSERVE_PORT)\n",
    "    print(f'The \"{model_type}\" model takes {times[2] - times[1]:1.3f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92a1f5-3624-4f22-8a95-0b9d9c2f32d6",
   "metadata": {},
   "source": [
    "Our benchmark generates a trace of requests, then plays back the trace and measures the \n",
    "latency of each request. \n",
    "\n",
    "The request rate changes each second, with the rate of a particular 1-second window drawn from the Poisson\n",
    "distribution. Here's the code to generate the start times for the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "110ace1f-9c9d-4b55-a50b-9c20d2d9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_start_times(requests_per_sec: float, num_sec: int,\n",
    "                    seed: int) -> np.ndarray:\n",
    "    '''\n",
    "    Generate a trace of inference request start times. Divides the trace\n",
    "    into 1-second intervals. Each interval gets a number of requests drawn\n",
    "    from a Poissson distribution. These requests are evenly spread through the\n",
    "    interval.\n",
    "\n",
    "    :param requests_per_sec: Average requests per second overall\n",
    "    :param num_sec: Number of seconds of trace to generate\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Numpy array of timestamps (starting from 0) for the requests\n",
    "     in the trace\n",
    "    '''\n",
    "    trace = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Compute the number of requests in each 1-second window.\n",
    "    req_per_window = rng.poisson(requests_per_sec, size=num_sec)\n",
    "\n",
    "    for window_num in range(num_sec):\n",
    "        num_requests = req_per_window[window_num]\n",
    "        if num_requests > 0:\n",
    "            request_interval = 1.0 / num_requests\n",
    "            for i in range(num_requests):\n",
    "                trace.append(window_num + request_interval * i)\n",
    "\n",
    "    return np.array(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c681f9-e06e-4da8-a6e8-08ae5e54fa85",
   "metadata": {},
   "source": [
    "Each request goes to a randomly-selected model. The choice of models is\n",
    "weighted according to a truncated Poisson distribution. Here's the code to generate\n",
    "the list of model IDs for the requests in the trace. When we play back the trace,\n",
    "we'll map each integer model ID to a combination of a language code and a model type\n",
    "--- for example, `('en', 'sentiment')` for the English sentiment model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "451f1281-6c46-45e9-b397-5d8898717e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en', 'intent'),\n",
       " ('en', 'sentiment'),\n",
       " ('en', 'qa'),\n",
       " ('en', 'generate'),\n",
       " ('es', 'intent'),\n",
       " ('es', 'sentiment'),\n",
       " ('es', 'qa'),\n",
       " ('es', 'generate'),\n",
       " ('zh', 'intent'),\n",
       " ('zh', 'sentiment'),\n",
       " ('zh', 'qa'),\n",
       " ('zh', 'generate')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_model_ids(lambda_: float, num_models: int, num_points: int,\n",
    "                  seed: int) -> np.ndarray:\n",
    "    '''\n",
    "    Draw integer model IDs at random from a truncated Poisson distribution.\n",
    "\n",
    "    :param lambda_: Primary parameter of the distribution, which also happens to \n",
    "     be the mean value of the (untruncated) distribution.\n",
    "    :param num_models: Number of models; generated IDs will range from 0 to\n",
    "                       `num_models - 1`, inclusive.\n",
    "    :param num_points: Number of random model IDs to return.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Randomly generated model IDs for a series of requests, as a\n",
    "     1D Numpy array of integers.\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Draw integers from a truncated Poisson distribution. Start with a \n",
    "    # non-truncated distribution, then resample for\n",
    "    # any values that went over the limit.\n",
    "    int_ids = rng.poisson(lambda_, size=num_points)\n",
    "    while np.any(int_ids >= num_models):\n",
    "        new_values = rng.poisson(lambda_, size=np.sum(int_ids >= num_models))\n",
    "        int_ids[int_ids >= num_models] = new_values\n",
    "    return int_ids\n",
    "\n",
    "\n",
    "# Map the integer model IDs from the trace to pairs of language code and\n",
    "# model type.\n",
    "MODEL_ID_TO_PARAMS = [\n",
    "    (lang_code, model_name)\n",
    "    for lang_code in LANGUAGES\n",
    "    for model_name in MODEL_TYPES\n",
    "]\n",
    "\n",
    "\n",
    "MODEL_ID_TO_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61453629-426e-4b09-8430-4b2c11b71a64",
   "metadata": {},
   "source": [
    "The benchmark itself generates and then plays back the trace, measuring the end-to-end latency of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfa55ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>model_type</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>result_code</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.106287</td>\n",
       "      <td>200</td>\n",
       "      <td>0.105009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125134</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>200</td>\n",
       "      <td>0.027239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250084</td>\n",
       "      <td>0.624799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.374714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.385401</td>\n",
       "      <td>0.655882</td>\n",
       "      <td>200</td>\n",
       "      <td>0.270481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.543638</td>\n",
       "      <td>0.786676</td>\n",
       "      <td>200</td>\n",
       "      <td>0.243038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625343</td>\n",
       "      <td>0.950615</td>\n",
       "      <td>200</td>\n",
       "      <td>0.325272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750379</td>\n",
       "      <td>1.445074</td>\n",
       "      <td>200</td>\n",
       "      <td>0.694695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875465</td>\n",
       "      <td>0.922473</td>\n",
       "      <td>200</td>\n",
       "      <td>0.047009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000457</td>\n",
       "      <td>1.308138</td>\n",
       "      <td>200</td>\n",
       "      <td>0.307681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.125372</td>\n",
       "      <td>1.691211</td>\n",
       "      <td>200</td>\n",
       "      <td>0.565839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.263157</td>\n",
       "      <td>2.792700</td>\n",
       "      <td>200</td>\n",
       "      <td>1.529543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.375377</td>\n",
       "      <td>3.116737</td>\n",
       "      <td>200</td>\n",
       "      <td>1.741359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500417</td>\n",
       "      <td>1.552153</td>\n",
       "      <td>200</td>\n",
       "      <td>0.051736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.625433</td>\n",
       "      <td>1.677866</td>\n",
       "      <td>200</td>\n",
       "      <td>0.052434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.750408</td>\n",
       "      <td>2.408754</td>\n",
       "      <td>200</td>\n",
       "      <td>0.658346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.875379</td>\n",
       "      <td>3.257088</td>\n",
       "      <td>200</td>\n",
       "      <td>1.381710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.001417</td>\n",
       "      <td>3.384329</td>\n",
       "      <td>200</td>\n",
       "      <td>1.382911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>2.116125</td>\n",
       "      <td>3.513872</td>\n",
       "      <td>200</td>\n",
       "      <td>1.397747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>2.222755</td>\n",
       "      <td>3.664476</td>\n",
       "      <td>200</td>\n",
       "      <td>1.441721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.335371</td>\n",
       "      <td>4.877291</td>\n",
       "      <td>200</td>\n",
       "      <td>2.541921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>2.444898</td>\n",
       "      <td>3.002291</td>\n",
       "      <td>200</td>\n",
       "      <td>0.557393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>2.556283</td>\n",
       "      <td>5.149932</td>\n",
       "      <td>200</td>\n",
       "      <td>2.593649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.667228</td>\n",
       "      <td>5.276132</td>\n",
       "      <td>200</td>\n",
       "      <td>2.608904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>2.782933</td>\n",
       "      <td>5.401476</td>\n",
       "      <td>200</td>\n",
       "      <td>2.618543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>2.889338</td>\n",
       "      <td>5.533724</td>\n",
       "      <td>200</td>\n",
       "      <td>2.644386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.002340</td>\n",
       "      <td>5.733261</td>\n",
       "      <td>200</td>\n",
       "      <td>2.730922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>3.143378</td>\n",
       "      <td>5.878153</td>\n",
       "      <td>200</td>\n",
       "      <td>2.734775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>3.286219</td>\n",
       "      <td>6.009294</td>\n",
       "      <td>200</td>\n",
       "      <td>2.723074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>3.429020</td>\n",
       "      <td>6.154443</td>\n",
       "      <td>200</td>\n",
       "      <td>2.725422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>3.572015</td>\n",
       "      <td>6.289900</td>\n",
       "      <td>200</td>\n",
       "      <td>2.717885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.714788</td>\n",
       "      <td>4.533993</td>\n",
       "      <td>200</td>\n",
       "      <td>0.819205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857588</td>\n",
       "      <td>6.417437</td>\n",
       "      <td>200</td>\n",
       "      <td>2.559849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.013794</td>\n",
       "      <td>4.999021</td>\n",
       "      <td>200</td>\n",
       "      <td>0.985227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.167862</td>\n",
       "      <td>6.612769</td>\n",
       "      <td>200</td>\n",
       "      <td>2.444907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.343234</td>\n",
       "      <td>6.800234</td>\n",
       "      <td>200</td>\n",
       "      <td>2.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.501029</td>\n",
       "      <td>6.932409</td>\n",
       "      <td>200</td>\n",
       "      <td>2.431379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.667151</td>\n",
       "      <td>7.080470</td>\n",
       "      <td>200</td>\n",
       "      <td>2.413319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>4.836485</td>\n",
       "      <td>7.210445</td>\n",
       "      <td>200</td>\n",
       "      <td>2.373960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    request_id  model_num lang_code model_type  desired_start  actual_start  \\\n",
       "0            0          1        en  sentiment       0.000000      0.001278   \n",
       "1            1          1        en  sentiment       0.125000      0.125134   \n",
       "2            2          0        en     intent       0.250000      0.250084   \n",
       "3            3          2        en         qa       0.375000      0.385401   \n",
       "4            4          0        en     intent       0.500000      0.543638   \n",
       "5            5          0        en     intent       0.625000      0.625343   \n",
       "6            6          0        en     intent       0.750000      0.750379   \n",
       "7            7          1        en  sentiment       0.875000      0.875465   \n",
       "8            8          1        en  sentiment       1.000000      1.000457   \n",
       "9            9          0        en     intent       1.125000      1.125372   \n",
       "10          10          0        en     intent       1.250000      1.263157   \n",
       "11          11          0        en     intent       1.375000      1.375377   \n",
       "12          12          1        en  sentiment       1.500000      1.500417   \n",
       "13          13          1        en  sentiment       1.625000      1.625433   \n",
       "14          14          2        en         qa       1.750000      1.750408   \n",
       "15          15          0        en     intent       1.875000      1.875379   \n",
       "16          16          0        en     intent       2.000000      2.001417   \n",
       "17          17          0        en     intent       2.111111      2.116125   \n",
       "18          18          0        en     intent       2.222222      2.222755   \n",
       "19          19          0        en     intent       2.333333      2.335371   \n",
       "20          20          1        en  sentiment       2.444444      2.444898   \n",
       "21          21          0        en     intent       2.555556      2.556283   \n",
       "22          22          0        en     intent       2.666667      2.667228   \n",
       "23          23          0        en     intent       2.777778      2.782933   \n",
       "24          24          0        en     intent       2.888889      2.889338   \n",
       "25          25          0        en     intent       3.000000      3.002340   \n",
       "26          26          0        en     intent       3.142857      3.143378   \n",
       "27          27          0        en     intent       3.285714      3.286219   \n",
       "28          28          0        en     intent       3.428571      3.429020   \n",
       "29          29          0        en     intent       3.571429      3.572015   \n",
       "30          30          1        en  sentiment       3.714286      3.714788   \n",
       "31          31          0        en     intent       3.857143      3.857588   \n",
       "32          32          1        en  sentiment       4.000000      4.013794   \n",
       "33          33          0        en     intent       4.166667      4.167862   \n",
       "34          34          0        en     intent       4.333333      4.343234   \n",
       "35          35          0        en     intent       4.500000      4.501029   \n",
       "36          36          0        en     intent       4.666667      4.667151   \n",
       "37          37          0        en     intent       4.833333      4.836485   \n",
       "\n",
       "         end  result_code   latency  \n",
       "0   0.106287          200  0.105009  \n",
       "1   0.152373          200  0.027239  \n",
       "2   0.624799          200  0.374714  \n",
       "3   0.655882          200  0.270481  \n",
       "4   0.786676          200  0.243038  \n",
       "5   0.950615          200  0.325272  \n",
       "6   1.445074          200  0.694695  \n",
       "7   0.922473          200  0.047009  \n",
       "8   1.308138          200  0.307681  \n",
       "9   1.691211          200  0.565839  \n",
       "10  2.792700          200  1.529543  \n",
       "11  3.116737          200  1.741359  \n",
       "12  1.552153          200  0.051736  \n",
       "13  1.677866          200  0.052434  \n",
       "14  2.408754          200  0.658346  \n",
       "15  3.257088          200  1.381710  \n",
       "16  3.384329          200  1.382911  \n",
       "17  3.513872          200  1.397747  \n",
       "18  3.664476          200  1.441721  \n",
       "19  4.877291          200  2.541921  \n",
       "20  3.002291          200  0.557393  \n",
       "21  5.149932          200  2.593649  \n",
       "22  5.276132          200  2.608904  \n",
       "23  5.401476          200  2.618543  \n",
       "24  5.533724          200  2.644386  \n",
       "25  5.733261          200  2.730922  \n",
       "26  5.878153          200  2.734775  \n",
       "27  6.009294          200  2.723074  \n",
       "28  6.154443          200  2.725422  \n",
       "29  6.289900          200  2.717885  \n",
       "30  4.533993          200  0.819205  \n",
       "31  6.417437          200  2.559849  \n",
       "32  4.999021          200  0.985227  \n",
       "33  6.612769          200  2.444907  \n",
       "34  6.800234          200  2.457000  \n",
       "35  6.932409          200  2.431379  \n",
       "36  7.080470          200  2.413319  \n",
       "37  7.210445          200  2.373960  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_single_benchmark(\n",
    "        model_callback: Callable, \n",
    "        requests_per_sec: float,\n",
    "        num_sec: int,\n",
    "        model_id_to_params: List[Tuple[str, str]],\n",
    "        model_lambda: float = 0.3,\n",
    "        seed: int = 42) -> pd.DataFrame:\n",
    "    '''\n",
    "    A single run of the benchmark.\n",
    "\n",
    "    Sends a stream of requests to multiple models, with the rate varying\n",
    "    according to a Poisson distribution and division of traffic among models\n",
    "    following a truncated Poisson distribution.\n",
    "\n",
    "    :param model_callback: Thread-safe callback function that makes a \n",
    "                           single request and returns a tuple of\n",
    "                           ``(result code, start time, end time)``.\n",
    "                           Should have the signature\n",
    "                           `f(model_type: str, language: str)`\n",
    "    :param request_per_sec: Mean of the Poisson distribution that determines\n",
    "     the number of requests in each 1-second window.\n",
    "    :param num_sec: Seconds of traffic to generate at the requested rate.\n",
    "     The actual session will extend past this window until all open requests\n",
    "     have finished.\n",
    "    :param model_lambda: Primary parameter of the truncated Poisson\n",
    "     distribution used to split requests among models. Approximately\n",
    "     equal to the mean of the distribution. The default value of 0.3 sends\n",
    "     70% of traffic to model 0.\n",
    "    :param model_id_to_params: List that maps integer model ID to a tuple of \n",
    "     (language code, model name) for each of the models.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: DataFrame of benchmark results at per-request granularity\n",
    "    '''\n",
    "    # Preallocate the trace as a set of lists.\n",
    "    benchmark_start_time = time.time()\n",
    "    desired_start_times = (\n",
    "        gen_start_times(requests_per_sec, num_sec, seed)\n",
    "        + benchmark_start_time)\n",
    "    num_requests = desired_start_times.shape[0]\n",
    "    model_nums = gen_model_ids(model_lambda, len(model_id_to_params),\n",
    "                               num_requests, seed)\n",
    "    language_codes = [model_id_to_params[num][0] for num in model_nums]\n",
    "    model_types = [model_id_to_params[num][1] for num in model_nums]\n",
    "    actual_start_times = [None] * num_requests\n",
    "    end_times = [None] * num_requests\n",
    "    result_codes = [None] * num_requests\n",
    "\n",
    "    # Because some notebook servers (i.e. VSCode) don't play well with\n",
    "    # asyncio, we use threads to manage concurrent requests.\n",
    "    thread_pool = concurrent.futures.ThreadPoolExecutor(1000)\n",
    "\n",
    "    # Map from request object to request number\n",
    "    active_requests = {}  # type: Dict[concurrent.futures.Future, int]\n",
    "\n",
    "    # Main event loop: Spawn background requests, get their responses.\n",
    "    request_num = 0\n",
    "    while request_num < num_requests or len(active_requests) > 0:\n",
    "        sec_to_next = (\n",
    "            1.0 if request_num >= num_requests\n",
    "            else desired_start_times[request_num] - time.time()\n",
    "        )\n",
    "        if sec_to_next <= 0:\n",
    "            # Time to send the next request\n",
    "            lang_code = language_codes[request_num]\n",
    "            model_type = model_types[request_num]\n",
    "            future = thread_pool.submit(\n",
    "                model_callback, model_type, lang_code)\n",
    "            active_requests[future] = request_num\n",
    "            request_num += 1\n",
    "        else:\n",
    "            # Block until it's time to send the next request or a previous\n",
    "            # request is done.\n",
    "            ready_set, _ = concurrent.futures.wait(\n",
    "                list(active_requests.keys()),\n",
    "                timeout=sec_to_next)\n",
    "\n",
    "            # Record timings from any open requests that have completed.\n",
    "            for future in ready_set:\n",
    "                request_id = active_requests.pop(future)\n",
    "                result_code, start_time, end_time = future.result()\n",
    "                actual_start_times[request_id] = start_time\n",
    "                end_times[request_id] = end_time\n",
    "                result_codes[request_id] = result_code\n",
    "\n",
    "    # Collate results as a DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        'request_id': range(num_requests),\n",
    "        'model_num': model_nums,\n",
    "        'lang_code': language_codes,\n",
    "        'model_type': model_types,\n",
    "        'desired_start': desired_start_times,\n",
    "        'actual_start': actual_start_times,\n",
    "        'end': end_times,\n",
    "        'result_code': result_codes\n",
    "    })\n",
    "\n",
    "    # Make all times relative to start of the trace\n",
    "    for key in (\"desired_start\", \"actual_start\", \"end\"):\n",
    "        result[key] -= benchmark_start_time\n",
    "    result[\"latency\"] = result[\"end\"] - result[\"actual_start\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def call_torchserve(model_type: str, language: str):\n",
    "    return call_model(model_type, language, TORCHSERVE_PORT)\n",
    "\n",
    "\n",
    "# Quick test run\n",
    "run_single_benchmark(call_torchserve, 6, 5, MODEL_ID_TO_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5047d",
   "metadata": {},
   "source": [
    "Now we can define a top-level function that runs the entire benchmark, gradually ramping up the average request rate of the bursty traffic until requests start timing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccb1d361-41b6-481c-a770-6f802d1b782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_TIME_SEC = 60\n",
    "\n",
    "def run_benchmarks(\n",
    "        model_callback: Callable, \n",
    "        num_sec: int = 60,\n",
    "        min_request_rate: int = 2,\n",
    "        request_rate_step: float = 0.5,\n",
    "        max_failure_fraction: float = 0.9) -> pd.DataFrame:\n",
    "    '''\n",
    "    Perform multiple runs of the benchmark, increasing the request\n",
    "    rate gradually until requests start returning errors.\n",
    "\n",
    "    :param num_sec: Seconds of traffic to generate for each run.\n",
    "                    The actual session will extend past this window \n",
    "                    until all open requests have finished.\n",
    "    :param min_request_rate: Mean request rate for the first run of the\n",
    "                             benchmark.\n",
    "                             The actual request rate will follow a Poisson \n",
    "                             distribution with this mean.\n",
    "    :param request_rate_step: Amount by which the request rate increases\n",
    "                              with each subsequent run of the benchmark,\n",
    "                              in requests per second.\n",
    "    :param max_failure_fraction: What fraction of failed web service calls \n",
    "                                 the benchmark will tolerate per run before \n",
    "                                 stopping the overall process.\n",
    "\n",
    "    :returns: A Pandas DataFrame of detailed timings for all web service\n",
    "              requests. The column ``request_rate`` tells which run of the\n",
    "              benchmark each request belongs to.\n",
    "    '''\n",
    "    to_concat = []\n",
    "    request_rate = min_request_rate\n",
    "    failure_fraction = 0.\n",
    "\n",
    "    while failure_fraction <= max_failure_fraction:\n",
    "        print(f'Running at {request_rate} requests/sec.', end='')\n",
    "        times = run_single_benchmark(model_callback, \n",
    "                                     request_rate, num_sec,\n",
    "                                     MODEL_ID_TO_PARAMS)\n",
    "        times.insert(0, 'request_rate', request_rate)\n",
    "        to_concat.append(times)\n",
    "        num_failures = sum(times['result_code'] != 200)\n",
    "        num_requests = len(times.index)\n",
    "        failure_fraction = num_failures / num_requests\n",
    "        print(f' => {failure_fraction * 100.:0.1f}% failure rate')\n",
    "        request_rate += request_rate_step\n",
    "\n",
    "    print(f'Stopping due to fraction of failures ({failure_fraction}) '\n",
    "          f'exceeding allowable limit ({max_failure_fraction})')\n",
    "    return pd.concat(to_concat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403c8b3-7dda-4c32-86ec-1ba0eee6be34",
   "metadata": {},
   "source": [
    "### Baseline benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "510f535f-12a8-4104-a9c1-ea4c0a24936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at 2 requests/sec. => 0.0% failure rate\n",
      "Running at 2.5 requests/sec. => 0.0% failure rate\n",
      "Running at 3.0 requests/sec. => 0.0% failure rate\n",
      "Running at 3.5 requests/sec. => 1.4% failure rate\n",
      "Running at 4.0 requests/sec. => 10.1% failure rate\n",
      "Running at 4.5 requests/sec. => 28.2% failure rate\n",
      "Running at 5.0 requests/sec. => 92.1% failure rate\n",
      "Stopping due to fraction of failures (0.9207920792079208) exceeding allowable limit (0.9)\n"
     ]
    }
   ],
   "source": [
    "results = run_benchmarks(call_torchserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e96396d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_rate</th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>model_type</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>result_code</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>200</td>\n",
       "      <td>0.059999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166812</td>\n",
       "      <td>0.195527</td>\n",
       "      <td>200</td>\n",
       "      <td>0.028715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333495</td>\n",
       "      <td>0.469570</td>\n",
       "      <td>200</td>\n",
       "      <td>0.136075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500121</td>\n",
       "      <td>0.671306</td>\n",
       "      <td>200</td>\n",
       "      <td>0.171185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.669406</td>\n",
       "      <td>0.830529</td>\n",
       "      <td>200</td>\n",
       "      <td>0.161123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833464</td>\n",
       "      <td>0.993221</td>\n",
       "      <td>200</td>\n",
       "      <td>0.159757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000126</td>\n",
       "      <td>1.188344</td>\n",
       "      <td>200</td>\n",
       "      <td>0.188218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333497</td>\n",
       "      <td>1.452331</td>\n",
       "      <td>200</td>\n",
       "      <td>0.118834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.666864</td>\n",
       "      <td>1.696764</td>\n",
       "      <td>200</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000190</td>\n",
       "      <td>2.196562</td>\n",
       "      <td>200</td>\n",
       "      <td>0.196372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   request_rate  request_id  model_num lang_code model_type  desired_start  \\\n",
       "0           4.0           0          1        en  sentiment       0.000000   \n",
       "1           4.0           1          1        en  sentiment       0.166667   \n",
       "2           4.0           2          0        en     intent       0.333333   \n",
       "3           4.0           3          2        en         qa       0.500000   \n",
       "4           4.0           4          0        en     intent       0.666667   \n",
       "5           4.0           5          0        en     intent       0.833333   \n",
       "6           4.0           6          0        en     intent       1.000000   \n",
       "7           4.0           7          1        en  sentiment       1.333333   \n",
       "8           4.0           8          1        en  sentiment       1.666667   \n",
       "9           4.0           9          0        en     intent       2.000000   \n",
       "\n",
       "   actual_start       end  result_code   latency  \n",
       "0      0.001476  0.061475          200  0.059999  \n",
       "1      0.166812  0.195527          200  0.028715  \n",
       "2      0.333495  0.469570          200  0.136075  \n",
       "3      0.500121  0.671306          200  0.171185  \n",
       "4      0.669406  0.830529          200  0.161123  \n",
       "5      0.833464  0.993221          200  0.159757  \n",
       "6      1.000126  1.188344          200  0.188218  \n",
       "7      1.333497  1.452331          200  0.118834  \n",
       "8      1.666864  1.696764          200  0.029900  \n",
       "9      2.000190  2.196562          200  0.196372  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some example outputs\n",
    "results[results['request_rate'] == 4].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4f8696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the successful requests from the timeouts and compute aggregates\n",
    "def compute_stats(results_df: pd.DataFrame) -> pd.DataFrame: \n",
    "    timeout_results = results_df[results_df['result_code'] != 200]\n",
    "    success_results = results_df[results_df['result_code'] == 200]\n",
    "\n",
    "    timeout_counts = (\n",
    "        timeout_results\n",
    "        .groupby('request_rate')\n",
    "        .aggregate({'request_id': 'count'})\n",
    "        .rename(columns={'request_id': 'timeouts'}))\n",
    "    stats = (\n",
    "        success_results\n",
    "        .groupby('request_rate')\n",
    "        .aggregate({'latency': ['mean', 'median', 'max'],\n",
    "                    'request_id': 'count'}))\n",
    "\n",
    "    # Column names come out from the aggregations all messed up\n",
    "    stats.columns=['mean', 'median', 'max', 'successes']\n",
    "    stats = stats.join(timeout_counts).fillna(0)\n",
    "    stats['timeout_fraction'] = stats['timeouts'] / (stats['successes'] + stats['timeouts'])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d9f5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>successes</th>\n",
       "      <th>timeouts</th>\n",
       "      <th>timeout_fraction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request_rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.140433</td>\n",
       "      <td>0.136992</td>\n",
       "      <td>0.403386</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5</th>\n",
       "      <td>0.151730</td>\n",
       "      <td>0.141906</td>\n",
       "      <td>0.773904</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.141435</td>\n",
       "      <td>0.135038</td>\n",
       "      <td>0.929305</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.5</th>\n",
       "      <td>0.923068</td>\n",
       "      <td>0.218088</td>\n",
       "      <td>4.840252</td>\n",
       "      <td>206</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.014354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>1.021542</td>\n",
       "      <td>0.228379</td>\n",
       "      <td>4.972044</td>\n",
       "      <td>204</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.101322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.5</th>\n",
       "      <td>1.382455</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>4.996696</td>\n",
       "      <td>186</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.281853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>2.892508</td>\n",
       "      <td>3.210044</td>\n",
       "      <td>5.004417</td>\n",
       "      <td>24</td>\n",
       "      <td>279.0</td>\n",
       "      <td>0.920792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean    median       max  successes  timeouts  \\\n",
       "request_rate                                                      \n",
       "2.0           0.140433  0.136992  0.403386        109       0.0   \n",
       "2.5           0.151730  0.141906  0.773904        147       0.0   \n",
       "3.0           0.141435  0.135038  0.929305        183       0.0   \n",
       "3.5           0.923068  0.218088  4.840252        206       3.0   \n",
       "4.0           1.021542  0.228379  4.972044        204      23.0   \n",
       "4.5           1.382455  0.815284  4.996696        186      73.0   \n",
       "5.0           2.892508  3.210044  5.004417         24     279.0   \n",
       "\n",
       "              timeout_fraction  \n",
       "request_rate                    \n",
       "2.0                   0.000000  \n",
       "2.5                   0.000000  \n",
       "3.0                   0.000000  \n",
       "3.5                   0.014354  \n",
       "4.0                   0.101322  \n",
       "4.5                   0.281853  \n",
       "5.0                   0.920792  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_stats = compute_stats(results)\n",
    "baseline_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9496843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Timeout Fraction')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partial plot. Don't include this plot in the blog.\n",
    "plt.scatter(baseline_stats.index, baseline_stats['timeout_fraction'])\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Timeout Fraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c948be2-7759-4ee1-866c-ef52df1cd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Make sure that TorchServe is shut down before we continue.\n",
    "torchserve_is_running = True\n",
    "try:\n",
    "    requests.get('http://127.0.0.1:8081/models').json()\n",
    "except requests.exceptions.ConnectionError:\n",
    "    torchserve_is_running = False\n",
    "if torchserve_is_running:\n",
    "    raise ValueError('Please shut down TorchServe before continuing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87735cb-441e-46c2-95d4-89179e84a1d1",
   "metadata": {},
   "source": [
    "### Benchmark run with zero-copy model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71e2ec-77c9-4ef1-abea-dc3fad338794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the Ray models are up\n",
    "try:\n",
    "    requests.put(\n",
    "        'http://127.0.0.1:8000/predictions/intent_en', \n",
    "        json.dumps(INTENT_INPUT)).json()\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    raise ValueError('Please start up the zero-copy model deployment before continuing.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6ede3-e8e4-4ea9-8788-bbaf07187af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same benchmark, but pointed at our zero-copy implementation.\n",
    "def call_ray(model_type: str, language: str):\n",
    "    return call_model(model_type, language, 8000)\n",
    "\n",
    "results_zerocopy = run_benchmarks(call_ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e417cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zerocopy_stats = compute_stats(results_zerocopy)\n",
    "zerocopy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606a607-ef0c-4bbd-8093-05099c24f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeout_rate = results_zerocopy['request_rate'].max()\n",
    "# agg_results_zerocopy = (\n",
    "#     results_zerocopy[results_zerocopy['request_rate'] < timeout_rate]\n",
    "#     .groupby(\"request_rate\")\n",
    "#     .aggregate({\"latency\": [\"mean\", \"median\", \"max\"]}))\n",
    "# agg_results_zerocopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41323031-2b33-4835-a40f-cb01e503388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two sets of results against each other.\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(baseline_stats.index, baseline_stats['timeout_fraction'],\n",
    "         \"-o\", label='Without zero-copy loading')\n",
    "plt.plot(zerocopy_stats.index, \n",
    "         zerocopy_stats['timeout_fraction'],\n",
    "         \"-o\", label=\"With zero-copy loading\")\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Timeout Fraction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91dea0-2356-4dee-ae4f-bbfd54cb8561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afa7e0f34d224467fd24b0cfa9c212efa127bdf53fe1c4e3ddf54198f34a39e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
