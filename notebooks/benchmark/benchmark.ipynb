{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8270a16d",
   "metadata": {},
   "source": [
    "# benchmark.ipynb\n",
    "\n",
    "This notebook contains the text and code for the next blog post in the zero-copy model series, \n",
    "title TBD.\n",
    "\n",
    "The first post explained how to load PyTorch models for inference extremely fast by leveraging the Plasma object store's ability to load numeric data directly from shared memory.\n",
    "\n",
    "In this post, we talk in more concrete terms about how to use this zero-copy model loading for model serving. We put together a simple model serving system, then set up a microbenchmark that simulates a heavy-tailed traffic pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbf7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import zerocopy\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69fefd",
   "metadata": {},
   "source": [
    "# Title of new blog post goes here\n",
    "\n",
    "*Recap of previous blog post goes here.*\n",
    "\n",
    "In a [previous post](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c), we introduced the concept of *zero-copy model loading*. Zero-copy model loading involves keeping the weights of a deep learning model in shared memory, so that different processes can \"load\" the model for inference without needing to have a copy of the weights in their local heap memory.\n",
    "\n",
    "We showed that the Plasma object store integrated into Ray makes it easy to do zero-copy model loading, and that that implementing this technique on Ray can accelerate model loading by several orders of magnitude. If you'd like to find out more about the details of zero-copy model loading, follow [this link](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c) to view the previous post.\n",
    "\n",
    "In this post, we focus on how to use zero-copy model loading for production deployments of large natural language processing (NLP) models. We introduce `zerocopy`, a Python package that makes it extra simple to apply zero-copy model loading to PyTorch models. We show how easy it is to deploy modern NLP models using with `zerocopy` and Ray Serve. Finally, we present an end-to-end model serving benchmark that shows how we can serve 12 state-of-the-art NLP models with a single cloud VM and achieve (**TODO: final numbers**)x better scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7add2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 15:57:26,911\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=16252)\u001b[0m 2022-03-04 15:57:31,076\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=16252)\u001b[0m 2022-03-04 15:57:31,186\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:XTDcTe:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-04 15:57:31,596\tINFO api.py:475 -- Started Serve instance in namespace 'dac37c52-87e4-4e60-a461-5d85c99250dc'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16251)\u001b[0m INFO:     Started server process [16251]\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog post.\n",
    "# Fire up Ray\n",
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()\n",
    "\n",
    "# Wait a moment to make sure that all log output goes to this cell\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cedcb5-62a4-401b-ac58-d2fe41044dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introducing `zerocopy`\n",
    "\n",
    "Our previous post included code snippets that show how to rewrite a PyTorch model to use zero-copy model loading. \n",
    "We've recently created a Python package, `zerocopy`, that lets you apply this technique to your models without having to copy and paste a bunch of Python code. \n",
    "\n",
    "**TODO: Explain the relationship between `zerocopy` and Project Codeflare**\n",
    "\n",
    "**(TODO: Publish the package to PyPI and insert installation instructions here)**\n",
    "\n",
    "Using the `zerocopy` package is a three-step process:\n",
    "1. Import the package\n",
    "2. Move your model's weights onto the Plasma object store\n",
    "3. Run your model in an asynchronous Ray task\n",
    "\n",
    "Let's show these three steps in action with the [BERT language model](https://arxiv.org/abs/1810.04805) as implemented in the [Transformers](https://huggingface.co/docs/transformers/index) library.\n",
    "\n",
    "Step 1 is just a Python `import` statement:\n",
    "```python \n",
    "import zerocopy\n",
    "```\n",
    "\n",
    "Then it's on to step 2: Moving your model's weights onto Plasma. You will of course need a PyTorch model to do this step. Here we use the Transformers library's `from_pretrained()` function to load a copy of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c168f1-faa9-4dbb-aeea-64940ec5f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727b62eb-186b-49a5-a612-c554282323c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  1005,  1055,  2178,  2773,  2005,  1005,  1996, 22244,\n",
       "          1005,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog\n",
    "# Load a tokenizer to go with the model\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased')\n",
    "text = \"What's another word for 'thesaurus'?\"\n",
    "bert_input = bert_tokenizer(text, return_tensors='pt')\n",
    "bert_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c449584-9ad4-484f-957e-162bf40f8781",
   "metadata": {},
   "source": [
    "To move this model's weights onto Plasma, you first need to pass the model through `zerocopy.extract_tensors`, which converts the model into two things: a copy of the model without any weights and a separate Python dictionary containing the weights. Then you need to copy the model and its weights to Plasma using the function `ray.put()`. You can do both of these operations with a single line of Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30e8b0e-d6c2-4ec2-a3fe-369ab675f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ref = ray.put(zerocopy.extract_tensors(bert_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b456a-f5aa-48b1-81f6-c14e53835015",
   "metadata": {},
   "source": [
    "The return value from `ray.put()` is a Ray [object reference](https://arrow.apache.org/docs/python/plasma.html#object-ids). This object reference lets you load the model almost instantly from any location on your Ray cluster. This capability is what enables step 3: Running your model in an asynchronous Ray task.\n",
    "\n",
    "In our previous post, we showed how you can define a stateless Ray task that loads the model, runs inference over an input, and returns the result. The `zerocopy` package includes a built-in function `call_model()` that lets you do all these steps in one line of Python code. You just pass in the object reference, the name of the method on the model you want to invoke, and the arguments to that method. `call_model.remote()` takes care of the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2edae889-4937-450a-9813-b3a44dceb68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ref = zerocopy.call_model.remote(bert_ref, [], bert_input, '__call__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c8211-6dbe-46a4-b9b6-f0bfc0dc1fe1",
   "metadata": {},
   "source": [
    "As with any other Ray task, `call_model.remote()` returns a [future](https://docs.ray.io/en/latest/ray-overview/index.html#parallelizing-python-java-functions-with-ray-tasks) --- a Ray object reference to the place where the result will appear once the task has completed. You can retrieve this result with `ray.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a1e427-fc22-4685-9f49-02dcc6145744",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ray.get(result_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23599bf9-9e86-46eb-8f5d-e95a21732bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The time to invoke the rewritten model once is almost the same as running the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c162f92a-93f0-4732-9abc-b7d2db7f84fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time to run locally: 79.5 ms ± 1.34 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Time to run with zero-copy: 85.1 ms ± 1.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit bert_model(**bert_input)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(bert_ref, [], bert_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748fc9e-bb77-4697-9557-e96e1b788c26",
   "metadata": {},
   "source": [
    "If we run inference multiple times, `zero_copy.call_model()` can send those inference requests to separate Ray tasks that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e1c83a-585e-4b13-ad18-7a5dd937a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 200 times with zero-copy: 3.68 s ± 17.2 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "       Time to run 200 times locally: 15.9 s ± 69.4 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def run_local(num_repeats: int):\n",
    "    return [bert_model(**bert_input)\n",
    "            for _ in range(num_repeats)]\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    return ray.get([zerocopy.call_model.remote(bert_ref, [], bert_input)\n",
    "                    for _ in range(num_repeats)])\n",
    "\n",
    "\n",
    "NUM_REPEATS = 200\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334ff0f-d0ad-4d8d-9b58-9f9fbf5bada5",
   "metadata": {},
   "source": [
    "## Model inference pipelines\n",
    "\n",
    "With the `zerocopy` library's `extract_tensors()` and `call_model()` functions, you can apply zero-copy model loading to a Pytorch model with two lines of Python. But what about the end-to-end program that this model came from? \n",
    "\n",
    "Most machine learning models require some additional code to use them in a meaningful application. NLP models in particular require *preprocessing* to convert natural language text into a format amenable to model inference and *postprocessing* to convert the model's answer into a format that a person can understand. \n",
    "\n",
    "It's common to package the model as *pipeline* that includes preprocessing, inference, and postprocessing bundled together in a single Python object. For example, the Transformers library's BERT model that we have been using in our examples so far comes with a pipeline that performs the end-to-end task of *masked language modeling*: Identifying the most likely word to fill in a blank in a phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0938883f-cd5f-4ed6-b2f7-d195652859ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6013057231903076,\n",
       "  'token': 2033,\n",
       "  'token_str': 'me',\n",
       "  'sequence': 'all your base are belong to me.'},\n",
       " {'score': 0.08610764890909195,\n",
       "  'token': 2032,\n",
       "  'token_str': 'him',\n",
       "  'sequence': 'all your base are belong to him.'},\n",
       " {'score': 0.054536789655685425,\n",
       "  'token': 2017,\n",
       "  'token_str': 'you',\n",
       "  'sequence': 'all your base are belong to you.'},\n",
       " {'score': 0.04619930312037468,\n",
       "  'token': 2149,\n",
       "  'token_str': 'us',\n",
       "  'sequence': 'all your base are belong to us.'},\n",
       " {'score': 0.03938837721943855,\n",
       "  'token': 2068,\n",
       "  'token_str': 'them',\n",
       "  'sequence': 'all your base are belong to them.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pipeline = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "bert_pipeline('All your base are belong to [MASK].')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb709640-def3-415a-b0e0-8ef49968f7b3",
   "metadata": {},
   "source": [
    "The `zerocopy` library includes a function `rewrite_pipeline` that transforms any models embedded into Python object into Ray tasks that use zero-copy model loading to load weights. If we apply this function to a pipeline, the resulting rewritten pipeline faithfully performs all the preprocessing and postprocessing that the original pipeline performed. However, this rewritten pipeline runs the embedded PyTorch model in remote Ray tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2add9f8b-b401-4cf4-accb-c753bfe6a26c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local memory used before rewrite: 715664 kb\n",
      " Local memory used after rewrite: 6848 kb\n",
      "Output before rewrite: 0.6013057231903076\n",
      " Output after rewrite: 0.6013057231903076\n"
     ]
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "\n",
    "zero_copy_bert_pipeline = zerocopy.rewrite_pipeline(bert_pipeline)\n",
    "print(f\"Local memory used before rewrite: {asizeof.asizeof(bert_pipeline)} kb\")\n",
    "print(f\" Local memory used after rewrite: {asizeof.asizeof(zero_copy_bert_pipeline)} kb\")\n",
    "\n",
    "print(f\"Output before rewrite: {bert_pipeline('All your base are belong to [MASK].')[0]['score']}\")\n",
    "print(f\" Output after rewrite: {zero_copy_bert_pipeline('All your base are belong to [MASK].')[0]['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db391cd4-b007-4f8a-a320-751a8b0644bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(f777f4d299f5dc31ffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 15:59:30,745\tERROR worker.py:85 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::call_model()\u001b[39m (pid=16250, ip=127.0.0.1)\n",
      "  File \"/Users/freiss/ray/zero-copy-model-loading/zerocopy/invoke.py\", line 54, in call_model\n",
      "    model_skeleton, model_weights = model_ref\n",
      "TypeError: cannot unpack non-iterable _Callback object\n"
     ]
    }
   ],
   "source": [
    "a = zero_copy_bert_pipeline.preprocess('All your base are belong to [MASK].')\n",
    "b = zerocopy.call_model.remote(\n",
    "            zero_copy_bert_pipeline.model, [], a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e89618-037d-4440-883d-9bcf45f9afce",
   "metadata": {},
   "source": [
    "## Deploying Models with `zerocopy` and Ray Serve\n",
    "\n",
    "TODO: Describe the most common approach (as in TorchServe, TensorFlow Serving, Seldon Core, and others)\n",
    "* Put each model in its own container\n",
    "* Reserve dedicated CPU capacity for each container\n",
    "* Reserve enough memory to hold the model's weights\n",
    "* Reserve enough additional memory to hold any intermediate results (typically several GB) that the model will produce while running over its largest allowable input size\n",
    "* Many conflicting knobs to tune \n",
    "    * How many replicas of each model to keep up at all times\n",
    "    * How quickly to spawn additional replicas (spinning up a replica is expensive)\n",
    "    * How quickly to shut down unused replicas\n",
    "    * How much memory and CPU to allocate to each replica\n",
    "    * Should each replica operate over batches or single requests\n",
    "* Lots of tuning required to achieve good performance for a given workload\n",
    "* Common to operate with low utilizations (TODO: Any public stats to cite here?)\n",
    "\n",
    "TODO: Describe our approach of using zero copy: \n",
    "* Model inference happens in stateless Ray tasks.\n",
    "* Argument handling, preprocessing, and postprocessing happens in Ray Serve endpoints\n",
    "* Lower memory footprint, since you don't need to reserve heap memory for models that aren't running\n",
    "* Better system utilization, because nearly all memory and CPU is available to any Ray task at any time\n",
    "* Instant scaleout in response to bursts of requests for a particular model\n",
    "* Far fewer parameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4075d9-2771-42cf-b308-40e9e5a8ffde",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c25fe785-18c4-4561-886d-be5f9ac8ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "\n",
    "@serve.deployment\n",
    "class MyDeployment:\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        \n",
    "        # Load the entire pipeline\n",
    "        self._pipeline = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "        \n",
    "        # Move the model weights to Plasma\n",
    "        self._pipeline.model = zerocopy.extract_tensors(self._pipeline.model)\n",
    "        \n",
    "        \n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        '''\n",
    "        Web service entry point.\n",
    "        \n",
    "        Args:\n",
    "            request: HTTP request object for a REST web service call\n",
    "                     in the form:\n",
    "                     { \"input\": \"<input text with [MASK]>\" }\n",
    "        '''\n",
    "        # Parse JSON. A real deployment would also sanitize the input.\n",
    "        json_request = await request.json()\n",
    "        input_ = json_request['input']\n",
    "        \n",
    "        # Preprocessing\n",
    "        features = self._pipeline.preprocess(input_)\n",
    "        \n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        raw_output = await zerocopy.call_model.remote(\n",
    "            self._pipeline.model, [], features)\n",
    "\n",
    "        # Postprocessing\n",
    "        raw_output[\"input_ids\"] = features[\"input_ids\"]\n",
    "        return self._pipeline.postprocess(raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f6985-99a1-4bb9-a9e5-addd94e983bd",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9daa3577-ad3a-4cfb-b2fb-5e7ab9f5caa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 16:10:21,825\tINFO api.py:249 -- Updating deployment 'my_model'. component=serve deployment=my_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=16252)\u001b[0m 2022-03-04 16:10:21,885\tINFO deployment_state.py:882 -- Stopping 1 replicas of deployment 'my_model' with outdated versions. component=serve deployment=my_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=16252)\u001b[0m 2022-03-04 16:10:24,008\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'my_model'. component=serve deployment=my_model\n",
      "2022-03-04 16:10:30,761\tINFO api.py:261 -- Deployment 'my_model' is ready at `http://127.0.0.1:8000/my_model`. component=serve deployment=my_model\n"
     ]
    }
   ],
   "source": [
    "MyDeployment.options(name='my_model', ray_actor_options={\"num_cpus\": 0.1}).deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130c9b6-ae1e-4990-b7ad-13a54abbb957",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "720bdb27-f993-4213-aa8c-6878c4d69263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.6013057231903076,\n",
      "    \"token\": 2033,\n",
      "    \"token_str\": \"me\",\n",
      "    \"sequence\": \"all your base are belong to me.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08610764890909195,\n",
      "    \"token\": 2032,\n",
      "    \"token_str\": \"him\",\n",
      "    \"sequence\": \"all your base are belong to him.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.054536789655685425,\n",
      "    \"token\": 2017,\n",
      "    \"token_str\": \"you\",\n",
      "    \"sequence\": \"all your base are belong to you.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.04619930312037468,\n",
      "    \"token\": 2149,\n",
      "    \"token_str\": \"us\",\n",
      "    \"sequence\": \"all your base are belong to us.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.03938837721943855,\n",
      "    \"token\": 2068,\n",
      "    \"token_str\": \"them\",\n",
      "    \"sequence\": \"all your base are belong to them.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(requests.put('http://127.0.0.1:8000/my_model', \n",
    "             '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afd04d-ab4b-4573-a65d-615846d615dd",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33920dd8-32ff-47cb-976e-13251819318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class PipelineActor:\n",
    "    '''\n",
    "    Threaded Ray actor\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        pipeline_tmp = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(pipeline_tmp)\n",
    "    \n",
    "    def run(self, input_: str):\n",
    "        # Model inference calls inside this pipeline will happen in remote\n",
    "        # Ray tasks.\n",
    "        return self._pipeline(input_)\n",
    "\n",
    "    \n",
    "@serve.deployment\n",
    "class MyDeployment2:\n",
    "    def __init__(self):\n",
    "        self._pipeline_actor = PipelineActor.options(max_concurrency=100,\n",
    "                                                     num_cpus=0.1).remote()\n",
    "        \n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "        input_ = json_request['input']\n",
    "        \n",
    "        result = await self._pipeline_actor.run.remote(input_)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef46a02f-b940-486d-9abe-cf9bdd6c4d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 16:17:48,921\tINFO api.py:249 -- Updating deployment 'my_model2'. component=serve deployment=my_model2\n",
      "\u001b[2m\u001b[36m(ServeController pid=16252)\u001b[0m 2022-03-04 16:17:48,952\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'my_model2'. component=serve deployment=my_model2\n",
      "2022-03-04 16:17:49,281\tINFO api.py:261 -- Deployment 'my_model2' is ready at `http://127.0.0.1:8000/my_model2`. component=serve deployment=my_model2\n"
     ]
    }
   ],
   "source": [
    "MyDeployment2.options(name='my_model2', ray_actor_options={\"num_cpus\": 0}).deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cc6889f-d3d5-46e0-b60d-1e5e37a225c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.6013057231903076,\n",
      "    \"token\": 2033,\n",
      "    \"token_str\": \"me\",\n",
      "    \"sequence\": \"all your base are belong to me.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08610764890909195,\n",
      "    \"token\": 2032,\n",
      "    \"token_str\": \"him\",\n",
      "    \"sequence\": \"all your base are belong to him.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.054536789655685425,\n",
      "    \"token\": 2017,\n",
      "    \"token_str\": \"you\",\n",
      "    \"sequence\": \"all your base are belong to you.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.04619930312037468,\n",
      "    \"token\": 2149,\n",
      "    \"token_str\": \"us\",\n",
      "    \"sequence\": \"all your base are belong to us.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.03938837721943855,\n",
      "    \"token\": 2068,\n",
      "    \"token_str\": \"them\",\n",
      "    \"sequence\": \"all your base are belong to them.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(requests.put('http://127.0.0.1:8000/my_model2', \n",
    "             '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd77fc0-06fb-409f-9b80-7a17be557251",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95790e6e-c8a4-4419-8b88-b50b8ead26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class ThreadedDeployment:\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        pipeline_tmp = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(pipeline_tmp)\n",
    "        \n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "        \n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        '''\n",
    "        Web service entry point.\n",
    "        \n",
    "        Args:\n",
    "            request: HTTP request object for a REST web service call\n",
    "                     in the form:\n",
    "                     { \"input\": \"<input text with [MASK]>\" }\n",
    "        '''\n",
    "        # Parse JSON. A real deployment would also sanitize the input.\n",
    "        json_request = await request.json()\n",
    "        masked_string = json_request['input']\n",
    "        \n",
    "        # The original `transformers` code is not async-aware, so we\n",
    "        # call it from `run_in_executor()`.\n",
    "        # Preprocessing and postprocessing code will happen inside this\n",
    "        # process, but model inference will occur in a remote Ray task.\n",
    "        # While that task is running, the local thread will block on\n",
    "        # a call to `ray.get()`\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(masked_string))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789fa4d-58aa-48b6-a4ba-3af62b9c7e46",
   "metadata": {},
   "source": [
    "Text goes here.\n",
    "\n",
    "preprocessing and postprocessing happen inside the local thread pool, but model inference occurs in Ray tasks\n",
    "\n",
    "each thread spends most of its time blocked waiting for the Ray task to complete\n",
    "so the local overhead of the threads is minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0210e287-2ee5-4c8d-b784-b29e282323d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 16:03:01,843\tINFO api.py:249 -- Updating deployment 'my_model'. component=serve deployment=my_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=16252)\u001b[0m 2022-03-04 16:03:01,923\tINFO deployment_state.py:882 -- Stopping 1 replicas of deployment 'my_model' with outdated versions. component=serve deployment=my_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=16252)\u001b[0m 2022-03-04 16:03:04,038\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'my_model'. component=serve deployment=my_model\n",
      "2022-03-04 16:03:11,079\tINFO api.py:261 -- Deployment 'my_model' is ready at `http://127.0.0.1:8000/my_model`. component=serve deployment=my_model\n"
     ]
    }
   ],
   "source": [
    "ThreadedDeployment.options(name='my_model', ray_actor_options={\"num_cpus\": 0.1}).deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ee145-0e88-44c4-b101-37654fa884c3",
   "metadata": {},
   "source": [
    "Text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "923cf583-abb2-4757-b513-eb98578db1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"score\": 0.6013057231903076,\n",
      "    \"token\": 2033,\n",
      "    \"token_str\": \"me\",\n",
      "    \"sequence\": \"all your base are belong to me.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.08610764890909195,\n",
      "    \"token\": 2032,\n",
      "    \"token_str\": \"him\",\n",
      "    \"sequence\": \"all your base are belong to him.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.054536789655685425,\n",
      "    \"token\": 2017,\n",
      "    \"token_str\": \"you\",\n",
      "    \"sequence\": \"all your base are belong to you.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.04619930312037468,\n",
      "    \"token\": 2149,\n",
      "    \"token_str\": \"us\",\n",
      "    \"sequence\": \"all your base are belong to us.\"\n",
      "  },\n",
      "  {\n",
      "    \"score\": 0.03938837721943855,\n",
      "    \"token\": 2068,\n",
      "    \"token_str\": \"them\",\n",
      "    \"sequence\": \"all your base are belong to them.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(requests.put('http://127.0.0.1:8000/my_model', \n",
    "             '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4614552-0203-4ecc-a7c3-986c78c8c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Stop this notebook's copy of Ray so as not to interfere with the\n",
    "# copy in `ray_deploy.ipynb`\n",
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eccd9",
   "metadata": {},
   "source": [
    "## The Scenario\n",
    "\n",
    "The end-to-end scenario for our benchmark involves supporting an AI chatbot.\n",
    "The chatbot's conversational AI runs off of a conversation tree. Some of the \n",
    "nodes of this tree invoke models.\n",
    "\n",
    "\n",
    "> **TODO:** Cartoon block diagram of the end-to-end scenario. \n",
    "> Diagram should show a user interacting with a chatbot. The chatbot runs off of a conversation tree. \n",
    "> Some of the nodes of the conversation tree have question answering models hanging off of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc5ddc-35bc-4c70-b3c8-c9e0debbc8bb",
   "metadata": {},
   "source": [
    "Our benchmark will cover the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs four different types of models:\n",
    "* *Intent detection* models that determine what is the user's goal.\n",
    "* *Sentiment analysis* models that monitor the user's mood.\n",
    "* *Question answering* models that provide the answers to specific factual questions.\n",
    "* *Natural language generation* models that give the chatbot's responses a less scripted flavor.\n",
    "\n",
    "Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models.\n",
    "\n",
    "In a real application, you would want to train custom versions of each type\n",
    "of model for the topics your chatbot covers.\n",
    "Since we're only interested in modeling throughput and latency, we skipped that customization\n",
    "step and just used the most popular pretrained model from each category from the \n",
    "[Huggingface model marketplace](https://huggingface.co/models).\n",
    "\n",
    "Each of these models uses a [Transformer](https://arxiv.org/abs/1706.03762)-based neural network,\n",
    "with a *language model* and a task specific *head*, tuned over \n",
    "a domain-specific training set.  The table below summarizes the four models that we used.\n",
    "\n",
    "\n",
    "| Task                 | Model Name                                   | Language Model  |  Pre/post Processing\n",
    "| -----------          | -----------                                  | ------------    | ---------------\n",
    "| Intent Detection     | `mrm8488/t5-base-finetuned-e2m-intent`       | T5              | Reference code\n",
    "| Sentiment Analysis   | `cardiffnlp/twitter-roberta-base-sentiment`  | RoBERTa         | Reference code\n",
    "| Question Answering   | `deepset/roberta-base-squad2`                | RoBERTa         | Pipeline\n",
    "| Text Generation      | `gpt2`                                       | GPT-2           | Pipeline\n",
    "\n",
    "\n",
    "Although all four models came from the same marketplace, they are quite diverse. The models use three different core language models: [Text-to-Text Transfer Transformer](https://arxiv.org/pdf/1910.10683.pdf) (T5) from Google Research, \n",
    "[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) from Facebook AI, and [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) from OpenAI. \n",
    "\n",
    "The models also use two very different ways to package their preprocessing and postprocessing code. The intent and sentiment models provide small blocks of reference Python code, with the intent being that the user will adapt this reference code to the specific circumstances of the end-to-end appliction. \n",
    "\n",
    "The question answering and text generation models both use the Transformers library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines) to package their preprocessing and postprocesing code. Unlike the \"example reference code\" approach, the Pipelines API's end-to-end inference code is intended for direct production use. It includes support for model retraining, as well as performance optimizations like batching and GPU acceleration, plus code for handling corner cases like long input strings. This prepackaged code can save a lot of time, provided that your application is structured in a way that can easily accomodate a large block of non-modifiable third-party Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b1aa",
   "metadata": {},
   "source": [
    "## Baseline implementation\n",
    "\n",
    "Our baseline implementation of the model serving backend for our benchmark emulates running each model in a separate container. We used [TorchServe](https://pytorch.org/serve/) as our model serving framework for the baseline deployment. By configuing TorchServe to use a pool of processes, we were able to simulate running each model in a separate container without having to set up a dedicated Kubernetes cluster. See [this notebook](./torchserve.ipynb) for details of the TorchServe deployment.\n",
    "\n",
    "*Note that earlier versions of this notebook implemented the baseline model deployment with a pool of Ray actors. That older version is preserved in [a separate notebook](./ray_baseline.ipynb).*\n",
    "\n",
    "With TorchServe running in the background, we can invoke our models via their REST APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025582b-6588-42c8-ba47-545903b3511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the management API to verify that TorchServe is running.\n",
    "try:\n",
    "    print(requests.get('http://127.0.0.1:8081/models').json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Stop notebook execution\n",
    "    raise ValueError('TorchServe does not appear to be running. Please start TorchServe.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011ecef-438d-458d-a542-90118b7d36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCHSERVE_PORT = 8080\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}\n",
    "\n",
    "intent_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/intent_en',\n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/sentiment_en',\n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/qa_en',\n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    f'http://127.0.0.1:{TORCHSERVE_PORT}/predictions/generate_en',\n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6796f9-841b-4b8f-aaa7-8812dbbd097c",
   "metadata": {},
   "source": [
    "## Zero-copy implementation\n",
    "\n",
    "TODO: Describe how we deployed the four models, with reference to [this notebook](./ray_deploy.ipynb) that shows the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4316-86af-481f-8cc0-887a88c30c0d",
   "metadata": {},
   "source": [
    "## The Benchmark\n",
    "\n",
    "Now that we have deployed each of our models with a web service front end, we can define a benchmark that sends inference traffic to these web service endpoints and measures response time.\n",
    "\n",
    "TODO: Cite a reference for Poisson being a realistic distribution of traffic for interactive services.\n",
    "\n",
    "Our benchmark generates a trace of requests, then plays back the trace and measures the latency of each request. The request rate changes each second, with the rate of a particular 1-second window drawn from the Poisson distribution. Each request goes to a randomly-selected model. The choice of models is weighted according to a truncated Poisson distribution.  \n",
    "\n",
    "The benchmark plays back the trace, measuring the end-to-end latency of each request. We repeat this process of generating and playing back the trace, gradually ramping up the average request rate of the bursty traffic until requests start timing out.\n",
    "\n",
    "Code is [here](./benchmark.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403c8b3-7dda-4c32-86ec-1ba0eee6be34",
   "metadata": {},
   "source": [
    "### Baseline benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0e951-9592-48f2-94e4-07c328e2965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the management API to verify that TorchServe is running.\n",
    "try:\n",
    "    print(requests.get('http://127.0.0.1:8081/models').json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Stop notebook execution\n",
    "    raise ValueError('TorchServe does not appear to be running. Please start TorchServe.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "510f535f-12a8-4104-a9c1-ea4c0a24936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port is 8080 and output CSV file is outputs/baseline.csv\n",
      "Running at 2 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 2.5 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 3.0 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 3.5 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 4.0 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 4.5 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 5.0 requests/sec.\n",
      " => 56.1% failure rate\n",
      "Running at 5.5 requests/sec.\n",
      " => 73.4% failure rate\n",
      "Stopping due to fraction of failures (0.7337110481586402) exceeding allowable limit (0.6)\n"
     ]
    }
   ],
   "source": [
    "# This call may be disabled to avoid overwriting local results.\n",
    "# Toggle cell type to \"code\" to run.\n",
    "!python3 benchmark.py 8080 outputs/baseline.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c948be2-7759-4ee1-866c-ef52df1cd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Make sure that TorchServe is shut down before we continue.\n",
    "torchserve_is_running = True\n",
    "try:\n",
    "    requests.get('http://127.0.0.1:8081/models').json()\n",
    "except requests.exceptions.ConnectionError:\n",
    "    torchserve_is_running = False\n",
    "if torchserve_is_running:\n",
    "    raise ValueError('Please shut down TorchServe before continuing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06033854-2849-450f-b7e2-382f79b5654b",
   "metadata": {},
   "source": [
    "### Optimized benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f71e2ec-77c9-4ef1-abea-dc3fad338794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the Ray models are up\n",
    "try:\n",
    "    INTENT_INPUT = {\n",
    "        'context':\n",
    "            (\"I came here to eat chips and beat you up, \"\n",
    "             \"and I'm all out of chips.\")\n",
    "    }\n",
    "    requests.put(\n",
    "        'http://127.0.0.1:8000/predictions/intent_en', \n",
    "        json.dumps(INTENT_INPUT)).json()\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    raise ValueError('Please start up the zero-copy model deployment before continuing.') from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb1a2c9a-7bad-49cf-98f6-bdd5712494af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port is 8000 and output CSV file is outputs/zerocopy.csv\n",
      "Running at 2 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 2.5 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 3.0 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 3.5 requests/sec.\n",
      " => 0.0% failure rate\n",
      "Running at 4.0 requests/sec.\n",
      " => 1.3% failure rate\n",
      "Running at 4.5 requests/sec.\n",
      " => 5.8% failure rate\n",
      "Running at 5.0 requests/sec.\n",
      " => 6.6% failure rate\n",
      "Running at 5.5 requests/sec.\n",
      " => 13.0% failure rate\n",
      "Running at 6.0 requests/sec.\n",
      " => 8.0% failure rate\n",
      "Running at 6.5 requests/sec.\n",
      " => 6.5% failure rate\n",
      "Running at 7.0 requests/sec.\n",
      " => 5.8% failure rate\n",
      "Running at 7.5 requests/sec.\n",
      " => 10.0% failure rate\n",
      "Running at 8.0 requests/sec.\n",
      " => 6.7% failure rate\n",
      "Running at 8.5 requests/sec.\n",
      " => 5.0% failure rate\n",
      "Running at 9.0 requests/sec.\n",
      " => 5.8% failure rate\n",
      "Running at 9.5 requests/sec.\n",
      " => 14.1% failure rate\n",
      "Running at 10.0 requests/sec.\n",
      " => 5.0% failure rate\n",
      "Running at 10.5 requests/sec.\n",
      " => 8.3% failure rate\n",
      "Running at 11.0 requests/sec.\n",
      " => 49.4% failure rate\n",
      "Running at 11.5 requests/sec.\n",
      " => 66.3% failure rate\n",
      "Stopping due to fraction of failures (0.6632047477744807) exceeding allowable limit (0.6)\n"
     ]
    }
   ],
   "source": [
    "# This call may be disabled to avoid overwriting local results.\n",
    "# Toggle cell type to \"code\" to run.\n",
    "!python3 benchmark.py 8000 outputs/zerocopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15aebd-46d9-41f1-9ae4-1947cb51b0b1",
   "metadata": {},
   "source": [
    "### Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4f8696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog\n",
    "\n",
    "# Aggregate benchmark results.\n",
    "def compute_stats(results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    timeout_results = results_df[results_df['result_code'] != 200]\n",
    "    success_results = results_df[results_df['result_code'] == 200]\n",
    "\n",
    "    timeout_counts = (\n",
    "        timeout_results\n",
    "        .groupby('request_rate')\n",
    "        .aggregate({'request_id': 'count'})\n",
    "        .rename(columns={'request_id': 'timeouts'}))\n",
    "    stats = (\n",
    "        success_results\n",
    "        .groupby('request_rate')\n",
    "        .aggregate({'latency': ['mean', 'median', 'max'],\n",
    "                    'request_id': 'count'}))\n",
    "\n",
    "    # Column names come out from the aggregations all messed up\n",
    "    stats.columns=['mean', 'median', 'max', 'successes']\n",
    "    stats = stats.join(timeout_counts).fillna(0)\n",
    "    stats['timeout_fraction'] = stats['timeouts'] / (stats['successes'] + stats['timeouts'])\n",
    "    return stats\n",
    "\n",
    "def maybe_generate_agg(prefix: str):\n",
    "    '''\n",
    "    Regenerate aggregate results for a benchmark run if a trace\n",
    "    is present.\n",
    "    \n",
    "    :param prefix: Name of run, i.e. 'baseline' or 'zerocopy'\n",
    "    '''\n",
    "    if os.path.exists(f'outputs/{prefix}.csv'):\n",
    "        results = pd.read_csv(f'outputs/{prefix}.csv')\n",
    "        stats = compute_stats(results)\n",
    "        stats.to_csv(f'outputs/{prefix}_agg.csv')\n",
    "        \n",
    "maybe_generate_agg('baseline')\n",
    "maybe_generate_agg('zerocopy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d9f5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_rate</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>successes</th>\n",
       "      <th>timeouts</th>\n",
       "      <th>timeout_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.313566</td>\n",
       "      <td>0.311650</td>\n",
       "      <td>0.620853</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.342991</td>\n",
       "      <td>0.311077</td>\n",
       "      <td>0.983981</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.365949</td>\n",
       "      <td>0.319422</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.429346</td>\n",
       "      <td>0.358365</td>\n",
       "      <td>1.263816</td>\n",
       "      <td>209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.672837</td>\n",
       "      <td>0.654021</td>\n",
       "      <td>1.760497</td>\n",
       "      <td>227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.911417</td>\n",
       "      <td>2.021288</td>\n",
       "      <td>4.213658</td>\n",
       "      <td>259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.258248</td>\n",
       "      <td>0.557560</td>\n",
       "      <td>4.993434</td>\n",
       "      <td>133</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.561056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.178513</td>\n",
       "      <td>0.084288</td>\n",
       "      <td>2.148058</td>\n",
       "      <td>94</td>\n",
       "      <td>259.0</td>\n",
       "      <td>0.733711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   request_rate      mean    median       max  successes  timeouts  \\\n",
       "0           2.0  0.313566  0.311650  0.620853        109       0.0   \n",
       "1           2.5  0.342991  0.311077  0.983981        147       0.0   \n",
       "2           3.0  0.365949  0.319422  0.996296        183       0.0   \n",
       "3           3.5  0.429346  0.358365  1.263816        209       0.0   \n",
       "4           4.0  0.672837  0.654021  1.760497        227       0.0   \n",
       "5           4.5  1.911417  2.021288  4.213658        259       0.0   \n",
       "6           5.0  1.258248  0.557560  4.993434        133     170.0   \n",
       "7           5.5  0.178513  0.084288  2.148058         94     259.0   \n",
       "\n",
       "   timeout_fraction  \n",
       "0          0.000000  \n",
       "1          0.000000  \n",
       "2          0.000000  \n",
       "3          0.000000  \n",
       "4          0.000000  \n",
       "5          0.000000  \n",
       "6          0.561056  \n",
       "7          0.733711  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load up the baseline results\n",
    "baseline_stats = pd.read_csv('outputs/baseline_agg.csv')\n",
    "baseline_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5bb6f85-63ad-427d-875c-24ca86ad4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_rate</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>successes</th>\n",
       "      <th>timeouts</th>\n",
       "      <th>timeout_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.408997</td>\n",
       "      <td>0.301005</td>\n",
       "      <td>3.089440</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.689526</td>\n",
       "      <td>0.311503</td>\n",
       "      <td>4.142979</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.402645</td>\n",
       "      <td>0.301247</td>\n",
       "      <td>3.347312</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.573503</td>\n",
       "      <td>0.309472</td>\n",
       "      <td>3.800097</td>\n",
       "      <td>209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.444287</td>\n",
       "      <td>0.852400</td>\n",
       "      <td>4.668393</td>\n",
       "      <td>224</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.013216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.490221</td>\n",
       "      <td>0.855068</td>\n",
       "      <td>4.993091</td>\n",
       "      <td>244</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.057915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.371780</td>\n",
       "      <td>0.818604</td>\n",
       "      <td>4.899146</td>\n",
       "      <td>283</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.066007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.5</td>\n",
       "      <td>1.478960</td>\n",
       "      <td>1.301579</td>\n",
       "      <td>4.981363</td>\n",
       "      <td>307</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.130312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.609648</td>\n",
       "      <td>1.250937</td>\n",
       "      <td>4.954760</td>\n",
       "      <td>334</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.079890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.5</td>\n",
       "      <td>1.548910</td>\n",
       "      <td>1.251555</td>\n",
       "      <td>4.910374</td>\n",
       "      <td>357</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.065445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.785618</td>\n",
       "      <td>1.553264</td>\n",
       "      <td>4.950860</td>\n",
       "      <td>391</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.057831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.5</td>\n",
       "      <td>1.957743</td>\n",
       "      <td>1.617995</td>\n",
       "      <td>4.912470</td>\n",
       "      <td>405</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.703809</td>\n",
       "      <td>1.756327</td>\n",
       "      <td>4.994082</td>\n",
       "      <td>444</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.067227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.5</td>\n",
       "      <td>1.702376</td>\n",
       "      <td>1.619443</td>\n",
       "      <td>4.943022</td>\n",
       "      <td>477</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.049801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.066785</td>\n",
       "      <td>1.962502</td>\n",
       "      <td>4.993357</td>\n",
       "      <td>499</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.058491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.5</td>\n",
       "      <td>2.516075</td>\n",
       "      <td>2.741317</td>\n",
       "      <td>5.003969</td>\n",
       "      <td>501</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.140652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.307207</td>\n",
       "      <td>2.190499</td>\n",
       "      <td>4.981188</td>\n",
       "      <td>556</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.049573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.5</td>\n",
       "      <td>3.006874</td>\n",
       "      <td>3.526240</td>\n",
       "      <td>5.011034</td>\n",
       "      <td>552</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.083056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.488596</td>\n",
       "      <td>2.197583</td>\n",
       "      <td>5.004599</td>\n",
       "      <td>327</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.493808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.5</td>\n",
       "      <td>1.343230</td>\n",
       "      <td>0.768448</td>\n",
       "      <td>4.991664</td>\n",
       "      <td>227</td>\n",
       "      <td>447.0</td>\n",
       "      <td>0.663205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    request_rate      mean    median       max  successes  timeouts  \\\n",
       "0            2.0  0.408997  0.301005  3.089440        109       0.0   \n",
       "1            2.5  0.689526  0.311503  4.142979        147       0.0   \n",
       "2            3.0  0.402645  0.301247  3.347312        183       0.0   \n",
       "3            3.5  0.573503  0.309472  3.800097        209       0.0   \n",
       "4            4.0  1.444287  0.852400  4.668393        224       3.0   \n",
       "5            4.5  1.490221  0.855068  4.993091        244      15.0   \n",
       "6            5.0  1.371780  0.818604  4.899146        283      20.0   \n",
       "7            5.5  1.478960  1.301579  4.981363        307      46.0   \n",
       "8            6.0  1.609648  1.250937  4.954760        334      29.0   \n",
       "9            6.5  1.548910  1.251555  4.910374        357      25.0   \n",
       "10           7.0  1.785618  1.553264  4.950860        391      24.0   \n",
       "11           7.5  1.957743  1.617995  4.912470        405      45.0   \n",
       "12           8.0  1.703809  1.756327  4.994082        444      32.0   \n",
       "13           8.5  1.702376  1.619443  4.943022        477      25.0   \n",
       "14           9.0  2.066785  1.962502  4.993357        499      31.0   \n",
       "15           9.5  2.516075  2.741317  5.003969        501      82.0   \n",
       "16          10.0  2.307207  2.190499  4.981188        556      29.0   \n",
       "17          10.5  3.006874  3.526240  5.011034        552      50.0   \n",
       "18          11.0  2.488596  2.197583  5.004599        327     319.0   \n",
       "19          11.5  1.343230  0.768448  4.991664        227     447.0   \n",
       "\n",
       "    timeout_fraction  \n",
       "0           0.000000  \n",
       "1           0.000000  \n",
       "2           0.000000  \n",
       "3           0.000000  \n",
       "4           0.013216  \n",
       "5           0.057915  \n",
       "6           0.066007  \n",
       "7           0.130312  \n",
       "8           0.079890  \n",
       "9           0.065445  \n",
       "10          0.057831  \n",
       "11          0.100000  \n",
       "12          0.067227  \n",
       "13          0.049801  \n",
       "14          0.058491  \n",
       "15          0.140652  \n",
       "16          0.049573  \n",
       "17          0.083056  \n",
       "18          0.493808  \n",
       "19          0.663205  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zerocopy_stats = pd.read_csv('outputs/zerocopy_agg.csv')\n",
    "zerocopy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41323031-2b33-4835-a40f-cb01e503388a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8511ecbbe0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the two sets of results against each other.\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(baseline_stats.index, baseline_stats['timeout_fraction'],\n",
    "         \"-o\", label='Without zero-copy loading')\n",
    "plt.plot(zerocopy_stats.index, \n",
    "         zerocopy_stats['timeout_fraction'],\n",
    "         \"-o\", label=\"With zero-copy loading\")\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Timeout Fraction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91dea0-2356-4dee-ae4f-bbfd54cb8561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7bfbbb1e-0f5e-45b5-8462-6e2f9d526bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Old code that demos `zerocopy` with the intent model\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference without zero-copy loading\n",
    "print('Result without zero-copy loading: '\n",
    "      + str(intent_model.generate(**features)))\n",
    "\n",
    "# Inference with zero-copy loading\n",
    "intent_model_ref = ray.put(zerocopy.extract_tensors(intent_model))\n",
    "print(' Result *with* zero-copy loading: ' +\n",
    "      str(ray.get(zerocopy.call_model.remote(\n",
    "          intent_model_ref, [], features, 'generate'))))\n",
    "          \n",
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit intent_model.generate(**features)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(intent_model_ref, [], features, 'generate'))\n",
    "\n",
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        intent_model.generate(**features)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [\n",
    "        zerocopy.call_model.remote(intent_model_ref, [], features, 'generate')\n",
    "        for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 50\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58769647-0991-4da2-ac40-61960952b09c",
   "metadata": {},
   "source": [
    "# Old code that demos `zerocopy` on the QA model pipeline\n",
    "zero_copy_qa = zerocopy.rewrite_pipeline(qa_pipeline)\n",
    "print(f\"Before rewrite: {qa_pipeline(**QA_INPUT)}\")\n",
    "print(f\" After rewrite: {zero_copy_qa(**QA_INPUT)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afa7e0f34d224467fd24b0cfa9c212efa127bdf53fe1c4e3ddf54198f34a39e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
