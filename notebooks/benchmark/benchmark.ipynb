{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0de8e77-2ee4-464c-b2da-d4983168dfbc",
   "metadata": {},
   "source": [
    "# Easier Model Serving with Ray Serve and zerocopy\n",
    "\n",
    "In a [previous post](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c), we introduced the concept of *zero-copy model loading*. Zero-copy model loading involves keeping the weights of a machine learning model in shared memory so that different processes can load the model for inference instantly without copying data.\n",
    "\n",
    "We showed that the Plasma object store integrated into [Ray](https://www.ray.io/) makes it easy to do zero-copy model loading, and that implementing this technique on Ray can accelerate model loading by several orders of magnitude. If you'd like to find out more about the details of zero-copy model loading, follow [this link](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c) to view the previous post.\n",
    "\n",
    "In this follow-on post, we focus on how zero-copy model loading with Ray lets you deploy models in production with fewer resources, fewer knobs to tune, and better performance. We introduce `zerocopy`, a Python package that makes it extra simple to apply the technique. We show how to deploy models using with `zerocopy` and Ray Serve. And then we present an end-to-end model serving benchmark that shows how we can serve multiple large NLP models with a single cloud VM and achieve 7x better scalability with no tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbf7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import zerocopy\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7add2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 10:00:24,190\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=84618)\u001b[0m 2022-04-01 10:00:29,489\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=84618)\u001b[0m 2022-04-01 10:00:29,593\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:Gdwiai:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-04-01 10:00:30,030\tINFO api.py:521 -- Started Serve instance in namespace 'ba7da799-0818-4b38-be80-0efbd71c4fe7'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=84617)\u001b[0m INFO:     Started server process [84617]\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog post.\n",
    "# Fire up Ray\n",
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()\n",
    "\n",
    "# Wait a moment to make sure that all log output goes to this cell\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cedcb5-62a4-401b-ac58-d2fe41044dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introducing `zerocopy`\n",
    "\n",
    "Our previous post included code snippets that show how to rewrite a PyTorch model to use zero-copy model loading. \n",
    "We've recently released a Python package, [`zerocopy`](https://pypi.org/project/zerocopy), that lets you apply this technique to your models without having to copy and paste Python code. \n",
    "This package is part of IBM's [Project Codeflare](https://github.com/project-codeflare), a framework to simplify the integration, scaling and acceleration of complex multi-step analytics and machine learning pipelines.\n",
    "\n",
    "You can install this package by typing:\n",
    "\n",
    "```\n",
    "pip install zerocopy\n",
    "```\n",
    "\n",
    "Once it's installed, using the `zerocopy` package is a three-step process:\n",
    "1. Import the package.\n",
    "2. Move your model's weights onto the Plasma object store.\n",
    "3. Run your model in an asynchronous Ray task.\n",
    "\n",
    "Let's show these three steps in action. \n",
    "\n",
    "Step 1 is just a Python `import` statement:\n",
    "```python \n",
    "import zerocopy\n",
    "```\n",
    "\n",
    "Then it's on to step 2: Moving your model's weights onto Plasma. You will of course need a PyTorch model to do this step. As an example, let's load up the most popular [intent detection](https://paperswithcode.com/task/intent-detection) model from the [Huggingface model marketplace](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c168f1-faa9-4dbb-aeea-64940ec5f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            'mrm8488/t5-base-finetuned-e2m-intent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727b62eb-186b-49a5-a612-c554282323c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  27,  764,  270,   12,    3, 1544, 8852,   11, 3853,   25,   95,    6,\n",
       "           11,   27,   31,   51,   66,   91,   13, 8852,    5,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Load a tokenizer to go with the model and generate some preprocessed\n",
    "# model inputs\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "text = (\"I came here to eat chips and beat you up, \"\n",
    "        \"and I'm all out of chips.\")\n",
    "model_input = tokenizer(text, return_tensors='pt')\n",
    "model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c449584-9ad4-484f-957e-162bf40f8781",
   "metadata": {},
   "source": [
    "To move this model's weights onto Plasma, you first need to pass the model through `zerocopy.extract_tensors()`, which separates the weights from the model's Python objects. Then you need to copy the model and its weights to Plasma using the function `ray.put()`. You can do both of these operations with a single line of Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30e8b0e-d6c2-4ec2-a3fe-369ab675f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = ray.put(zerocopy.extract_tensors(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b456a-f5aa-48b1-81f6-c14e53835015",
   "metadata": {},
   "source": [
    "The return value from `ray.put()` is a Ray [object reference](https://arrow.apache.org/docs/python/plasma.html#object-ids). This object reference lets you load the model almost instantly from any location on your Ray cluster. This capability is what enables step 3: Running your model in an asynchronous Ray task.\n",
    "\n",
    "In our previous post, we showed how you can define a stateless Ray task that loads the model, runs inference over an input, and returns the result. The `zerocopy` package includes a built-in function `call_model()` that lets you do all these steps in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2edae889-4937-450a-9813-b3a44dceb68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model's `generate()` method from a remote Ray task\n",
    "result_ref = zerocopy.call_model.remote(model_ref, [], model_input,\n",
    "                                        'generate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c8211-6dbe-46a4-b9b6-f0bfc0dc1fe1",
   "metadata": {},
   "source": [
    "As with any other Ray task, `call_model.remote()` returns a [future](https://docs.ray.io/en/latest/ray-overview/index.html#parallelizing-python-java-functions-with-ray-tasks) --- a Ray object reference to the place where the result will appear once the task has completed. You can retrieve this result with `ray.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a1e427-fc22-4685-9f49-02dcc6145744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   12,    3, 1544, 8852,    1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(result_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23599bf9-9e86-46eb-8f5d-e95a21732bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The time to invoke the rewritten model is almost the same as running the model locally. If you run inference multiple times, `zero_copy.call_model()` can send those inference requests to separate Ray tasks that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c162f92a-93f0-4732-9abc-b7d2db7f84fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Time to run once locally: 330 ms ± 11.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     Time to run once with zero-copy: 328 ms ± 8.87 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "       Time to run 100 times locally: 33.1 s ± 1.04 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Time to run 100 times with zero-copy: 13.9 s ± 463 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"            Time to run once locally: \", end=\"\")\n",
    "%timeit model.generate(**model_input)\n",
    "print(\"     Time to run once with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(model_ref, [], model_input, 'generate'))\n",
    "\n",
    "NUM_REPEATS = 100\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit [model.generate(**model_input) for _ in range(NUM_REPEATS)]\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit ray.get([zerocopy.call_model.remote(model_ref, [], model_input, 'generate') for _ in range(NUM_REPEATS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e1c83a-585e-4b13-ad18-7a5dd937a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 50 times with zero-copy 2: 7.19 s ± 130 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Time to run 50 times with zero-copy: 7.07 s ± 216 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "       Time to run 50 times locally: 17.3 s ± 577 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# More in-depth benchmarking for internal use.\n",
    "# TODO: Use output from running this cell on a large machine for the blog post\n",
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        model.generate(**model_input)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [\n",
    "        zerocopy.call_model.remote(model_ref, [],\n",
    "                                   model_input, 'generate')\n",
    "        for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "def run_zero_copy_2(num_repeats: int):\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    num_not_started = num_repeats\n",
    "    futures = []\n",
    "    # Keep `num_cores` tasks live at a time\n",
    "    while num_not_started > 0:\n",
    "        addl_futures = [\n",
    "            zerocopy.call_model.remote(model_ref, [],\n",
    "                                       model_input, 'generate')\n",
    "            for _ in range(min(num_not_started, num_cores - len(futures)))]\n",
    "        num_not_started -= len(addl_futures)\n",
    "        futures.extend(addl_futures)\n",
    "        ready, futures = ray.wait(futures)\n",
    "        ray.get(ready)\n",
    "    # Fetch remaining results\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 50\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy 2: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy_2(NUM_REPEATS)\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e89618-037d-4440-883d-9bcf45f9afce",
   "metadata": {},
   "source": [
    "## A better way to deploy models\n",
    "\n",
    "In a world without zero-copy model loading, anyone who runs deep learning models in production encounters a common set of design constraints. To run inference on a model, you need to have the model's weights loaded into a process's memory. But loading the model from storage is orders of magnitude more expensive than running inference. So you need to keep the model perpetually loaded in a process's memory. This process needs to have enough CPU and memory resources so that it can run inference when it is called upon to do so.\n",
    "\n",
    "These constraints are why systems like [TorchServe](https://github.com/pytorch/serve/blob/master/docs/management_api.md), [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/serving_kubernetes), and [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/overview.html#e2e-serving-with-model-servers) run each model in its own container, or equivalently in a UNIX process with reserved CPU and memory resources.\n",
    "\n",
    "**TODO: Find and insert an appropriate block diagram here**\n",
    "\n",
    "It is possible to achieve good response times and low costs with this design, but you will need to tune many parameters to arrive at that point. How much CPU and memory capacity should you reserve for each model's containers? How many replicas of each container should you keep up at all times? How quickly should you spin up additional replicas in response to changing loads? How quickly should you spin down replicas when they aren't being used? How do you even measure the current load? All of these important decisions take time and energy.\n",
    "\n",
    "Zero-copy model loading with Ray turns model inference into a stateless process. As long as the weights are in the local segment of the Plasma object store, your code can load a copy of the model instantly, run a single inference request, and then unload the model. This statelessness removes the design constraints that underlie conventional model serving systems. With zero-copy loading, there's no need to manage a pool of containers. Just load all the model weights onto Plasma and let Ray take care of replicating objects to nodes of the cluster as needed.\n",
    "\n",
    "A small Ray cluster can keep the weights for hundreds of models in shared memory. It can instantly dedicate the entire cluster's CPU resources to a single model. Then it can just as quickly retask all of those resources to a different model. You get an instant, optimal response to whatever workload the application sends your way. And no manual tuning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4075d9-2771-42cf-b308-40e9e5a8ffde",
   "metadata": {},
   "source": [
    "The best part about this approach is that deploying models with Ray and zero-copy is actually *easier* than deploying them with a traditional model serving framework. Here's the code to deploy our example model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25fe785-18c4-4561-886d-be5f9ac8ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 10:02:58,433\tINFO api.py:262 -- Updating deployment 'my_model'. component=serve deployment=my_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=84618)\u001b[0m 2022-04-01 10:02:58,440\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'my_model'. component=serve deployment=my_model\n",
      "2022-04-01 10:03:06,994\tINFO api.py:274 -- Deployment 'my_model' is ready at `http://127.0.0.1:8000/my_model`. component=serve deployment=my_model\n"
     ]
    }
   ],
   "source": [
    "from ray import serve\n",
    "\n",
    "\n",
    "# Step 1: Define a Ray Serve deployment\n",
    "@serve.deployment\n",
    "class MyDeployment:\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "        # Load the model and immediately move it to Plasma\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            'mrm8488/t5-base-finetuned-e2m-intent')\n",
    "        self._model_ref = zerocopy.extract_tensors(model)\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        '''\n",
    "        Web service entry point.\n",
    "\n",
    "        Args:\n",
    "            request: HTTP request object for a REST web service call\n",
    "                     in the form:\n",
    "                     { \"context\": \"<input text>\" }\n",
    "        '''\n",
    "        # Parse JSON. A real deployment would also sanitize the input.\n",
    "        json_request = await request.json()\n",
    "        text = json_request['context']\n",
    "\n",
    "        # Preprocessing\n",
    "        tokens = self._tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        raw_output = await zerocopy.call_model.remote(\n",
    "            self._model_ref, [], tokens, 'generate')\n",
    "\n",
    "        # Postprocessing\n",
    "        result_string = self._tokenizer.decode(raw_output[0])\n",
    "        result_string = result_string.replace('<pad>', '')\n",
    "        result_string = result_string[len(' '):-len('</s>')]\n",
    "\n",
    "        return {'intent': result_string}\n",
    "\n",
    "\n",
    "# Step 2: Attach the deployment to an HTTP endpoint\n",
    "MyDeployment.options(name='my_model', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "# There is no step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130c9b6-ae1e-4990-b7ad-13a54abbb957",
   "metadata": {},
   "source": [
    "Let's walk through this listing one part at a time. \n",
    "\n",
    "At the top of the listing, we create a [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) *deployment* by applying the `@serve.deployment` decorator to a Python class.\n",
    "\n",
    "Inside that Python class are two methods, `__init__()` and `__call__()`. The constructor, or `__init__()` method, is called once when Ray Serve creates a replica of the endpoint. This method starts by creating a Tokenizer object to translate between text and the numeric tensors that the model uses. Then the `__init__()` method loads the model from disk and immediately moves the model and its weights to Ray's Plasma object store.\n",
    "\n",
    "A second method, `__call__()`, handles incoming inference requests. This method first translates the request's data from JSON format. Then it performs some preprocessing to build the model's inputs. Next, the `__call__()` method uses `zerocopy.call_model` to run model inference in a background Ray task. Finally, the method retrieves the inference results from Ray's Plasma object store, performs some postprocessing, and sends a result back to the HTTP client.\n",
    "\n",
    "The `__call__()` method uses Python's [asyncio framework](https://docs.python.org/3/library/asyncio.html) to handle multiple simultaneous requests at once. Each active request will use a separate Ray task to run model inference.\n",
    "\n",
    "The last line of the listing instructs Ray Serve to deploy the deployment we've just created. Behind the scenes, Ray Serve creates a replicated HTTP endpoint that handles incoming requests. So we can call our model by passing the appropriate JSON data through an HTTP `PUT` or `GET` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eace12a-4e5c-43e0-ba1e-dda606f0d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"intent\": \"to be able to use the latest drivers\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(requests.put('http://127.0.0.1:8000/my_model',\n",
    "      '''\n",
    "      { \n",
    "         \"context\": \"I'm looking for some Windows drivers for a model 3X-Q.\" \n",
    "      }\n",
    "      ''').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c514dfe3-9e33-4a70-9b4a-74c97b25416b",
   "metadata": {},
   "source": [
    "As you can see, deploying this model on Ray with zero-copy model loading is a simple two-step process: first define a small Python class, then call the `deploy()` function. There's no need for any of the complicated steps, like creating Docker containers or implementing multi-step callbacks, that usually accompany scalable model deployment. \n",
    "\n",
    "There are no knobs to tune, because Ray creates as many new tasks as it needs to handle incoming requests and removes them when they are done. In the unlikely event that the preprocessing and postprocessing code exceeds the capacity of a single core, Ray Serve can automatically replicate the endpoint for unbounded scalability. If every node in the cluster becomes 100 percent loaded, [Ray's autoscaler](https://www.anyscale.com/blog/autoscaling-clusters-with-ray) can automatically add new nodes. All this happens in the background with no user intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4614552-0203-4ecc-a7c3-986c78c8c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Stop this notebook's copy of Ray so as not to interfere with the\n",
    "# copy in `ray_deploy.ipynb`\n",
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eccd9",
   "metadata": {},
   "source": [
    "## A Simple Benchmark\n",
    "\n",
    "We created a benchmark to test how well Ray Serve can adjust its configuration on the fly to handle a dynamic, bursty stream of application requests. The end-to-end scenario for our benchmark involves supporting an AI chatbot for customer care. The chatbot's conversational AI uses a preprogrammed [dialog flow](https://www.ibm.com/cloud/architecture/tutorials/watson_conversation_support?task=6) to control the interaction with the customer. Some of the nodes of this flow use machine learning models to guide their decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc5ddc-35bc-4c70-b3c8-c9e0debbc8bb",
   "metadata": {},
   "source": [
    "Our benchmark covers the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs four different types of models:\n",
    "* *Intent detection* models that determine what is the user's goal.\n",
    "* *Sentiment analysis* models that monitor the user's mood.\n",
    "* *Question answering* models that provide the answers to specific factual questions.\n",
    "* *Natural language generation* models that give the chatbot's responses a less scripted flavor.\n",
    "\n",
    "Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models.\n",
    "\n",
    "In a real application, you would want to train custom versions of each type\n",
    "of model for the topics your chatbot covers.\n",
    "Since we're only interested in measuring throughput and latency, we skipped that customization\n",
    "step and just used the most popular pretrained model from each category from the \n",
    "[Huggingface model marketplace](https://huggingface.co/models).\n",
    "\n",
    "Each of these models uses a [Transformer](https://arxiv.org/abs/1706.03762)-based neural network,\n",
    "with a *language model* and a task specific *head*, tuned over \n",
    "a domain-specific training set.  The table below summarizes the four models that we used.\n",
    "\n",
    "\n",
    "| Task                 | Model Name                                   | Language Model  | Model Size (in memory)\n",
    "| -----------          | -----------                                  | ------------    | --------------------\n",
    "| Intent Detection     | `mrm8488/t5-base-finetuned-e2m-intent`       | T5              | 1133 MiB\n",
    "| Sentiment Analysis   | `cardiffnlp/twitter-roberta-base-sentiment`  | RoBERTa         | 476 MiB\n",
    "| Question Answering   | `deepset/roberta-base-squad2`                | RoBERTa         | 474 MiB\n",
    "| Text Generation      | `gpt2`                                       | GPT-2           | 634 MiB\n",
    "\n",
    "\n",
    "Although all four models came from the same marketplace, they are quite diverse. The models use three different core language models: [Text-to-Text Transfer Transformer](https://arxiv.org/pdf/1910.10683.pdf) (T5) from Google Research, \n",
    "[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf) from Facebook AI, and [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) from OpenAI. And the models vary in size by almost a factor of 3.\n",
    "\n",
    "<!--\n",
    "The models also use two very different ways to package their preprocessing and postprocessing code. The intent and sentiment models provide small blocks of reference Python code, with the intent being that the user will adapt this reference code to the specific circumstances of the end-to-end appliction. The question answering and text generation models both use the Transformers library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines) to package their preprocessing and postprocesing code. \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6796f9-841b-4b8f-aaa7-8812dbbd097c",
   "metadata": {},
   "source": [
    "### Deploying the models\n",
    "\n",
    "The intent detection model in the benchmark is the same model that we have been using in all our example code so far in this article. We used exactly that code to deploy that model for the benchmark. To simulate having three versions of the model for three different languages, we instructed Ray Serve to deploy three copies of the model, one for each language, with a separate HTTP endpoint for each deployment.\n",
    "\n",
    "We followed a similar process for the remaining three model types to deploy a total of 12 models. You can find the full code that we used to deploy all 12 models in [this notebook](./ray_deploy.ipynb).\n",
    "\n",
    "As a baseline to compare against our Ray Serve deployment, we also deployed the 12 models using [TorchServe](https://pytorch.org/serve/) with that system's default configuration. See [this notebook](./torchserve.ipynb) for details of the TorchServe deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b1aa",
   "metadata": {},
   "source": [
    "*Note (not to be included in the blog): earlier versions of this notebook implemented the baseline model deployment with a pool of Ray actors. That older version is preserved in [a separate notebook](./ray_baseline.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4316-86af-481f-8cc0-887a88c30c0d",
   "metadata": {},
   "source": [
    "## Running the Benchmark\n",
    "\n",
    "Now that we have deployed each of our models with a web service front end, we can define a benchmark that sends inference traffic to these web service endpoints and measures response time.\n",
    "\n",
    "We wrote a simple discrete event simulation to simulate a variable number of customers interacting with the chatbot. Each simulated customer types a series of chat messages, waiting for a randomly-distributed \"think time\" between messages. Our simulation draws these thnk times randomly from a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) with a mean value of 10 seconds. Each chat message results in a single model inference request, with the choice of models drawn randomly from another Poisson distribution.\n",
    "\n",
    "The benchmark runs the simulation to generate a trace, then plays back the trace, sending requests to the backend under test and measuring the end-to-end latency of each request. We repeat this process of generating and playing back the trace, gradually ramping up the average request rate of the bursty traffic until requests start timing out. We used a timeout threshold of 5 seconds.\n",
    "\n",
    "We ran this benchmark against our two model deployments, using the same trace of requests. Both runs used the same hardware, a 16-core [IBM Cloud VM](https://www.ibm.com/cloud/virtual-servers), to run the entire benchmark, including the client portion of the benchmark, the serving framework, and the processes that performed model inference.\n",
    "\n",
    "The code that we used to implement the benchmark can be found [here](./benchmark.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403c8b3-7dda-4c32-86ec-1ba0eee6be34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (not in blog) Baseline benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0e951-9592-48f2-94e4-07c328e2965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Probe the management API to verify that TorchServe is running.\n",
    "try:\n",
    "    print(requests.get('http://127.0.0.1:8081/models').json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Stop notebook execution\n",
    "    raise ValueError('TorchServe does not appear to be running. Please start TorchServe.') from None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f99d8a4-c1b4-4529-a984-a1eb9cc47344",
   "metadata": {},
   "source": [
    "# This call may be disabled to avoid overwriting local results.\n",
    "# Toggle cell type to \"code\" to run.\n",
    "!python3 benchmark.py 8080 outputs/baseline.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c948be2-7759-4ee1-866c-ef52df1cd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Make sure that TorchServe is shut down before we continue.\n",
    "torchserve_is_running = True\n",
    "try:\n",
    "    requests.get('http://127.0.0.1:8081/models').json()\n",
    "except requests.exceptions.ConnectionError:\n",
    "    torchserve_is_running = False\n",
    "if torchserve_is_running:\n",
    "    raise ValueError('Please shut down TorchServe before continuing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06033854-2849-450f-b7e2-382f79b5654b",
   "metadata": {},
   "source": [
    "### (not in blog) Optimized benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71e2ec-77c9-4ef1-abea-dc3fad338794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the Ray models are up\n",
    "try:\n",
    "    INTENT_INPUT = {\n",
    "        'context':\n",
    "            (\"I came here to eat chips and beat you up, \"\n",
    "             \"and I'm all out of chips.\")\n",
    "    }\n",
    "    requests.put(\n",
    "        'http://127.0.0.1:8000/predictions/intent_en', \n",
    "        json.dumps(INTENT_INPUT)).json()\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    raise ValueError('Please start up the zero-copy model deployment before continuing.') from None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ef7404d-354d-4a8e-aef6-b3906be6d0ad",
   "metadata": {},
   "source": [
    "# This call may be disabled to avoid overwriting local results.\n",
    "# Toggle cell type to \"code\" to run.\n",
    "!python3 benchmark.py 8000 outputs/zerocopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15aebd-46d9-41f1-9ae4-1947cb51b0b1",
   "metadata": {},
   "source": [
    "### (not in blog) Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4f8696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog\n",
    "\n",
    "# Aggregate benchmark results.\n",
    "def compute_stats(results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    timeout_results = results_df[results_df['result_code'] != 200]\n",
    "    success_results = results_df[results_df['result_code'] == 200]\n",
    "\n",
    "    timeout_counts = (\n",
    "        timeout_results\n",
    "        .groupby('num_users')\n",
    "        .aggregate({'request_id': 'count'})\n",
    "        .rename(columns={'request_id': 'timeouts'}))\n",
    "    stats = (\n",
    "        success_results\n",
    "        .groupby('num_users')\n",
    "        .aggregate({'latency': ['mean', 'median', 'max'],\n",
    "                    'request_id': 'count'}))\n",
    "\n",
    "    # Column names come out from the aggregations all messed up\n",
    "    stats.columns=['mean', 'median', 'max', 'successes']\n",
    "    stats = stats.join(timeout_counts).fillna(0)\n",
    "    stats['timeout_fraction'] = stats['timeouts'] / (stats['successes'] \n",
    "                                                     + stats['timeouts'])\n",
    "    stats['timeouts'] = stats['timeouts'].astype(int)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def maybe_generate_agg(prefix: str):\n",
    "    '''\n",
    "    Regenerate aggregate results for a benchmark run if a trace\n",
    "    is present.\n",
    "\n",
    "    :param prefix: Name of run, i.e. 'baseline' or 'zerocopy'\n",
    "    '''\n",
    "    if os.path.exists(f'outputs/{prefix}.csv'):\n",
    "        results = pd.read_csv(f'outputs/{prefix}.csv')\n",
    "        stats = compute_stats(results)\n",
    "        stats.to_csv(f'outputs/{prefix}_agg.csv')\n",
    "\n",
    "\n",
    "maybe_generate_agg('baseline')\n",
    "maybe_generate_agg('zerocopy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d9f5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_users</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>successes</th>\n",
       "      <th>timeouts</th>\n",
       "      <th>timeout_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.405181</td>\n",
       "      <td>0.442816</td>\n",
       "      <td>0.920758</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.504624</td>\n",
       "      <td>0.478369</td>\n",
       "      <td>1.316841</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.705766</td>\n",
       "      <td>0.499433</td>\n",
       "      <td>2.214844</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1.319962</td>\n",
       "      <td>1.112257</td>\n",
       "      <td>3.718396</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>1.767164</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>5.005147</td>\n",
       "      <td>95</td>\n",
       "      <td>76</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>0.833144</td>\n",
       "      <td>0.108438</td>\n",
       "      <td>4.975030</td>\n",
       "      <td>74</td>\n",
       "      <td>125</td>\n",
       "      <td>0.628141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_users      mean    median       max  successes  timeouts  \\\n",
       "0         10  0.405181  0.442816  0.920758         57         0   \n",
       "1         15  0.504624  0.478369  1.316841         88         0   \n",
       "2         20  0.705766  0.499433  2.214844        114         0   \n",
       "3         25  1.319962  1.112257  3.718396        141         0   \n",
       "4         30  1.767164  0.797872  5.005147         95        76   \n",
       "5         35  0.833144  0.108438  4.975030         74       125   \n",
       "\n",
       "   timeout_fraction  \n",
       "0          0.000000  \n",
       "1          0.000000  \n",
       "2          0.000000  \n",
       "3          0.000000  \n",
       "4          0.444444  \n",
       "5          0.628141  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog\n",
    "# Load up the baseline results\n",
    "baseline_stats = pd.read_csv('outputs/baseline_agg.csv')\n",
    "baseline_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5bb6f85-63ad-427d-875c-24ca86ad4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_users</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>successes</th>\n",
       "      <th>timeouts</th>\n",
       "      <th>timeout_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.447012</td>\n",
       "      <td>0.418391</td>\n",
       "      <td>2.191628</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.471699</td>\n",
       "      <td>0.434592</td>\n",
       "      <td>1.899624</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.563979</td>\n",
       "      <td>0.440218</td>\n",
       "      <td>2.402398</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.615734</td>\n",
       "      <td>0.452039</td>\n",
       "      <td>2.400618</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.640276</td>\n",
       "      <td>0.456938</td>\n",
       "      <td>2.431353</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>0.749192</td>\n",
       "      <td>0.505191</td>\n",
       "      <td>2.568592</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>0.779392</td>\n",
       "      <td>0.537649</td>\n",
       "      <td>3.076906</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "      <td>0.850357</td>\n",
       "      <td>0.546175</td>\n",
       "      <td>3.438410</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>0.741535</td>\n",
       "      <td>0.535226</td>\n",
       "      <td>2.448053</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>0.769926</td>\n",
       "      <td>0.532423</td>\n",
       "      <td>3.353690</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>0.749726</td>\n",
       "      <td>0.536834</td>\n",
       "      <td>3.788003</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>65</td>\n",
       "      <td>0.811626</td>\n",
       "      <td>0.556395</td>\n",
       "      <td>3.554438</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70</td>\n",
       "      <td>0.755175</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>3.610731</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>75</td>\n",
       "      <td>0.811418</td>\n",
       "      <td>0.592266</td>\n",
       "      <td>3.569565</td>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>80</td>\n",
       "      <td>0.850736</td>\n",
       "      <td>0.627164</td>\n",
       "      <td>4.291506</td>\n",
       "      <td>449</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>85</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.606239</td>\n",
       "      <td>3.572249</td>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>90</td>\n",
       "      <td>0.910004</td>\n",
       "      <td>0.641215</td>\n",
       "      <td>4.492730</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>95</td>\n",
       "      <td>0.950791</td>\n",
       "      <td>0.696543</td>\n",
       "      <td>3.309551</td>\n",
       "      <td>531</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>0.839075</td>\n",
       "      <td>0.615966</td>\n",
       "      <td>3.727902</td>\n",
       "      <td>561</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>105</td>\n",
       "      <td>0.909114</td>\n",
       "      <td>0.718076</td>\n",
       "      <td>3.600761</td>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>110</td>\n",
       "      <td>0.884011</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>4.591192</td>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>115</td>\n",
       "      <td>0.825233</td>\n",
       "      <td>0.650660</td>\n",
       "      <td>4.281049</td>\n",
       "      <td>643</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>120</td>\n",
       "      <td>0.846136</td>\n",
       "      <td>0.679775</td>\n",
       "      <td>3.683052</td>\n",
       "      <td>670</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>125</td>\n",
       "      <td>0.909696</td>\n",
       "      <td>0.732213</td>\n",
       "      <td>4.344499</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>130</td>\n",
       "      <td>0.935397</td>\n",
       "      <td>0.715681</td>\n",
       "      <td>4.290693</td>\n",
       "      <td>725</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>135</td>\n",
       "      <td>0.895279</td>\n",
       "      <td>0.707690</td>\n",
       "      <td>4.493290</td>\n",
       "      <td>752</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>140</td>\n",
       "      <td>0.939610</td>\n",
       "      <td>0.771092</td>\n",
       "      <td>4.525835</td>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>145</td>\n",
       "      <td>0.982242</td>\n",
       "      <td>0.816506</td>\n",
       "      <td>4.306702</td>\n",
       "      <td>809</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>150</td>\n",
       "      <td>1.056291</td>\n",
       "      <td>0.910722</td>\n",
       "      <td>4.217720</td>\n",
       "      <td>835</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>155</td>\n",
       "      <td>1.076282</td>\n",
       "      <td>0.921286</td>\n",
       "      <td>4.448676</td>\n",
       "      <td>860</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>160</td>\n",
       "      <td>1.048450</td>\n",
       "      <td>0.909684</td>\n",
       "      <td>4.681247</td>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>165</td>\n",
       "      <td>1.156882</td>\n",
       "      <td>1.010279</td>\n",
       "      <td>4.910607</td>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>170</td>\n",
       "      <td>1.117601</td>\n",
       "      <td>1.046078</td>\n",
       "      <td>4.676699</td>\n",
       "      <td>945</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>175</td>\n",
       "      <td>1.178816</td>\n",
       "      <td>1.070963</td>\n",
       "      <td>4.498291</td>\n",
       "      <td>970</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>180</td>\n",
       "      <td>1.177107</td>\n",
       "      <td>0.969258</td>\n",
       "      <td>4.205892</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>185</td>\n",
       "      <td>1.116465</td>\n",
       "      <td>0.960508</td>\n",
       "      <td>3.779688</td>\n",
       "      <td>1028</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>190</td>\n",
       "      <td>1.123547</td>\n",
       "      <td>0.994436</td>\n",
       "      <td>4.069702</td>\n",
       "      <td>1057</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>195</td>\n",
       "      <td>1.396095</td>\n",
       "      <td>1.389106</td>\n",
       "      <td>4.618074</td>\n",
       "      <td>1085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>200</td>\n",
       "      <td>1.239189</td>\n",
       "      <td>0.952826</td>\n",
       "      <td>4.700874</td>\n",
       "      <td>1110</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>205</td>\n",
       "      <td>1.504536</td>\n",
       "      <td>1.464199</td>\n",
       "      <td>3.967543</td>\n",
       "      <td>1135</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>210</td>\n",
       "      <td>1.657478</td>\n",
       "      <td>1.446270</td>\n",
       "      <td>4.974497</td>\n",
       "      <td>1163</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>215</td>\n",
       "      <td>2.114105</td>\n",
       "      <td>2.371931</td>\n",
       "      <td>4.029741</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>220</td>\n",
       "      <td>2.523392</td>\n",
       "      <td>3.108910</td>\n",
       "      <td>4.755757</td>\n",
       "      <td>1212</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>225</td>\n",
       "      <td>3.023692</td>\n",
       "      <td>3.851934</td>\n",
       "      <td>5.010300</td>\n",
       "      <td>1172</td>\n",
       "      <td>68</td>\n",
       "      <td>0.054839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>230</td>\n",
       "      <td>2.567905</td>\n",
       "      <td>2.717991</td>\n",
       "      <td>5.013681</td>\n",
       "      <td>743</td>\n",
       "      <td>528</td>\n",
       "      <td>0.415421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>235</td>\n",
       "      <td>2.258633</td>\n",
       "      <td>1.857889</td>\n",
       "      <td>5.007427</td>\n",
       "      <td>641</td>\n",
       "      <td>657</td>\n",
       "      <td>0.506163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>240</td>\n",
       "      <td>1.962504</td>\n",
       "      <td>1.896545</td>\n",
       "      <td>5.006300</td>\n",
       "      <td>534</td>\n",
       "      <td>790</td>\n",
       "      <td>0.596677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>245</td>\n",
       "      <td>2.249069</td>\n",
       "      <td>2.388798</td>\n",
       "      <td>4.996917</td>\n",
       "      <td>510</td>\n",
       "      <td>843</td>\n",
       "      <td>0.623060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_users      mean    median       max  successes  timeouts  \\\n",
       "0          10  0.447012  0.418391  2.191628         57         0   \n",
       "1          15  0.471699  0.434592  1.899624         88         0   \n",
       "2          20  0.563979  0.440218  2.402398        114         0   \n",
       "3          25  0.615734  0.452039  2.400618        141         0   \n",
       "4          30  0.640276  0.456938  2.431353        171         0   \n",
       "5          35  0.749192  0.505191  2.568592        199         0   \n",
       "6          40  0.779392  0.537649  3.076906        228         0   \n",
       "7          45  0.850357  0.546175  3.438410        256         0   \n",
       "8          50  0.741535  0.535226  2.448053        286         0   \n",
       "9          55  0.769926  0.532423  3.353690        312         0   \n",
       "10         60  0.749726  0.536834  3.788003        340         0   \n",
       "11         65  0.811626  0.556395  3.554438        370         0   \n",
       "12         70  0.755175  0.545371  3.610731        399         0   \n",
       "13         75  0.811418  0.592266  3.569565        424         0   \n",
       "14         80  0.850736  0.627164  4.291506        449         0   \n",
       "15         85  0.848362  0.606239  3.572249        477         0   \n",
       "16         90  0.910004  0.641215  4.492730        504         0   \n",
       "17         95  0.950791  0.696543  3.309551        531         1   \n",
       "18        100  0.839075  0.615966  3.727902        561         0   \n",
       "19        105  0.909114  0.718076  3.600761        590         0   \n",
       "20        110  0.884011  0.681573  4.591192        615         0   \n",
       "21        115  0.825233  0.650660  4.281049        643         0   \n",
       "22        120  0.846136  0.679775  3.683052        670         1   \n",
       "23        125  0.909696  0.732213  4.344499        697         0   \n",
       "24        130  0.935397  0.715681  4.290693        725         0   \n",
       "25        135  0.895279  0.707690  4.493290        752         0   \n",
       "26        140  0.939610  0.771092  4.525835        780         0   \n",
       "27        145  0.982242  0.816506  4.306702        809         0   \n",
       "28        150  1.056291  0.910722  4.217720        835         1   \n",
       "29        155  1.076282  0.921286  4.448676        860         1   \n",
       "30        160  1.048450  0.909684  4.681247        889         0   \n",
       "31        165  1.156882  1.010279  4.910607        915         0   \n",
       "32        170  1.117601  1.046078  4.676699        945         0   \n",
       "33        175  1.178816  1.070963  4.498291        970         1   \n",
       "34        180  1.177107  0.969258  4.205892       1000         1   \n",
       "35        185  1.116465  0.960508  3.779688       1028         2   \n",
       "36        190  1.123547  0.994436  4.069702       1057         1   \n",
       "37        195  1.396095  1.389106  4.618074       1085         1   \n",
       "38        200  1.239189  0.952826  4.700874       1110         2   \n",
       "39        205  1.504536  1.464199  3.967543       1135         1   \n",
       "40        210  1.657478  1.446270  4.974497       1163         2   \n",
       "41        215  2.114105  2.371931  4.029741       1188         3   \n",
       "42        220  2.523392  3.108910  4.755757       1212         3   \n",
       "43        225  3.023692  3.851934  5.010300       1172        68   \n",
       "44        230  2.567905  2.717991  5.013681        743       528   \n",
       "45        235  2.258633  1.857889  5.007427        641       657   \n",
       "46        240  1.962504  1.896545  5.006300        534       790   \n",
       "47        245  2.249069  2.388798  4.996917        510       843   \n",
       "\n",
       "    timeout_fraction  \n",
       "0           0.000000  \n",
       "1           0.000000  \n",
       "2           0.000000  \n",
       "3           0.000000  \n",
       "4           0.000000  \n",
       "5           0.000000  \n",
       "6           0.000000  \n",
       "7           0.000000  \n",
       "8           0.000000  \n",
       "9           0.000000  \n",
       "10          0.000000  \n",
       "11          0.000000  \n",
       "12          0.000000  \n",
       "13          0.000000  \n",
       "14          0.000000  \n",
       "15          0.000000  \n",
       "16          0.000000  \n",
       "17          0.001880  \n",
       "18          0.000000  \n",
       "19          0.000000  \n",
       "20          0.000000  \n",
       "21          0.000000  \n",
       "22          0.001490  \n",
       "23          0.000000  \n",
       "24          0.000000  \n",
       "25          0.000000  \n",
       "26          0.000000  \n",
       "27          0.000000  \n",
       "28          0.001196  \n",
       "29          0.001161  \n",
       "30          0.000000  \n",
       "31          0.000000  \n",
       "32          0.000000  \n",
       "33          0.001030  \n",
       "34          0.000999  \n",
       "35          0.001942  \n",
       "36          0.000945  \n",
       "37          0.000921  \n",
       "38          0.001799  \n",
       "39          0.000880  \n",
       "40          0.001717  \n",
       "41          0.002519  \n",
       "42          0.002469  \n",
       "43          0.054839  \n",
       "44          0.415421  \n",
       "45          0.506163  \n",
       "46          0.596677  \n",
       "47          0.623060  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog\n",
    "zerocopy_stats = pd.read_csv('outputs/zerocopy_agg.csv')\n",
    "zerocopy_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e277a52-2556-4ff3-96a5-59cde4322f7a",
   "metadata": {},
   "source": [
    "The chart below shows the results of this benchmark. The X axis of the chart measures the number of simulated customers interacting with the chatbot. The Y axis measures what fraction of users' chat messages exceed the 5-second timeout limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41323031-2b33-4835-a40f-cb01e503388a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8bc66ff790>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't include this code in the blog, but do include the chart.\n",
    "# Plot the two sets of results against each other.\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(baseline_stats['num_users'],\n",
    "         baseline_stats['timeout_fraction'],\n",
    "         '-o', label='Without zero-copy loading')\n",
    "plt.plot(zerocopy_stats['num_users'],\n",
    "         zerocopy_stats['timeout_fraction'],\n",
    "         '-o', label=\"With zero-copy loading\")\n",
    "plt.xlabel('Number of Simultaneous Chat Sessions')\n",
    "plt.ylabel('Timeout Fraction\\n(lower is better)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb81ed-cdaa-46af-a3e4-14f271f09e35",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Annotated copy of the above chart that doesn't change when this notebook is recomputed:*\n",
    "![Performance graph](../images/zerocopy_perf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ea8c9-3a23-4156-a0f0-477ba3f154dc",
   "metadata": {},
   "source": [
    "The baseline deployment can handle up to 30 simultaneous chat sessions. Beyond 30 sessions, the number of timeouts increases rapidly and the system becomes unstable. The baseline deployment uses a fixed allocation of CPU and memory because it hasn't been extensively tuned. When there are more than 30 simulated customers on the line, the CPU resources that are dedicated to the model with the highest traffic can no longer keep up with the increasingly large bursts of chat messages. \n",
    "\n",
    "Our Ray-based deployment with zero-copy model loading, on the other hand, handles up to 220 sessions without any timeouts. This performance represents a 7x improvement in scalability without any tuning. The zero-copy based deployment is able to instantly retask the underlying hardware resources to whatever models are currently experiencing high traffic.\n",
    "\n",
    "This result shows how zero-copy model loading gives you zero-effort performance tuning.\n",
    "\n",
    "If you'd like to try this technology out yourself, try the `zerocopy` package (TODO: Link to PyPI once package is posted)\n",
    "\n",
    "\n",
    "*All the code used in this blog post is available (TODO: link goes here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df620ecd-6b86-48ab-bcdb-856e9f1b51ad",
   "metadata": {},
   "source": [
    "# Extra material not included in the blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bdd06-d6a5-409b-946d-f2b2426f585b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model inference pipelines\n",
    "\n",
    "\n",
    "With the `zerocopy` library's `extract_tensors()` and `call_model()` functions, you can apply zero-copy model loading to a Pytorch model with three lines of Python. But what about the end-to-end program that this model came from? \n",
    "\n",
    "Most machine learning models require additional code to apply them in a meaningful way. NLP models in particular require *preprocessing* to convert natural language text into a format the model can understand and *postprocessing* to convert the model's answer into a format that a person can understand. \n",
    "\n",
    "Some models provide this preprocessing and postprocessing code as simple reference code, with the intent that someone will copy that initial version of the code and turn it into a full production-ready application.\n",
    "\n",
    "It's also common to package the model as *pipeline* that includes preprocessing, inference, and postprocessing bundled together in a single Python object. For example, the Transformers library's BERT model that we have been using in our examples so far comes with a pipeline that performs the end-to-end task of *masked language modeling*: Identifying the most likely word to fill in a blank.\n",
    "\n",
    "Unlike the \"example reference code\" approach, the Pipelines API's end-to-end inference code is intended for direct production use. It includes support for model retraining, as well as performance optimizations like batching and GPU acceleration, plus code for handling corner cases like long input strings. This prepackaged code can save a lot of time, provided that your application is structured in a way that can easily accomodate a large block of non-modifiable third-party Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938883f-cd5f-4ed6-b2f7-d195652859ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pipeline = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "bert_pipeline('All your base are belong to [MASK].')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0793b8-4c0e-478d-b8bf-02395699ed06",
   "metadata": {},
   "source": [
    "The `zerocopy` library includes a function `rewrite_pipeline` that transforms any models embedded into Python object into Ray tasks that use zero-copy model loading to load weights. If we apply this function to a pipeline, the resulting rewritten pipeline faithfully performs all the preprocessing and postprocessing that the original pipeline performed. However, this rewritten pipeline runs the embedded PyTorch model in remote Ray tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add9f8b-b401-4cf4-accb-c753bfe6a26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "\n",
    "zero_copy_bert_pipeline = zerocopy.rewrite_pipeline(bert_pipeline)\n",
    "print(f\"Heap memory used before rewrite: {asizeof.asizeof(bert_pipeline)}\")\n",
    "print(f\" Heap memory used after rewrite: {asizeof.asizeof(zero_copy_bert_pipeline)}\")\n",
    "\n",
    "print(f\"Output before rewrite: {bert_pipeline('All your base are belong to [MASK].')[0]['score']}\")\n",
    "print(f\" Output after rewrite: {zero_copy_bert_pipeline('All your base are belong to [MASK].')[0]['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33920dd8-32ff-47cb-976e-13251819318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Alternate version uses a multithreaded actor to run the pipeline via\n",
    "# its __call__ method.\n",
    "\n",
    "@ray.remote\n",
    "class PipelineActor:\n",
    "    '''\n",
    "    Threaded Ray actor\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        pipeline_tmp = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(pipeline_tmp)\n",
    "\n",
    "    def run(self, input_: str):\n",
    "        # Model inference calls inside this pipeline will happen in remote\n",
    "        # Ray tasks.\n",
    "        return self._pipeline(input_)\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class MyDeployment2:\n",
    "    def __init__(self):\n",
    "        self._pipeline_actor = PipelineActor.options(max_concurrency=100,\n",
    "                                                     num_cpus=0.1).remote()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "        input_ = json_request['input']\n",
    "\n",
    "        result = await self._pipeline_actor.run.remote(input_)\n",
    "        return result\n",
    "\n",
    "MyDeployment2.options(name='my_model2', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "print(requests.put('http://127.0.0.1:8000/my_model2', \n",
    "      '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95790e6e-c8a4-4419-8b88-b50b8ead26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include this cell in the blog.\n",
    "# Alternate version uses a thread pool and acyncio's run_in_executor()\n",
    "# method run the pipeline via its __call__ method.\n",
    "\n",
    "@serve.deployment\n",
    "class MyDeployment3:\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        pipeline_tmp = transformers.pipeline('fill-mask', model='bert-base-uncased')\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(pipeline_tmp)\n",
    "\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        '''\n",
    "        Web service entry point.\n",
    "\n",
    "        Args:\n",
    "            request: HTTP request object for a REST web service call\n",
    "                     in the form:\n",
    "                     { \"input\": \"<input text with [MASK]>\" }\n",
    "        '''\n",
    "        # Parse JSON. A real deployment would also sanitize the input.\n",
    "        json_request = await request.json()\n",
    "        masked_string = json_request['input']\n",
    "\n",
    "        # The original `transformers` code is not async-aware, so we\n",
    "        # call it from `run_in_executor()`.\n",
    "        # Preprocessing and postprocessing code will happen inside this\n",
    "        # process, but model inference will occur in a remote Ray task.\n",
    "        # While that task is running, the local thread will block on\n",
    "        # a call to `ray.get()`\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(masked_string))\n",
    "        return result\n",
    "\n",
    "MyDeployment3.options(name='my_model3', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "print(requests.put('http://127.0.0.1:8000/my_model3', \n",
    "      '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bfbbb1e-0f5e-45b5-8462-6e2f9d526bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Old code that demos `zerocopy` with the intent model\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference without zero-copy loading\n",
    "print('Result without zero-copy loading: '\n",
    "      + str(intent_model.generate(**features)))\n",
    "\n",
    "# Inference with zero-copy loading\n",
    "intent_model_ref = ray.put(zerocopy.extract_tensors(intent_model))\n",
    "print(' Result *with* zero-copy loading: ' +\n",
    "      str(ray.get(zerocopy.call_model.remote(\n",
    "          intent_model_ref, [], features, 'generate'))))\n",
    "          \n",
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit intent_model.generate(**features)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(zerocopy.call_model.remote(intent_model_ref, [], features, 'generate'))\n",
    "\n",
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        intent_model.generate(**features)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [\n",
    "        zerocopy.call_model.remote(intent_model_ref, [], features, 'generate')\n",
    "        for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 50\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58769647-0991-4da2-ac40-61960952b09c",
   "metadata": {},
   "source": [
    "# Old code that demos `zerocopy` on the QA model pipeline\n",
    "zero_copy_qa = zerocopy.rewrite_pipeline(qa_pipeline)\n",
    "print(f\"Before rewrite: {qa_pipeline(**QA_INPUT)}\")\n",
    "print(f\" After rewrite: {zero_copy_qa(**QA_INPUT)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc82a3f1-a6d9-4995-830b-d5b43539ab62",
   "metadata": {},
   "source": [
    "# Old code that deploys bert-base-uncased with Ray Serve\n",
    "from ray import serve\n",
    "\n",
    "\n",
    "# Step 1: Define a Ray Serve deployment\n",
    "@serve.deployment\n",
    "class MyDeployment:\n",
    "    def __init__(self):\n",
    "        transformers.logging.set_verbosity_error()\n",
    "\n",
    "        # Load the entire pipeline\n",
    "        self._pipeline = transformers.pipeline('fill-mask', \n",
    "                                               model='bert-base-uncased')\n",
    "\n",
    "        # Move the model weights to Plasma\n",
    "        self._pipeline.model = zerocopy.extract_tensors(self._pipeline.model)\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        '''\n",
    "        Web service entry point.\n",
    "\n",
    "        Args:\n",
    "            request: HTTP request object for a REST web service call\n",
    "                     in the form:\n",
    "                     { \"input\": \"<input text with [MASK]>\" }\n",
    "        '''\n",
    "        # Parse JSON. A real deployment would also sanitize the input.\n",
    "        json_request = await request.json()\n",
    "        input_ = json_request['input']\n",
    "\n",
    "        # Preprocessing\n",
    "        features = self._pipeline.preprocess(input_)\n",
    "\n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        raw_output = await zerocopy.call_model.remote(\n",
    "            self._pipeline.model, [], features)\n",
    "\n",
    "        # Postprocessing\n",
    "        raw_output[\"input_ids\"] = features[\"input_ids\"]\n",
    "        return self._pipeline.postprocess(raw_output)\n",
    "\n",
    "\n",
    "# Step 2: Attach the deployment to an HTTP endpoint\n",
    "MyDeployment.options(name='my_model', ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "# There is no step 3.\n",
    "\n",
    "# Show using the endpoint\n",
    "print(requests.put('http://127.0.0.1:8000/my_model',\n",
    "      '{ \"input\": \"All your base are belong to [MASK].\" }').text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afa7e0f34d224467fd24b0cfa9c212efa127bdf53fe1c4e3ddf54198f34a39e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
