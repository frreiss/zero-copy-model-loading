{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9c47fd-5631-4f71-999c-39141186ac33",
   "metadata": {},
   "source": [
    "# ray_deploy.ipynb\n",
    "\n",
    "Optimized model serving implementation from the [benchmark notebook](./benchmark.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70512e88-058d-4644-a86a-71cd8334f57c",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c73a81-6f3d-40a4-a0c7-20a955d4aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "import zerocopy\n",
    "\n",
    "import concurrent\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfec84ca-c245-44ec-b5e3-4d68aa6ea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "GENERATE_MODEL_NAME = 'gpt2'\n",
    "\n",
    "\n",
    "INTENT_INPUT = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and beat you up, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "\n",
    "QA_INPUT = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756da010-486f-49b7-8023-74af80661c11",
   "metadata": {},
   "source": [
    "## Example model code\n",
    "\n",
    "This is the single-node code on which the Serve deployments below are based.  Some of this code is duplicated in `benchmark.ipynb` and should be kept in sync."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f636547-d45b-4acd-ab7d-4268f2957416",
   "metadata": {},
   "source": [
    "### Intent model\n",
    "\n",
    "For our intent detection models, we'll use the model [`mrm8488/t5-base-finetuned-e2m-intent`](https://huggingface.co/mrm8488/t5-base-finetuned-e2m-intent).\n",
    "\n",
    "The intent model comes as three parts: A *tokenizer* that converts raw text into a sequence numeric token IDs, a core *model* that transforms these token sequences, and *preprocessing and postprocessing code* to choreograph the usage of the first two parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9bad291-e541-45f8-9a7a-d6c7b020f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to eat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block also appears in `benchmark.ipynb`\n",
    "\n",
    "# Load model and tokenizer\n",
    "intent_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "intent_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    INTENT_MODEL_NAME)\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference\n",
    "output = intent_model.generate(**features)\n",
    "\n",
    "# Postprocessing\n",
    "result_string = intent_tokenizer.decode(output[0])\n",
    "result_string = result_string.replace('<pad>', '')\n",
    "result_string = result_string[len(' '):-len('</s>')]\n",
    "\n",
    "result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46f6c1-720a-49cb-8078-400e286e7dbf",
   "metadata": {},
   "source": [
    "### Sentiment model\n",
    "\n",
    "For our sentiment models, we'll use model [`cardiffnlp/twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment).\n",
    "\n",
    "Like the intent model, the sentiment model is packaged as a tokenizer, a core model, and instructions for pre- and post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9c38c0-1210-431e-8024-9b47f77373aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 0.5419477820396423,\n",
       " 'neutral': 0.38251084089279175,\n",
       " 'negative': 0.07554134726524353}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model loading\n",
    "sentiment_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    SENTIMENT_MODEL_NAME)\n",
    "sentiment_model = (transformers.AutoModelForSequenceClassification\n",
    "                .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "\n",
    "# Preprocessing\n",
    "encoded_input = sentiment_tokenizer(SENTIMENT_INPUT['context'], \n",
    "                                 return_tensors='pt')   \n",
    "\n",
    "# Inference\n",
    "output = sentiment_model(**encoded_input)\n",
    "\n",
    "# Postprocessing\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = scipy.special.softmax(scores)\n",
    "scores = [float(s) for s in scores]\n",
    "scores = {k: v for k, v in zip(['positive', 'neutral', 'negative'], scores)}\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59f03b-dd87-4d87-845d-4123ccdad396",
   "metadata": {},
   "source": [
    "### Question Answering Model\n",
    "\n",
    "For our question answering models, we'll use the model [`deepset/roberta-base-squad2`](https://huggingface.co/deepset/roberta-base-squad2).\n",
    "\n",
    "Unlike the intent and sentiment models, the question answering model comes prepackaged as a `question-answering` pipeline via the `tokenizers` library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines). \n",
    "\n",
    "So we can load and run all parts of the model, including pre- and post-processing code, by creating an instance of the pipeline class. The pipeline object has methods `preprocess()`, `forward()`, and `postprocess()` to perform preprocessing, inference, and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96486779-6bc5-4a3f-8dd2-6faeeb117ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "qa_pipeline = transformers.pipeline('question-answering',\n",
    "                                    model=QA_MODEL_NAME)\n",
    "# Preprocessing (returns a Python generator)\n",
    "qa_pre = qa_pipeline.preprocess(qa_pipeline.create_sample(**QA_INPUT))\n",
    "\n",
    "# Inference\n",
    "qa_output = (qa_pipeline.forward(example) for example in qa_pre)\n",
    "\n",
    "# Postprocessing\n",
    "qa_result = qa_pipeline.postprocess(qa_output)\n",
    "\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c376f12-e679-4f37-8fe7-8ec336c009f8",
   "metadata": {},
   "source": [
    "There is also a convenience method `__call__()` that runs all three phases of processing in sequence.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b72d2a-8d20-48a6-9094-7189d2b76be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code also appears in `benchmark.ipynb`\n",
    "\n",
    "# Loading the model and associated resources\n",
    "qa_pipeline = transformers.pipeline('question-answering',\n",
    "                                    model=QA_MODEL_NAME)\n",
    "# Preprocessing, inference, and postprocessing all happen in\n",
    "# the Python object's the __call__() method.\n",
    "qa_result = qa_pipeline(**QA_INPUT)\n",
    "\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdc62b-d777-4b97-b144-dac251a68865",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Natural Language Generation Model\n",
    "\n",
    "For natural language generation, we'll use the [`gpt2`](https://huggingface.co/gpt2) language model. Like the question answering model, this natural language generation model comes wrapped in a `tokenizers` pipeline class. The class's `__call__()` method performs all the steps necessary to run end-to-end inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73389972-e841-45c5-a061-44a7696a4652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'All your base are all mine\\n\\n\"This means there are not many of mine.\"\\n\\nOne of my sisters has a knife and she\\'d been thinking of me. She\\'d got this over with, just because the knife was not on my'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "generate_pipeline = transformers.pipeline(\n",
    "    'text-generation', model=GENERATE_MODEL_NAME)\n",
    "pad_token_id = generate_pipeline.tokenizer.eos_token_id\n",
    "\n",
    "# Preprocessing\n",
    "generate_pre = generate_pipeline.preprocess(**GENERATE_INPUT)\n",
    "\n",
    "# Inference\n",
    "generate_output = generate_pipeline.forward(generate_pre,\n",
    "                                            pad_token_id=pad_token_id)\n",
    "\n",
    "# Postprocessing\n",
    "generate_result = generate_pipeline.postprocess(generate_output)\n",
    "generate_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa4967-b9a1-4843-820d-98cc3f775523",
   "metadata": {},
   "source": [
    "## Start Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3111006c-4a4a-4301-9e15-cc761ddb33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 11:35:18,780\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:35:23,196\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:35:23,308\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SioqGl:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-02 11:35:23,769\tINFO api.py:475 -- Started Serve instance in namespace '8bf422d7-bce3-4383-b4fe-28ad2a215978'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fe4a52d6fa0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983420f-8ac2-496d-9fb6-5632370a5387",
   "metadata": {},
   "source": [
    "## Optimized Model Deployments\n",
    "\n",
    "Some of these classes appear in slightly modified format in `benchmark.ipynb`. Make sure to keep the code in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356441ab-f4f1-4b6f-8e6c-fbd3d3eeae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=6452)\u001b[0m INFO:     Started server process [6452]\n"
     ]
    }
   ],
   "source": [
    "# This class also appears in `benchmark.ipynb`\n",
    "@serve.deployment\n",
    "class Intent:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    INTENT_MODEL_NAME)\n",
    "        \n",
    "        # Extract weights and load them onto the Plasma object store\n",
    "        self._model_ref = ray.put(zerocopy.extract_tensors(model))\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "        \n",
    "        # Preprocessing\n",
    "        input_text = f'{json_request[\"context\"]} </s>'\n",
    "        features = self._tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        output = await zerocopy.call_model.remote(\n",
    "            self._model_ref, [], features, 'generate')\n",
    "\n",
    "        # Postprocessing\n",
    "        result_string = self._tokenizer.decode(output[0])\n",
    "        result_string = result_string[len('<pad> '):-len('</s>')]\n",
    "        return {\n",
    "            'intent': result_string\n",
    "        }\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class Sentiment:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            SENTIMENT_MODEL_NAME)\n",
    "        \n",
    "        model = (transformers.AutoModelForSequenceClassification\n",
    "                 .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "        self._model_ref = ray.put(zerocopy.extract_tensors(model))\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "        \n",
    "        # Preprocessing\n",
    "        encoded_input = self._tokenizer(json_request['context'], \n",
    "                                         return_tensors='pt')   \n",
    "\n",
    "        # Inference\n",
    "        output = await zerocopy.call_model.remote(\n",
    "            self._model_ref, [], encoded_input)\n",
    "\n",
    "        # Postprocessing\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = scipy.special.softmax(scores)\n",
    "        scores = [float(s) for s in scores]\n",
    "        scores = {k: v for k, v in zip(['positive', 'neutral', 'negative'], scores)}\n",
    "        return scores\n",
    "\n",
    "\n",
    "# This class also appears in `benchmark.ipynb`\n",
    "@serve.deployment\n",
    "class QA:\n",
    "    def __init__(self):\n",
    "        # Load the pipeline and move the model's weights onto the\n",
    "        # Plasma object store.\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            transformers.pipeline('question-answering', \n",
    "                                  model=QA_MODEL_NAME))\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # The original `transformers` code is not async-aware, so we\n",
    "        # call it from `run_in_executor()`\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(**json_request))\n",
    "        return result\n",
    "    \n",
    "\n",
    "@serve.deployment\n",
    "class Generate:\n",
    "    def __init__(self):\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            transformers.pipeline('text-generation',\n",
    "                                  model=GENERATE_MODEL_NAME),\n",
    "            ('__call__', 'generate'))\n",
    "        self._pad_token_id = self._pipeline.tokenizer.eos_token_id\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "            self._threadpool, \n",
    "            lambda: self._pipeline(\n",
    "                json_request['prompt_text'], \n",
    "                pad_token_id=self._pad_token_id))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa3569-8f71-4158-a1ce-a47d1c0da78a",
   "metadata": {},
   "source": [
    "Now we can deploy all of these pipelines as Serve endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae657532-5495-43d4-9bba-ae102cba0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 11:35:23,817\tINFO api.py:249 -- Updating deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:35:23,845\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(intent_en pid=6450)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "2022-03-02 11:35:34,414\tINFO api.py:261 -- Deployment 'intent_en' is ready at `http://127.0.0.1:8000/predictions/intent_en`. component=serve deployment=intent_en\n",
      "2022-03-02 11:35:34,426\tINFO api.py:249 -- Updating deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:35:34,519\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "2022-03-02 11:35:43,494\tINFO api.py:261 -- Deployment 'sentiment_en' is ready at `http://127.0.0.1:8000/predictions/sentiment_en`. component=serve deployment=sentiment_en\n",
      "2022-03-02 11:35:43,504\tINFO api.py:249 -- Updating deployment 'qa_en'. component=serve deployment=qa_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:35:43,596\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_en'. component=serve deployment=qa_en\n",
      "2022-03-02 11:35:53,146\tINFO api.py:261 -- Deployment 'qa_en' is ready at `http://127.0.0.1:8000/predictions/qa_en`. component=serve deployment=qa_en\n",
      "2022-03-02 11:35:53,160\tINFO api.py:249 -- Updating deployment 'generate_en'. component=serve deployment=generate_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:35:53,248\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_en'. component=serve deployment=generate_en\n",
      "2022-03-02 11:36:02,633\tINFO api.py:261 -- Deployment 'generate_en' is ready at `http://127.0.0.1:8000/predictions/generate_en`. component=serve deployment=generate_en\n",
      "2022-03-02 11:36:02,647\tINFO api.py:249 -- Updating deployment 'intent_es'. component=serve deployment=intent_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:36:02,736\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_es'. component=serve deployment=intent_es\n",
      "\u001b[2m\u001b[36m(intent_es pid=6439)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "2022-03-02 11:36:13,032\tINFO api.py:261 -- Deployment 'intent_es' is ready at `http://127.0.0.1:8000/predictions/intent_es`. component=serve deployment=intent_es\n",
      "2022-03-02 11:36:13,045\tINFO api.py:249 -- Updating deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:36:13,135\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "2022-03-02 11:36:21,994\tINFO api.py:261 -- Deployment 'sentiment_es' is ready at `http://127.0.0.1:8000/predictions/sentiment_es`. component=serve deployment=sentiment_es\n",
      "2022-03-02 11:36:22,008\tINFO api.py:249 -- Updating deployment 'qa_es'. component=serve deployment=qa_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:36:22,098\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_es'. component=serve deployment=qa_es\n",
      "2022-03-02 11:36:31,647\tINFO api.py:261 -- Deployment 'qa_es' is ready at `http://127.0.0.1:8000/predictions/qa_es`. component=serve deployment=qa_es\n",
      "2022-03-02 11:36:31,660\tINFO api.py:249 -- Updating deployment 'generate_es'. component=serve deployment=generate_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:36:31,750\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_es'. component=serve deployment=generate_es\n",
      "2022-03-02 11:36:41,141\tINFO api.py:261 -- Deployment 'generate_es' is ready at `http://127.0.0.1:8000/predictions/generate_es`. component=serve deployment=generate_es\n",
      "2022-03-02 11:36:41,156\tINFO api.py:249 -- Updating deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:36:41,244\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "\u001b[2m\u001b[36m(intent_zh pid=6438)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "2022-03-02 11:36:52,082\tINFO api.py:261 -- Deployment 'intent_zh' is ready at `http://127.0.0.1:8000/predictions/intent_zh`. component=serve deployment=intent_zh\n",
      "2022-03-02 11:36:52,098\tINFO api.py:249 -- Updating deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:36:52,189\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "2022-03-02 11:37:01,206\tINFO api.py:261 -- Deployment 'sentiment_zh' is ready at `http://127.0.0.1:8000/predictions/sentiment_zh`. component=serve deployment=sentiment_zh\n",
      "2022-03-02 11:37:01,223\tINFO api.py:249 -- Updating deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:37:01,310\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "2022-03-02 11:37:10,945\tINFO api.py:261 -- Deployment 'qa_zh' is ready at `http://127.0.0.1:8000/predictions/qa_zh`. component=serve deployment=qa_zh\n",
      "2022-03-02 11:37:10,959\tINFO api.py:249 -- Updating deployment 'generate_zh'. component=serve deployment=generate_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:37:11,050\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_zh'. component=serve deployment=generate_zh\n",
      "2022-03-02 11:37:20,607\tINFO api.py:261 -- Deployment 'generate_zh' is ready at `http://127.0.0.1:8000/predictions/generate_zh`. component=serve deployment=generate_zh\n"
     ]
    }
   ],
   "source": [
    "# Define endpoints.\n",
    "# Everything gets deployed under the prefix /predictions/ to make\n",
    "# the deployment as similar as possible to the TorchServe baseline.\n",
    "LANGUAGES = ['en', 'es', 'zh']\n",
    "\n",
    "# We run the deployments in a blocking fashion for safety.\n",
    "for lang in LANGUAGES:\n",
    "    Intent.options(name=f'intent_{lang}',\n",
    "                   route_prefix=f'/predictions/intent_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "    Sentiment.options(name=f'sentiment_{lang}',\n",
    "                   route_prefix=f'/predictions/sentiment_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "    QA.options(name=f'qa_{lang}',\n",
    "                   route_prefix=f'/predictions/qa_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "    Generate.options(name=f'generate_{lang}',\n",
    "                   route_prefix=f'/predictions/generate_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1}).deploy()\n",
    "\n",
    "# Wait a moment so log output doesn't go to the next cell's output\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baac194f-9c7e-4d7c-8228-4e55b31738f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat'}\n",
      "Sentiment result: {'positive': 0.5419477820396423, 'neutral': 0.38251084089279175, 'negative': 0.07554134726524353}\n",
      "Question answering result: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      "Natural language generation result: [{'generated_text': \"All your base are going to change. You'll have to adjust your order accordingly.\\n\\nOnce your order is received and approved, you will then be able to order your next order. We want you to be able to read all about our services\"}]\n"
     ]
    }
   ],
   "source": [
    "# Verify that everything deployed properly.\n",
    "intent_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/intent_en', \n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/sentiment_en', \n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/qa_en', \n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/generate_en', \n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb1cf4-e53f-463f-a7f8-deadb3947186",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "Once the benchmark is complete, shut down this notebook's Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42017025-37d7-4dd1-b436-ae4fea31bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,917\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,922\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,925\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_en'. component=serve deployment=qa_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,929\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'generate_en'. component=serve deployment=generate_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,932\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_es'. component=serve deployment=intent_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,936\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,940\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_es'. component=serve deployment=qa_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,943\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'generate_es'. component=serve deployment=generate_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,947\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,950\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,954\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=6451)\u001b[0m 2022-03-02 11:51:16,958\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'generate_zh'. component=serve deployment=generate_zh\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ddf0f-c6a0-475e-b549-b16d29dce24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
