{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b887e3af-0ebb-4a37-89ee-9c60a1a18ed1",
   "metadata": {},
   "source": [
    "# torchserve.ipynb\n",
    "\n",
    "This notebook contains code for the portions of the benchmark in [the benchmark notebook](./benchmark.ipynb) that use [TorchServe](https://github.com/pytorch/serve).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca5621e-af12-443b-a2c8-8cd6186e2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import scipy.special\n",
    "import transformers\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82d5bd7-0042-446d-9260-89568fb58147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "\n",
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "GENERATE_MODEL_NAME = 'gpt2'\n",
    "\n",
    "\n",
    "INTENT_INPUT = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and beat you up, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "\n",
    "QA_INPUT = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444265e-4885-480e-85bc-39d26020b0d3",
   "metadata": {},
   "source": [
    "## Model Packaging\n",
    "\n",
    "TorchServe requires models to be packaged up as model archive files. Documentation for this process (such as it is) is [here](https://github.com/pytorch/serve/blob/master/README.md#serve-a-model) and [here](https://github.com/pytorch/serve/blob/master/model-archiver/README.md).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a11f1-9404-40c9-9928-4f176ed84927",
   "metadata": {},
   "source": [
    "### Intent Model\n",
    "\n",
    "The intent model requires the caller to call the pre- and post-processing code manually. Only the model and tokenizer are provided on the model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff878387-488f-4c35-a312-2b5bf7b23770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('torchserve/intent/tokenizer_config.json',\n",
       " 'torchserve/intent/special_tokens_map.json',\n",
       " 'torchserve/intent/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to dump the model into a local directory.\n",
    "intent_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    INTENT_MODEL_NAME)\n",
    "intent_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "intent_model.save_pretrained('torchserve/intent')\n",
    "intent_tokenizer.save_pretrained('torchserve/intent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73b6ec-d774-4617-b6d8-93dd9993e3cb",
   "metadata": {},
   "source": [
    "Next we wrapped the model in a [handler class](./torchserve/intent/handler.py), which \n",
    "needs to be in its own separate Python file in order for the `torch-model-archiver`\n",
    "utility to work.\n",
    "\n",
    "The following command turns this Python file, plus the data files created by the \n",
    "previous cell, into a model archive (`.mar`) file at `torchserve/model_store/intent.mar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d6058d-e704-4be5-a6ee-c344a6df649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.37 s, sys: 583 ms, total: 1.95 s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!mkdir -p torchserve/model_store\n",
    "!torch-model-archiver --model-name intent --version 1.0 \\\n",
    " --serialized-file torchserve/intent/pytorch_model.bin \\\n",
    " --handler torchserve/handler_intent.py \\\n",
    " --extra-files \"torchserve/intent/config.json,torchserve/intent/special_tokens_map.json,torchserve/intent/tokenizer_config.json,torchserve/intent/tokenizer.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e8234-c291-47a4-ba66-11a1a5d8fbb1",
   "metadata": {},
   "source": [
    "### Sentiment Model\n",
    "\n",
    "The sentiment model operates similarly to the intent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e54fd5-b992-47b7-b62d-b1e649f3c4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('torchserve/sentiment/tokenizer_config.json',\n",
       " 'torchserve/sentiment/special_tokens_map.json',\n",
       " 'torchserve/sentiment/vocab.json',\n",
       " 'torchserve/sentiment/merges.txt',\n",
       " 'torchserve/sentiment/added_tokens.json',\n",
       " 'torchserve/sentiment/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    SENTIMENT_MODEL_NAME)\n",
    "sentiment_model = (\n",
    "    transformers.AutoModelForSequenceClassification\n",
    "    .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "\n",
    "sentiment_model.save_pretrained('torchserve/sentiment')\n",
    "sentiment_tokenizer.save_pretrained('torchserve/sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9957b85-3417-430c-87a5-a0645dd13508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'positive': 0.13167865574359894,\n",
       "  'neutral': 0.6034972667694092,\n",
       "  'negative': 0.2648240327835083},\n",
       " {'positive': 0.22967900335788727,\n",
       "  'neutral': 0.5535955429077148,\n",
       "  'negative': 0.21672536432743073}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = ['hello', 'world']\n",
    "input_batch = sentiment_tokenizer(contexts, padding=True, \n",
    "                                  return_tensors='pt')\n",
    "\n",
    "inference_output = sentiment_model(**input_batch)\n",
    "\n",
    "scores = inference_output.logits.detach().numpy()\n",
    "scores = scipy.special.softmax(scores, axis=1).tolist()\n",
    "scores = [{k: v for k, v in zip(['positive', 'neutral', 'negative'], row)}\n",
    "          for row in scores]\n",
    "# return scores\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24304451-d4eb-43ea-9a93-67279a6fa68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 674 ms, sys: 295 ms, total: 968 ms\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!torch-model-archiver --model-name sentiment --version 1.0 \\\n",
    " --serialized-file torchserve/sentiment/pytorch_model.bin \\\n",
    " --handler torchserve/handler_sentiment.py \\\n",
    " --extra-files \"torchserve/sentiment/config.json,torchserve/sentiment/special_tokens_map.json,torchserve/sentiment/tokenizer_config.json,torchserve/sentiment/tokenizer.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ea6aa-4fca-4492-8a53-94e0c7e98ffb",
   "metadata": {},
   "source": [
    "### Question Answering Model\n",
    "\n",
    "The QA model uses a `transformers` pipeline. We squeeze this model into the TorchServe APIs by telling the pipeline to serialize all of its parts to a single directory, then passing the parts taht aren't `pytorch_model.bin` in as extra files. At runtime, our custom handler uses the model loading code from `transformers` on the reconstituted model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e50c4aec-51a7-489c-b15e-9aa4615a144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = transformers.pipeline('question-answering', model=QA_MODEL_NAME)\n",
    "qa_pipeline.save_pretrained('torchserve/qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2785cf2b-5156-4dca-ad1c-297238f5ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 650 ms, sys: 284 ms, total: 934 ms\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!torch-model-archiver --model-name qa --version 1.0 \\\n",
    " --serialized-file torchserve/qa/pytorch_model.bin \\\n",
    " --handler torchserve/handler_qa.py \\\n",
    " --extra-files \"torchserve/qa/config.json,torchserve/qa/merges.txt,torchserve/qa/special_tokens_map.json,torchserve/qa/tokenizer_config.json,torchserve/qa/tokenizer.json,torchserve/qa/vocab.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c453edaa-7441-4e08-be68-8a2445dbc7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'},\n",
       " {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [QA_INPUT, QA_INPUT]\n",
    "\n",
    "# Preprocessing\n",
    "samples = [qa_pipeline.create_sample(**r) for r in data]\n",
    "generators = [qa_pipeline.preprocess(s) for s in samples]\n",
    "\n",
    "# Inference\n",
    "inference_outputs = ((qa_pipeline.forward(example) for example in batch) for batch in generators)\n",
    "\n",
    "post_results = [qa_pipeline.postprocess(o) for o in inference_outputs]\n",
    "post_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9b624-f0da-4901-a4af-a4b154c5067c",
   "metadata": {},
   "source": [
    "### Natural Language Generation Model\n",
    "\n",
    "The text generation model is roughly similar to the QA model, albeit with important differences in how the three stages of the pipeline operate.  At least model loading is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd35d09-511e-4ab2-a02c-bead6f415162",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_pipeline = transformers.pipeline(\n",
    "    'text-generation', model=GENERATE_MODEL_NAME)\n",
    "generate_pipeline.save_pretrained('torchserve/generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "950c95ed-b189-4f4e-9a03-470a601305e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'All your base are now filled with goodies. To get your first batch, head on over to our main site\\n\\nPOPULAR SITE:\\n\\nBaked Goods is proud to announce the arrival of our original baked goods line! Our focus'}],\n",
       " [{'generated_text': 'All your base are set at 500HP and will stay there. I will wait for you guys to reach the final 2 dungeons, and bring the items. Your party can do quite a few of those anyway. Your party can also bring down the last'}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [GENERATE_INPUT, GENERATE_INPUT]\n",
    "\n",
    "\n",
    "pad_token_id = generate_pipeline.tokenizer.eos_token_id\n",
    "\n",
    "json_records = data\n",
    "\n",
    "# preprocess() takes a single input at a time, but we need to do \n",
    "# a batch at a time.\n",
    "input_batch = [generate_pipeline.preprocess(**r) for r in json_records]\n",
    "\n",
    "# forward() takes a single input at a time, but we need to run a\n",
    "# batch at a time.\n",
    "inference_output = [\n",
    "    generate_pipeline.forward(r, pad_token_id=pad_token_id)\n",
    "    for r in input_batch]\n",
    "\n",
    "# postprocess() takes a single generation result at a time, but we\n",
    "# need to run a batch at a time.\n",
    "generate_result = [generate_pipeline.postprocess(i)\n",
    "                   for i in inference_output]\n",
    "generate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "558cbdb8-9a78-4649-aaa0-cf6f6a8e9e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 683 ms, sys: 298 ms, total: 981 ms\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!torch-model-archiver --model-name generate --version 1.0 \\\n",
    " --serialized-file torchserve/generate/pytorch_model.bin \\\n",
    " --handler torchserve/handler_generate.py \\\n",
    " --extra-files \"torchserve/generate/config.json,torchserve/generate/merges.txt,torchserve/generate/special_tokens_map.json,torchserve/generate/tokenizer_config.json,torchserve/generate/tokenizer.json,torchserve/generate/vocab.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a273a15-fe07-42d3-82a7-d52f101be4ca",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now we can fire up TorchServe and test our models.\n",
    "\n",
    "For some reason, starting TorchServe needs to be done in a proper terminal window. Running the command from this notebook has no effect.  The commands to run (from the root of the repository) are:\n",
    "\n",
    "```\n",
    "> conda activate ./env\n",
    "> cd notebooks/benchmark/torchserve\n",
    "> torchserve --start --ncs --model-store model_store --ts-config torchserve.properties\n",
    "```\n",
    "\n",
    "Then pick up a cup of coffee and a book and wait a while. The startup process is like cold-starting a gas turbine and takes about 10 minutes.\n",
    "\n",
    "Once the server has started, we can test our deployed models by making POST requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26739838-035c-4ee4-be40-44c87ad66500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'modelName': 'generate_en', 'modelUrl': 'generate.mar'},\n",
       "  {'modelName': 'generate_es', 'modelUrl': 'generate.mar'},\n",
       "  {'modelName': 'generate_zh', 'modelUrl': 'generate.mar'},\n",
       "  {'modelName': 'intent_en', 'modelUrl': 'intent.mar'},\n",
       "  {'modelName': 'intent_es', 'modelUrl': 'intent.mar'},\n",
       "  {'modelName': 'intent_zh', 'modelUrl': 'intent.mar'},\n",
       "  {'modelName': 'qa_en', 'modelUrl': 'qa.mar'},\n",
       "  {'modelName': 'qa_es', 'modelUrl': 'qa.mar'},\n",
       "  {'modelName': 'qa_zh', 'modelUrl': 'qa.mar'},\n",
       "  {'modelName': 'sentiment_en', 'modelUrl': 'sentiment.mar'},\n",
       "  {'modelName': 'sentiment_es', 'modelUrl': 'sentiment.mar'},\n",
       "  {'modelName': 'sentiment_zh', 'modelUrl': 'sentiment.mar'}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probe the management API to verify that TorchServe is running.\n",
    "requests.get('http://127.0.0.1:8081/models').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8961900e-08cd-488e-bcd2-f88155ea04f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat chips'}\n",
      "Sentiment result: {'positive': 0.5419477820396423, 'neutral': 0.38251084089279175, 'negative': 0.07554134726524353}\n",
      "Question answering result: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      "Natural language generation result: [{'generated_text': 'All your base are still in the field and your plan is coming up to date?\" said O\\'Neill, pointing to his wife, who looked down at her feet like she had been slapped on the cheek.\\n\\n\"It\\'s important that we'}]\n"
     ]
    }
   ],
   "source": [
    "port = 8080\n",
    "\n",
    "intent_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/intent_en', \n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/sentiment_en', \n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/qa_en', \n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/generate_en', \n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc90924-65e1-4399-9994-5cd755326e7c",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "TorchServe consumes many resources even when it isn't doing anything. When you're done running the baseline portion of the benchmark, be sure to shut down the server by running:\n",
    "```\n",
    "> torchserve --stop\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
