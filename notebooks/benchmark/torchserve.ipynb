{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b887e3af-0ebb-4a37-89ee-9c60a1a18ed1",
   "metadata": {},
   "source": [
    "# torchserve.ipynb\n",
    "\n",
    "This notebook contains code for the portions of the benchmark in [the benchmark notebook](./benchmark.ipynb) that use [TorchServe](https://github.com/pytorch/serve).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca5621e-af12-443b-a2c8-8cd6186e2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import scipy.special\n",
    "import transformers\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82d5bd7-0042-446d-9260-89568fb58147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "\n",
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "GENERATE_MODEL_NAME = 'gpt2'\n",
    "\n",
    "\n",
    "INTENT_INPUT = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and beat you up, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "\n",
    "QA_INPUT = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444265e-4885-480e-85bc-39d26020b0d3",
   "metadata": {},
   "source": [
    "## Model Packaging\n",
    "\n",
    "TorchServe requires models to be packaged up as model archive files. Documentation for this process (such as it is) is [here](https://github.com/pytorch/serve/blob/master/README.md#serve-a-model) and [here](https://github.com/pytorch/serve/blob/master/model-archiver/README.md).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a11f1-9404-40c9-9928-4f176ed84927",
   "metadata": {},
   "source": [
    "### Intent Model\n",
    "\n",
    "The intent model requires the caller to call the pre- and post-processing code manually. Only the model and tokenizer are provided on the model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff878387-488f-4c35-a312-2b5bf7b23770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407a723d8f474631945d8f471fe9502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5902366cc25f4f97a4713319b030d46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cd70371bfb4340b498e7e6dfdea3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2b53dd8a824f50b3d023ab47e32917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3be79f182d4bc3b7590068f2d51261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('torchserve/intent/tokenizer_config.json',\n",
       " 'torchserve/intent/special_tokens_map.json',\n",
       " 'torchserve/intent/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to dump the model into a local directory.\n",
    "intent_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    INTENT_MODEL_NAME)\n",
    "intent_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "intent_model.save_pretrained('torchserve/intent')\n",
    "intent_tokenizer.save_pretrained('torchserve/intent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73b6ec-d774-4617-b6d8-93dd9993e3cb",
   "metadata": {},
   "source": [
    "Next we wrapped the model in a handler class, located at `./torchserve/handler_intent.py`, which \n",
    "needs to be in its own separate Python file in order for the `torch-model-archiver`\n",
    "utility to work.\n",
    "\n",
    "The following command turns this Python file, plus the data files created by the \n",
    "previous cell, into a model archive (`.mar`) file at `torchserve/model_store/intent.mar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d6058d-e704-4be5-a6ee-c344a6df649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 438 ms, sys: 116 ms, total: 553 ms\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!mkdir -p torchserve/model_store\n",
    "!torch-model-archiver --model-name intent --version 1.0 \\\n",
    " --serialized-file torchserve/intent/pytorch_model.bin \\\n",
    " --handler torchserve/handler_intent.py \\\n",
    " --extra-files \"torchserve/intent/config.json,torchserve/intent/special_tokens_map.json,torchserve/intent/tokenizer_config.json,torchserve/intent/tokenizer.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e8234-c291-47a4-ba66-11a1a5d8fbb1",
   "metadata": {},
   "source": [
    "### Sentiment Model\n",
    "\n",
    "The sentiment model operates similarly to the intent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e54fd5-b992-47b7-b62d-b1e649f3c4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714dbcc9ac1b41158499c3c94a23ce4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c904d31746a451f9afa13cc16ecef0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857a937b34fc41fdaaecc7a41469d878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d536218a01784b3d8999c194aad8060d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1227715feec94181ae2f9d25f9a86831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('torchserve/sentiment/tokenizer_config.json',\n",
       " 'torchserve/sentiment/special_tokens_map.json',\n",
       " 'torchserve/sentiment/vocab.json',\n",
       " 'torchserve/sentiment/merges.txt',\n",
       " 'torchserve/sentiment/added_tokens.json',\n",
       " 'torchserve/sentiment/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    SENTIMENT_MODEL_NAME)\n",
    "sentiment_model = (\n",
    "    transformers.AutoModelForSequenceClassification\n",
    "    .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "\n",
    "sentiment_model.save_pretrained('torchserve/sentiment')\n",
    "sentiment_tokenizer.save_pretrained('torchserve/sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9957b85-3417-430c-87a5-a0645dd13508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'positive': 0.13167870044708252,\n",
       "  'neutral': 0.6034972071647644,\n",
       "  'negative': 0.26482412219047546},\n",
       " {'positive': 0.22967909276485443,\n",
       "  'neutral': 0.5535956025123596,\n",
       "  'negative': 0.21672534942626953}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = ['hello', 'world']\n",
    "input_batch = sentiment_tokenizer(contexts, padding=True, \n",
    "                                  return_tensors='pt')\n",
    "\n",
    "inference_output = sentiment_model(**input_batch)\n",
    "\n",
    "scores = inference_output.logits.detach().numpy()\n",
    "scores = scipy.special.softmax(scores, axis=1).tolist()\n",
    "scores = [{k: v for k, v in zip(['positive', 'neutral', 'negative'], row)}\n",
    "          for row in scores]\n",
    "# return scores\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacdaeb-b136-450d-b81b-34ecd1ae26ee",
   "metadata": {},
   "source": [
    "As with the intent model, we created a handler class (located at `torchserve/handler_sentiment.py`), then\n",
    "pass that class and the serialized model from two cells ago\n",
    "through the `torch-model-archiver` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24304451-d4eb-43ea-9a93-67279a6fa68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 210 ms, sys: 114 ms, total: 324 ms\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!torch-model-archiver --model-name sentiment --version 1.0 \\\n",
    " --serialized-file torchserve/sentiment/pytorch_model.bin \\\n",
    " --handler torchserve/handler_sentiment.py \\\n",
    " --extra-files \"torchserve/sentiment/config.json,torchserve/sentiment/special_tokens_map.json,torchserve/sentiment/tokenizer_config.json,torchserve/sentiment/tokenizer.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ea6aa-4fca-4492-8a53-94e0c7e98ffb",
   "metadata": {},
   "source": [
    "### Question Answering Model\n",
    "\n",
    "The QA model uses a `transformers` pipeline. We squeeze this model into the TorchServe APIs by telling the pipeline to serialize all of its parts to a single directory, then passing the parts that aren't `pytorch_model.bin` in as extra files. At runtime, our custom handler uses the model loading code from `transformers` on the reconstituted model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50c4aec-51a7-489c-b15e-9aa4615a144d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448465d7e48d4d5aabea03ad6dcbcfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9985ef10e3b448eda08c251097e69ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96d2cfc47fd42bf924a3901608a7fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a064df9ecc43b08a7329114f1bd9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9a953a18714c9481f26200c81371a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb2aa49fa584d13b1897b8d5033b9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_pipeline = transformers.pipeline('question-answering', model=QA_MODEL_NAME)\n",
    "qa_pipeline.save_pretrained('torchserve/qa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11cfe9-32e1-4302-b29e-9c1126ced947",
   "metadata": {},
   "source": [
    "As with the previous models, we wrote a class (located at `torchserve/handler_qa.py`), then\n",
    "pass that wrapper class and the serialized model through the `torch-model-archiver` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2785cf2b-5156-4dca-ad1c-297238f5ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 287 ms, sys: 67.5 ms, total: 354 ms\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!torch-model-archiver --model-name qa --version 1.0 \\\n",
    " --serialized-file torchserve/qa/pytorch_model.bin \\\n",
    " --handler torchserve/handler_qa.py \\\n",
    " --extra-files \"torchserve/qa/config.json,torchserve/qa/merges.txt,torchserve/qa/special_tokens_map.json,torchserve/qa/tokenizer_config.json,torchserve/qa/tokenizer.json,torchserve/qa/vocab.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c453edaa-7441-4e08-be68-8a2445dbc7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 4.278918822819833e-06, 'start': 483, 'end': 484, 'answer': '5'},\n",
       " {'score': 4.278918822819833e-06, 'start': 483, 'end': 484, 'answer': '5'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [QA_INPUT, QA_INPUT]\n",
    "\n",
    "# Preprocessing\n",
    "samples = [qa_pipeline.create_sample(**r) for r in data]\n",
    "generators = [qa_pipeline.preprocess(s) for s in samples]\n",
    "\n",
    "# Inference\n",
    "inference_outputs = ((qa_pipeline.forward(example) for example in batch) for batch in generators)\n",
    "\n",
    "post_results = [qa_pipeline.postprocess(o) for o in inference_outputs]\n",
    "post_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9b624-f0da-4901-a4af-a4b154c5067c",
   "metadata": {},
   "source": [
    "### Natural Language Generation Model\n",
    "\n",
    "The text generation model is roughly similar to the QA model, albeit with important differences in how the three stages of the pipeline operate.  At least model loading is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd35d09-511e-4ab2-a02c-bead6f415162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f37b3dba81b4692b5d3692549b8f2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bfed5e50db41d2ae5e29418735990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ec1685cb9b4180ac4b98a1eb6ee7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea1478b32fd41a4b002e498107a08f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58c91158ae34ea8b61dbecfd44a91ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_pipeline = transformers.pipeline(\n",
    "    'text-generation', model=GENERATE_MODEL_NAME)\n",
    "generate_pipeline.save_pretrained('torchserve/generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950c95ed-b189-4f4e-9a03-470a601305e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"All your base are all fine. But don't bother telling me exactly what's going to change that. You'll likely keep getting annoyed, and a little further explanation is best. The best part? I don't want to hear you whine. Well\"}],\n",
       " [{'generated_text': 'All your base are so focused, and so focused because you want to leave the world and get away from the pain.\" I said, \"I\\'ll give you my keys and your cash, and your time. Now stop fighting with me, that\\'s'}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [GENERATE_INPUT, GENERATE_INPUT]\n",
    "\n",
    "\n",
    "pad_token_id = generate_pipeline.tokenizer.eos_token_id\n",
    "\n",
    "json_records = data\n",
    "\n",
    "# preprocess() takes a single input at a time, but we need to do \n",
    "# a batch at a time.\n",
    "input_batch = [generate_pipeline.preprocess(**r) for r in json_records]\n",
    "\n",
    "# forward() takes a single input at a time, but we need to run a\n",
    "# batch at a time.\n",
    "inference_output = [\n",
    "    generate_pipeline.forward(r, pad_token_id=pad_token_id)\n",
    "    for r in input_batch]\n",
    "\n",
    "# postprocess() takes a single generation result at a time, but we\n",
    "# need to run a batch at a time.\n",
    "generate_result = [generate_pipeline.postprocess(i)\n",
    "                   for i in inference_output]\n",
    "generate_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc388d-9499-404e-9118-d1ef1f262cac",
   "metadata": {},
   "source": [
    "Once again, we wrote a class (located at `torchserve/handler_generate.py`), then\n",
    "pass that wrapper class and the serialized model through the `torch-model-archiver` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558cbdb8-9a78-4649-aaa0-cf6f6a8e9e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 198 ms, sys: 96 ms, total: 294 ms\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!torch-model-archiver --model-name generate --version 1.0 \\\n",
    " --serialized-file torchserve/generate/pytorch_model.bin \\\n",
    " --handler torchserve/handler_generate.py \\\n",
    " --extra-files \"torchserve/generate/config.json,torchserve/generate/merges.txt,torchserve/generate/special_tokens_map.json,torchserve/generate/tokenizer_config.json,torchserve/generate/tokenizer.json,torchserve/generate/vocab.json\" \\\n",
    " --export-path torchserve/model_store \\\n",
    " --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a273a15-fe07-42d3-82a7-d52f101be4ca",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now we can fire up TorchServe and test our models.\n",
    "\n",
    "For some reason, starting TorchServe needs to be done in a proper terminal window. Running the command from this notebook has no effect.  The commands to run (from the root of the repository) are:\n",
    "\n",
    "```\n",
    "> conda activate ./env\n",
    "> cd notebooks/benchmark/torchserve\n",
    "> torchserve --start --ncs --model-store model_store --ts-config torchserve.properties\n",
    "```\n",
    "\n",
    "Then pick up a cup of coffee and a book and wait a while. The startup process is like cold-starting a gas turbine and takes about 10 minutes.\n",
    "\n",
    "Once the server has started, we can test our deployed models by making POST requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26739838-035c-4ee4-be40-44c87ad66500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe the management API to verify that TorchServe is running.\n",
    "requests.get('http://127.0.0.1:8081/models').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961900e-08cd-488e-bcd2-f88155ea04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 8080\n",
    "\n",
    "intent_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/intent_en', \n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/sentiment_en', \n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/qa_en', \n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    f'http://127.0.0.1:{port}/predictions/generate_en', \n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc90924-65e1-4399-9994-5cd755326e7c",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "TorchServe consumes many resources even when it isn't doing anything. When you're done running the baseline portion of the benchmark, be sure to shut down the server by running:\n",
    "```\n",
    "> torchserve --stop\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
