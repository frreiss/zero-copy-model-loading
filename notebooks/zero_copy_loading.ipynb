{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0280249f-bdd8-4b95-97db-fb6cb2b38e9c",
   "metadata": {},
   "source": [
    "# zero_copy_loading.ipynb\n",
    "\n",
    "This notebook contains a more-easily runnable version of the code in my blog post, [\"How to Load PyTorch Models 340 Times Faster\n",
    "with\n",
    "Ray\"](https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c).\n",
    "\n",
    "The notebook contains text from an early draft of the blog post, intermixed with the code used to generate the numbers in the plot. Formatting has been changed slightly so that the notebook passes the [`pycodestyle`](https://pycodestyle.pycqa.org/en/latest/intro.html) linter.\n",
    "\n",
    "Note that timings in the outputs included in this copy of the notebook are slightly different from the timings in the blog, because this notebook was rerun on a somewhat different setup prior to checking it into Github.\n",
    "\n",
    "There is also additional code at the bottom that performs timings on ResNet-50 and verifies that the models tested work correctly after being loaded with zero-copy loading. If you have a GPU with the CUDA libraries installed, some of the cells in this notebook will detect the presence of a GPU and perform additional tests.\n",
    "\n",
    "You can find instructions for setting up a Python environment to run this notebook in [README.md](./README.md).\n",
    "\n",
    "Once the environment is set up, you should be able to run this notebook from within that environment. Please open an issue against this Github repository if you have trouble running it on your machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e11236-d4ae-4f11-87c9-b1f1936f4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization boilerplate\n",
    "from typing import Tuple, List, Dict\n",
    "import time\n",
    "import ray\n",
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b81ab0-67d4-438c-b091-dc24f6133a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/freiss/pd/zero-copy-model-loading/env/lib/python3.8/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.238',\n",
       " 'raylet_ip_address': '192.168.0.238',\n",
       " 'redis_address': '192.168.0.238:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-09-20_14-59-49_674154_8167/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-09-20_14-59-49_674154_8167/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-09-20_14-59-49_674154_8167',\n",
       " 'metrics_export_port': 63004,\n",
       " 'node_id': '06d2409dfe10dd1b32de060ef23f1686831b6b909a28afd47fb20bef'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reboot_ray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c52453-258f-4199-b556-4bf09839c602",
   "metadata": {},
   "source": [
    "# How to Load PyTorch Models 340 Times Faster with Ray\n",
    "\n",
    "*One of the challenges of using deep learning in production is managing the cost of loading huge models for inference. In this article, we'll show how you can reduce this cost almost to zero by leveraging features of PyTorch and Ray.*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Deep learning models are big and cumbersome. Because of their size, they take a long time to load. This model loading cost leads to a great deal of engineering effort when deploying models in production. Model inference platforms like [TFX](https://www.tensorflow.org/tfx/serving/serving_basic), [TorchServe](https://github.com/pytorch/serve), and [IBM Spectrum Conductor Deep Learning Impact](https://www.ibm.com/products/spectrum-deep-learning-impact?cm_mmc=text_extensions_for_pandas) run deep learning models inside dedicated, long-lived processes and containers, with lots of complex code to start and stop containers and to pass data between them.\n",
    "\n",
    "![Block diagram of the TorchServe model inference platform, showing how TorchServe dedicates a pool of dedicated, long-lived processes to each model in order to amortize model loading costs. Source: https://github.com/pytorch/serve; License: Apache V2](images/torch_serve_arch.jpg)\n",
    "\n",
    "But what if this conventional wisdom isn't entirely correct? What if there was a way to load a deep learning model in a tiny fraction of a second? It might be possible to run model inference in production with a much simpler architecture.\n",
    "\n",
    "Let's see how fast we can make model loading go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25bb5a4-42d9-4b9c-a586-baed3e1f5435",
   "metadata": {},
   "source": [
    "## Background: BERT \n",
    "\n",
    "For the examples in this article, we'll use the [BERT](https://arxiv.org/abs/1810.04805) masked language model. BERT belongs to a group of general-purpose models that capture the nuances of human language in a (relatively) compact format. You can use these models to do many different natural language processing (NLP) tasks, ranging from document classification to machine translation. However, to do any task with high accuracy, you need to start with a model trained on your target language and [*fine-tune* the model](https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b) for the task.\n",
    "\n",
    "Tuning a BERT model for a task effectively creates a new model. If your application needs to perform three tasks in three different languages, you'll need *nine* copies of BERT --- one for each combination of language and task. This proliferation of models creates  headaches in production. Being able to load and unload different BERT-based model really fast would save a lot of trouble.\n",
    "\n",
    "Let's start by loading up a BERT model in the most straightforward way.\n",
    "\n",
    "## Loading a BERT Model\n",
    "\n",
    "The [transformers library](https://github.com/huggingface/transformers) from [Huggingface](https://huggingface.co/) provides convenient ways to load different variants of BERT. The code snippet that follows shows how to load `bert-base-uncased`, a medium-sized model with about 420 MB of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd910aa-a7ae-47ca-8c6a-d6bda4ddc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4fc77-ae7d-4613-95df-f805478323c1",
   "metadata": {},
   "source": [
    "The `transformers.BertModel.from_pretrained()` method follows PyTorch's [recommended practice](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended) for loading models: First, construct an instance of your model, which should be a subclass of `torch.nn.Module`. Then use `torch.load()` to load a PyTorch *state dictionary* of model weights. Finally, call your model's `load_state_dict()` method to copy the model weights from the state dictionary into your model's `torch.Tensor` objects.\n",
    "\n",
    "This method takes about 1.4 seconds to load BERT on my laptop, provided that the model is on local disk. That's fairly impressive for a model that's over 400MB in size, but it's still a long time. For comparison, running inference with this model only takes a fraction of a second.\n",
    "\n",
    "The main reason this method is so slow is that it is optimized for reading models in a portable way over a slow network connection. It copies the model's parameters several times while building the state dictionary, then it copies them some more while installing the weights into the model's Python object.\n",
    "\n",
    "PyTorch has an [alternate model loading method](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model) that gives up some compatibility but only copies model weights once. Here's what the code to load BERT with that method looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425dc7e8-8638-43e2-aaba-761934e10af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the model we loaded in the previous code listing.\n",
    "torch.save(bert, \"outputs/bert.pt\")\n",
    "\n",
    "# Load the model back in.\n",
    "bert_2 = torch.load(\"outputs/bert.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f786ab0-4809-47cc-99fc-27dc180ecad8",
   "metadata": {},
   "source": [
    "This method loads BERT in 0.125 seconds on the same machine. That's 11 times faster.\n",
    "\n",
    "If dropping the number of copies to 1 makes model loading that much faster, imagine what would happen if we dropped the number of copies to zero! Is it possible to do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee1da6-635e-45a6-9358-14547cab0468",
   "metadata": {},
   "source": [
    "## Zero-Copy Model Loading\n",
    "\n",
    "It turns out that we can indeed load PyTorch models while copying weights zero times. We can achieve this goal by leveraging some features of PyTorch and Ray.\n",
    "\n",
    "First, some background on [Ray](https://ray.io). Ray is an open source system for building high-performance distributed applications. One of Ray's unique features is its main-memory object store, [Plasma](https://docs.ray.io/en/master/serialization.html#plasma-store). Plasma uses shared memory to pass objects between processes on each machine in a Ray cluster. Ray uses Plasma's shared memory model to implement zero-copy transfer of [NumPy](https://numpy.org/) arrays. If a Ray [task](https://docs.ray.io/en/master/walkthrough.html#remote-functions-tasks) needs to read a NumPy array from Plasma, the task can access the array's data directly out of shared memory without copying any data into its local heap.\n",
    "\n",
    "So if we store the weights of a model as NumPy arrays on Plasma, we can access those weights directly out of Plasma's shared memory segments, without making any copies. \n",
    "\n",
    "But we still need to connect those weights to the rest of the PyTorch model, which requires them to be wrapped in PyTorch `Tensor` objects. The standard method of creating a `Tensor` involves copying the contents of the tensor, but PyTorch also has an alternate code path for initializing `Tensor`s *without* performing a copy. You can access this code path by passing your NumPy array to `torch.as_tensor()` instead of using `Tensor.__new__()`.\n",
    "\n",
    "With all of this background information in mind, here's a high-level overview of how to do zero-copy model loading from Plasma. \n",
    "\n",
    "First, you need to load the model into the Plasma object store, which is a three-step process:\n",
    "\n",
    "1. Load the model from disk.\n",
    "2. Separate the original PyTorch model into its weights and its graph of operations, and convert the weights to NumPy arrays.\n",
    "3. Upload the NumPy arrays and the model (minus weights) to Plasma.\n",
    "\n",
    "Once the model and its weights are in object storage, it becomes possible to do a zero-copy load of the model. Here are the steps to follow:\n",
    "\n",
    "1. Deserialize the model (minus weights) from Plasma\n",
    "2. Extract the weights from Plasma (without copying any data)\n",
    "3. Wrap the weights in PyTorch `Tensors` (without copying any data)\n",
    "4. Install the weight tensors back in the reconstructed model (without copying any data)\n",
    "\n",
    "If a copy of the model is in the local machine's Plasma shared memory segment, these steps will load load BERT in **0.004 seconds**. That's **340 times faster** than loading the model with `BertModel.from_pretrained()`!\n",
    "\n",
    "![Comparison of running times for different ways of loading the bert-base-uncased model. BertModel.from_pretrained() takes 1.4 seconds, torch.load() takes 0.125 seconds, and zero-copy loading takes 0.004 seconds.](images/bert_load_times.png)\n",
    "\n",
    "This loading time is an order of magnitude less than the time it takes to run one inference request on this model with a general purpose CPU. That means that you can load the model *on demand* with almost no performance penalty. There's need to spin up a dedicated model serving platform or a Ray [actor pool](https://docs.ray.io/en/master/actors.html#actor-pool), tying up resources for models that aren't currently running inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3ff84-a51d-4329-8384-9f456eaf3754",
   "metadata": {},
   "source": [
    "## The Details\n",
    "\n",
    "Let's break down how to implement each of the steps for zero-copy model loading, starting with getting the model onto Plasma in an appropriate format.\n",
    "\n",
    "We've already covered how to load a PyTorch model from disk. The next step after that initial loading is to separate the model into its weights and its graph of operations, converting the weights to NumPy arrays. Here's a Python function that will do all those things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2443eed7-a318-4b53-9b17-0ac64c950b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(m: torch.nn.Module) -> Tuple[torch.nn.Module, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Remove the tensors from a PyTorch model, convert them to NumPy\n",
    "    arrays, and return the stripped model and tensors.\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "    for _, module in m.named_modules():\n",
    "        # Store the tensors in Python dictionaries\n",
    "        params = {\n",
    "            name: torch.clone(param).detach().numpy()\n",
    "            for name, param in module.named_parameters(recurse=False)\n",
    "        }\n",
    "        buffers = {\n",
    "            name: torch.clone(buf).detach().numpy()\n",
    "            for name, buf in module.named_buffers(recurse=False)\n",
    "        }\n",
    "        tensors.append({\"params\": params, \"buffers\": buffers})\n",
    "\n",
    "    # Make a copy of the original model and strip all tensors and\n",
    "    # temporary buffers out of the copy.\n",
    "    m_copy = copy.deepcopy(m)\n",
    "    for _, module in m_copy.named_modules():\n",
    "        for name in (\n",
    "                [name for name, _ in module.named_parameters(recurse=False)]\n",
    "                + [name for name, _ in module.named_buffers(recurse=False)]):\n",
    "            setattr(module, name, None)\n",
    "\n",
    "    # Make sure the copy is configured for inference.\n",
    "    m_copy.train(False)\n",
    "    return m_copy, tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584edf0-864f-4590-9381-9e22c082991b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Most PyTorch models are built on top the PyTorch class `torch.nn.Module`. The model is a graph of Python objects, and every object is a subclasses of `Module`.\n",
    "\n",
    "The `Module` class provides two places to store model weights: *parameters* for weights that are trained by gradient descent, and *buffers* for weights that are trained in other ways. Lines 6-17 of the listing above iterate over the components of the model, pull out the parameters and buffers, and convert their values to NumPy arrays. Then lines 21-25 create a copy of the model and remove all the weights from the copy. Finally, line 29 returns the copy and the converted weight tensors as a Python tuple.\n",
    "\n",
    "We can pass the return value from this function directly to `ray.put()` to upload the model and its weights onto Plasma. Here's what the upload operation looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709dc8d5-b8ed-4f4d-a320-cbc0946d19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ref = ray.put(extract_tensors(bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa75e4e-a0a3-424b-9734-d92bd667b3b0",
   "metadata": {},
   "source": [
    "The variable `bert_ref` here is a Ray object reference. We can retrieve the model and weights by passing this object reference to `ray.get()`, as in the following listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7179b2e4-d6ba-44c1-aa04-b39939f89ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_skeleton, bert_weights = ray.get(bert_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b59ff-01b4-4676-a49e-855152dd8f4b",
   "metadata": {},
   "source": [
    "If the object that `bert_ref` points to isn't available on the current node of your Ray cluster, the first attempt to read the model will block while Ray [downloads the object to the node's local shared memory segment](https://github.com/ray-project/ray/blob/c1b9f921a614a0927013ff0daeb6e130aaebb473/src/ray/core_worker/store_provider/plasma_store_provider.cc#L274). Subsequent calls to `ray.get(bert_ref)` will return the local copy immediately.\n",
    "\n",
    "Now we need to convert `bert_weights` from NumPy arrays to `torch.Tensor` objects and attach them to the model in `bert_skeleton`, all without performing any additional copies. Here is a Python function that does those steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea542c3-441e-4e1d-adcf-5967d6943db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tensors(m: torch.nn.Module, tensors: List[Dict]):\n",
    "    \"\"\"\n",
    "    Restore the tensors that extract_tensors() stripped out of a \n",
    "    PyTorch model.\n",
    "    :param no_parameters_objects: Skip wrapping tensors in \n",
    "     ``torch.nn.Parameters`` objects (~20% speedup, may impact\n",
    "     some models)\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        modules = [module for _, module in m.named_modules()]\n",
    "        for module, tensor_dict in zip(modules, tensors):\n",
    "            # There are separate APIs to set parameters and buffers.\n",
    "            for name, array in tensor_dict[\"params\"].items():\n",
    "                module.register_parameter(\n",
    "                    name, torch.nn.Parameter(torch.as_tensor(array)))\n",
    "            for name, array in tensor_dict[\"buffers\"].items():\n",
    "                module.register_buffer(name, torch.as_tensor(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b993f-b2b6-481f-a386-e493e38e08d2",
   "metadata": {},
   "source": [
    "This function does roughly the same thing as PyTorch's `load_state_dict()` function, except that it avoids copying tensors. The `replace_tensors()` function modifies the reconstituted model in place. After calling `replace_tensors()`, we can run the model, producing the same results as the original copy of the model. Here's some code that shows running a BERT model after loading its weights with `replace_tensors()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecac0b2-eecb-47c2-bfae-bbf7633704d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-b7f49ff7af67>:17: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  module.register_buffer(name, torch.as_tensor(array))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model's output:\n",
      "tensor([[[-0.1153,  0.2566, -0.2220,  ..., -0.3130,  0.6333,  0.6588],\n",
      "         [ 0.2769,  0.5195,  0.2059,  ..., -0.1062,  1.1186,  0.3836],\n",
      "         [ 0.9019,  0.7557, -0.1615,  ...,  0.0588,  0.3570, -0.0296],\n",
      "         ...,\n",
      "         [ 0.0155, -0.0602,  0.3365,  ..., -0.0936,  0.8055, -0.5007],\n",
      "         [ 0.6198,  0.2695, -0.3402,  ...,  0.0860, -0.3373, -0.4606],\n",
      "         [ 0.8493,  0.3726, -0.2073,  ..., -0.1145, -0.5216, -0.4418]]])\n",
      "\n",
      "Model output after zero-copy model loading:\n",
      "tensor([[[-0.1153,  0.2566, -0.2220,  ..., -0.3130,  0.6333,  0.6588],\n",
      "         [ 0.2769,  0.5195,  0.2059,  ..., -0.1062,  1.1186,  0.3836],\n",
      "         [ 0.9019,  0.7557, -0.1615,  ...,  0.0588,  0.3570, -0.0296],\n",
      "         ...,\n",
      "         [ 0.0155, -0.0602,  0.3365,  ..., -0.0936,  0.8055, -0.5007],\n",
      "         [ 0.6198,  0.2695, -0.3402,  ...,  0.0860, -0.3373, -0.4606],\n",
      "         [ 0.8493,  0.3726, -0.2073,  ..., -0.1145, -0.5216, -0.4418]]])\n"
     ]
    }
   ],
   "source": [
    "# Load tensors into the model's graph of Python objects\n",
    "replace_tensors(bert_skeleton, bert_weights)\n",
    "\n",
    "# Preprocess an example input string for BERT.\n",
    "test_text = \"All work and no play makes Jack a dull boy.\"\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-uncased\")\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the original model and the copy that we just loaded\n",
    "with torch.inference_mode():\n",
    "    print(\"Original model's output:\")\n",
    "    print(bert(**test_tokens).last_hidden_state)\n",
    "    print(\"\\nModel output after zero-copy model loading:\")\n",
    "    print(bert_skeleton(**test_tokens).last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db213b9-956e-4df2-a797-9f3a00558457",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "The first time you call the `replace_tensors()` function, PyTorch will print out a warning:\n",
    "\n",
    "```\n",
    "UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. [...]\n",
    "```\n",
    "\n",
    "Most PyTorch models don't modify their own weights during inference, but PyTorch doesn't prevent models from doing so. If you load your weights via the zero-copy method and your model modifies a weights tensor, it will change the copy of those weights in Plasma's shared memory. Ray (as of version 1.4) [always opens shared memory segments in read-write mode](https://github.com/ray-project/plasma/blob/7d6acc7af2878fc932ec5314cbcda0e79a9d6a4b/src/plasma_client.c#L111). \n",
    "\n",
    "If you're sure that you model does not not modify its own weights during inference, you can safely ignore this warning. You can test for these modifications by comparing your model's weights before and after inference. If your model does modify some of its weights, it's important to copy the relevant tensors prior to running inference.\n",
    "\n",
    "Another thing to note is that this method loads the model for CPU-based inference. To use GPU acceleration, you will need to copy the model's weights once to load them onto GPU memory. This copy operation takes about 0.07 seconds, which is still three times faster than the second-fastest way to load the model onto a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e049228-939a-4bdc-a503-ed277075bb6a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I hope you've enjoyed this introduction to zero-copy model loading with Ray and PyTorch. Being able to load models in milliseconds opens up some interesting architectural possibilities for high performance model inference. We're planning to cover some of those options in a later post.\n",
    "\n",
    "In the meantime, take a look at [project CodeFlare](https://www.research.ibm.com/blog/codeflare-ml-experiments?cm_mmc=text_extensions_for_pandas) to find out more about IBM Research's ongoing open source work around Ray, or try running Ray yourself with [IBM Cloud Code Engine](https://www.ibm.com/cloud/code-engine?cm_mmc=text_extensions_for_pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ab21b-0fd1-4b98-bfa6-100c57de69db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (This part not in blog) Source code for timing measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e919855-6bbc-42ba-8277-50a1a68dd632",
   "metadata": {},
   "source": [
    "faster version that can skip wrapping models in Parameter objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfd31e2-85ba-4893-ac38-3088d2c82117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL LOADING TIMINGS:\n",
      "\n",
      "Loading via official API:\n",
      "1.49 s ± 182 ms per loop (mean ± std. dev. of 100 runs, 1 loop each)\n",
      "Loading with torch.load():\n",
      "124 ms ± 3.72 ms per loop (mean ± std. dev. of 100 runs, 10 loops each)\n",
      "Loading with ray.get():\n",
      "221 ms ± 7.04 ms per loop (mean ± std. dev. of 100 runs, 1 loop each)\n",
      "Zero-copy load, using official APIs\n",
      "4.38 ms ± 144 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n",
      "Zero-copy load, bypassing Parameter class\n",
      "3.74 ms ± 143 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Timing measurements\n",
    "# Don't include this cell in the blog\n",
    "\n",
    "# A version of replace_tensors() that optionally allows a slightly \n",
    "# faster but slightly dangerous shortcut when loading Parameters. \n",
    "def replace_tensors_direct(m: torch.nn.Module, tensors: List[Dict]):\n",
    "    \"\"\"\n",
    "    Restore the tensors that extract_tensors() stripped out of a \n",
    "    PyTorch model.\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        modules = [module for _, module in m.named_modules()] \n",
    "        for module, tensor_dict in zip(modules, tensors):\n",
    "            # There are separate APIs to set parameters and buffers.\n",
    "            for name, array in tensor_dict[\"params\"].items():\n",
    "                # Super fast, somewhat risky version avoids \n",
    "                # wrapping parameters in Parameters objects.\n",
    "                module._parameters[name] = torch.as_tensor(array)\n",
    "            for name, array in tensor_dict[\"buffers\"].items():\n",
    "                module.register_buffer(name, torch.as_tensor(array))\n",
    "\n",
    "\n",
    "def restore_from_plasma(model_and_tensors_ref):\n",
    "    model, tensors = ray.get(model_and_tensors_ref)\n",
    "    replace_tensors(model, tensors)\n",
    "    return model\n",
    "\n",
    "\n",
    "def restore_from_plasma_direct(model_and_tensors_ref):\n",
    "    model, tensors = ray.get(model_and_tensors_ref)\n",
    "    replace_tensors_direct(model, tensors)\n",
    "    return model\n",
    "\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Begin comparison\n",
    "print(\"MODEL LOADING TIMINGS:\\n\")\n",
    "\n",
    "# Baseline: Load via the official API\n",
    "print(\"Loading via official API:\")\n",
    "bert_times = %timeit -o -r 100 transformers.BertModel.from_pretrained(bert_model_name)\n",
    "bert = transformers.BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Baseline 2: torch.load()\n",
    "print(\"Loading with torch.load():\")\n",
    "bert = transformers.BertModel.from_pretrained(bert_model_name)\n",
    "bert_file = \"outputs/bert.pt\"\n",
    "torch.save(bert, bert_file)\n",
    "\n",
    "bert_2_times = %timeit -o -r 100 torch.load(bert_file)\n",
    "bert_2 = torch.load(bert_file)\n",
    "\n",
    "# Baseline 3: ray.get()\n",
    "print(\"Loading with ray.get():\")\n",
    "bert_ref = ray.put(bert)\n",
    "\n",
    "# Ray.put() actually returns before things have completely settled down.\n",
    "time.sleep(1)\n",
    "\n",
    "bert_3_times = %timeit -o -r 100 ray.get(bert_ref)\n",
    "bert_3 = ray.get(bert_ref)\n",
    "\n",
    "# The main event: Zero-copy load\n",
    "bert_4_ref = ray.put(extract_tensors(bert))\n",
    "\n",
    "# Ray.put() returns before things have completely settled down.\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"Zero-copy load, using official APIs\")\n",
    "bert_4_times = %timeit -o -r 100 restore_from_plasma(bert_4_ref)\n",
    "bert_4 = restore_from_plasma(bert_4_ref)\n",
    "\n",
    "print(\"Zero-copy load, bypassing Parameter class\")\n",
    "bert_5_times = %timeit -o -r 100 restore_from_plasma_direct(bert_4_ref)\n",
    "bert_5 = restore_from_plasma_direct(bert_4_ref)\n",
    "\n",
    "\n",
    "# Test with CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    def restore_from_plasma_to_cuda(model_and_tensors_ref):\n",
    "        model, tensors = ray.get(model_and_tensors_ref)\n",
    "        replace_tensors(model, tensors)\n",
    "        model.cuda()\n",
    "        return model\n",
    "\n",
    "    bert = transformers.BertModel.from_pretrained(bert_model_name)\n",
    "    torch.save(bert, bert_file)\n",
    "    print(\"Loading with torch.load() to CUDA\")\n",
    "    bert_2_cuda_times = %timeit -o -r 100 torch.load(bert_file).cuda()\n",
    "\n",
    "    print(\"Zero-copy load to CUDA\")\n",
    "    bert_4_cuda_times = %timeit -o -r 100 restore_from_plasma_to_cuda(bert_4_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d9d70f-e630-405a-a2ce-87948aeec07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>5_percentile</th>\n",
       "      <th>95_percentile</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from_pretrained()</td>\n",
       "      <td>1.371406</td>\n",
       "      <td>1.813882</td>\n",
       "      <td>1.487760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torch.load()</td>\n",
       "      <td>0.118490</td>\n",
       "      <td>0.130622</td>\n",
       "      <td>0.124232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ray.get()</td>\n",
       "      <td>0.209548</td>\n",
       "      <td>0.233930</td>\n",
       "      <td>0.221145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zero_copy</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.004380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zero_copy_hack</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.003738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  5_percentile  95_percentile      mean\n",
       "0  from_pretrained()      1.371406       1.813882  1.487760\n",
       "1       torch.load()      0.118490       0.130622  0.124232\n",
       "2          ray.get()      0.209548       0.233930  0.221145\n",
       "3          zero_copy      0.004208       0.004756  0.004380\n",
       "4     zero_copy_hack      0.003529       0.004011  0.003738"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "\n",
    "# Number crunching for performance graph\n",
    "\n",
    "def stats_to_triple(timeit_output, name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract out 5%-95% range and mean stats from the output of %timeit\n",
    "\n",
    "    :param timeit_output: Object returned by %timeit -o\n",
    "    :param name: Name for the run that produced the performance numbers\n",
    "\n",
    "    :returns: Dictionary with keys \"name\", \"5_percentile\", \"95_percentile\", \n",
    "      and \"mean\", suitable for populating one row of a DataFrame.\n",
    "    \"\"\"\n",
    "    times = np.array(timeit_output.all_runs) / timeit_output.loops\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"5_percentile\": np.percentile(times, 5),\n",
    "        \"95_percentile\": np.percentile(times, 96),\n",
    "        \"mean\": np.mean(times)\n",
    "    }\n",
    "\n",
    "\n",
    "name_to_run = {\n",
    "    \"from_pretrained()\": bert_times,\n",
    "    \"torch.load()\": bert_2_times,\n",
    "    \"ray.get()\": bert_3_times,\n",
    "    \"zero_copy\": bert_4_times,\n",
    "    \"zero_copy_hack\": bert_5_times,\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    name_to_run[\"torch.load() CUDA\"] = bert_2_cuda_times\n",
    "    name_to_run[\"zero_copy CUDA\"] = bert_4_cuda_times\n",
    "\n",
    "\n",
    "records = [\n",
    "    stats_to_triple(times, name) for name, times in name_to_run.items()\n",
    "]\n",
    "\n",
    "timings = pd.DataFrame.from_records(records)\n",
    "timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78716d-2d56-4bb4-aa8f-233bf810a124",
   "metadata": {},
   "source": [
    "## (This part not in blog) Measure how long inference takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667c1c94-d9e1-4612-bbb2-c381453a2a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL INFERENCE TIMINGS:\n",
      "\n",
      "Original model, no CUDA:\n",
      "59.3 ms ± 1.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Zero-copy model loading, no CUDA:\n",
      "64.3 ms ± 803 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "\n",
    "# Inference timings\n",
    "\n",
    "# Redo tokenization to make this cell self-contained\n",
    "test_text = \"All work and no play makes Jack a dull boy.\"\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-uncased\")\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Common code to run inference\n",
    "def run_bert(b, t):\n",
    "    with torch.no_grad():\n",
    "        return b(**t).last_hidden_state\n",
    "\n",
    "\n",
    "print(\"LOCAL INFERENCE TIMINGS:\\n\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # Reload from scratch each time to be sure we aren't using stale values\n",
    "    print(\"Original model, no CUDA:\")\n",
    "    bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    %timeit run_bert(bert, test_tokens)\n",
    "\n",
    "    print(\"Zero-copy model loading, no CUDA:\")\n",
    "    bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_ref = ray.put(extract_tensors(bert))\n",
    "    bert_skeleton, bert_weights = ray.get(bert_ref)\n",
    "    replace_tensors(bert_skeleton, bert_weights)\n",
    "    %timeit run_bert(bert_skeleton, test_tokens)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        def run_bert_cuda(b, t):\n",
    "            # Inputs need to be on GPU if model is on GPU\n",
    "            t = {k: v.to(\"cuda\") for k, v in t.items()}\n",
    "            with torch.no_grad():\n",
    "                return b(**t).last_hidden_state\n",
    "\n",
    "        print(\"Original model, CUDA:\")\n",
    "        bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        bert.cuda()\n",
    "        %timeit run_bert_cuda(bert, test_tokens)\n",
    "\n",
    "        print(\"Zero-copy model loading, CUDA:\")\n",
    "        bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        bert_ref = ray.put(extract_tensors(bert))\n",
    "        bert_skeleton, bert_weights = ray.get(bert_ref)\n",
    "        replace_tensors(bert_skeleton, bert_weights)\n",
    "        bert_skeleton.cuda()\n",
    "        %timeit run_bert_cuda(bert_skeleton, test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d66d9-692b-40bf-b572-89aa86901362",
   "metadata": {},
   "source": [
    "## (This part not in blog) Measure how long inference takes via a Ray task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9343cd1c-a095-4da4-9bcd-6ebe54c27ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.238',\n",
       " 'raylet_ip_address': '192.168.0.238',\n",
       " 'redis_address': '192.168.0.238:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-09-20_15-06-48_395730_8167/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-09-20_15-06-48_395730_8167/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-09-20_15-06-48_395730_8167',\n",
       " 'metrics_export_port': 62556,\n",
       " 'node_id': 'f03a251faaca93abb004d685e7d312d1f2efa5ca3c867fcf94cfcc77'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reboot_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb259dcd-5b62-476a-a7bc-024666f9f655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOTE INFERENCE TIMINGS:\n",
      "\n",
      "Actor, no CUDA:\n",
      "63.6 ms ± 3.33 ms per loop (mean ± std. dev. of 100 runs, 1 loop each)\n",
      "Zero-copy, no CUDA:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8353)\u001b[0m <ipython-input-8-b7f49ff7af67>:17: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 ms ± 5.44 ms per loop (mean ± std. dev. of 100 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Don't include this cell in the blog.\n",
    "\n",
    "# Inference timings in remote process\n",
    "bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_ref = ray.put(extract_tensors(bert))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def run_bert_zero_copy(tokens):\n",
    "    bert_skeleton, bert_weights = ray.get(bert_ref)\n",
    "    replace_tensors(bert_skeleton, bert_weights)\n",
    "    with torch.no_grad():\n",
    "        return bert_skeleton(**tokens).last_hidden_state.detach().numpy()\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def run_bert_zero_copy_cuda(tokens):\n",
    "    bert_skeleton, bert_weights = ray.get(bert_ref)\n",
    "    replace_tensors(bert_skeleton, bert_weights)\n",
    "    bert_skeleton.cuda()\n",
    "\n",
    "    # Inputs also need to be on the GPU\n",
    "    tokens = {k: v.to(\"cuda\") for k, v in tokens.items()}\n",
    "    with torch.no_grad():\n",
    "        return bert_skeleton(**tokens).last_hidden_state.detach().numpy()\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class BertActor:\n",
    "    def __init__(self):\n",
    "        import transformers\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        self._bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self._bert.train(False)\n",
    "\n",
    "    def run_bert(self, tokens):\n",
    "        with torch.no_grad():\n",
    "            return self._bert(**tokens).last_hidden_state.detach().numpy()\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class BertActorCuda:\n",
    "    def __init__(self):\n",
    "        import transformers\n",
    "        transformers.logging.set_verbosity_error()\n",
    "        self._bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "        self._bert.train(False)\n",
    "\n",
    "    def run_bert(self, tokens):\n",
    "        with torch.no_grad():\n",
    "            tokens = {k: v.to(\"cuda\") for k, v in tokens.items()}\n",
    "            return self._bert(**tokens).last_hidden_state.detach().numpy()\n",
    "\n",
    "\n",
    "# Redo tokenization to make this cell self-contained\n",
    "test_text = \"All work and no play makes Jack a dull boy.\"\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-uncased\")\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "print(\"REMOTE INFERENCE TIMINGS:\\n\")\n",
    "\n",
    "print(\"Actor, no CUDA:\")\n",
    "actor = BertActor.remote()\n",
    "%timeit -o -r 100 ray.get(actor.run_bert.remote(test_tokens))\n",
    "del(actor)\n",
    "\n",
    "print(\"Zero-copy, no CUDA:\")\n",
    "%timeit -o -r 100 ray.get(run_bert_zero_copy.remote(test_tokens))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Actor, with CUDA:\")\n",
    "    actor = BertActorCuda.remote()\n",
    "    %timeit -o -r 100 ray.get(actor.run_bert.remote(test_tokens))\n",
    "    del(actor)\n",
    "\n",
    "    print(\"Zero-copy, with CUDA:\")\n",
    "    %timeit -o -r 100 ray.get(run_bert_zero_copy_cuda.remote(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89104a65-7352-40d6-bb95-271968519f2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (this part not in blog) Experiments on ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f378c85-ed26-4b21-a20d-3e635676b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and cache the ResNet model\n",
    "resnet_model_name = \"resnet50\"\n",
    "resnet_func = torchvision.models.resnet50\n",
    "resnet_file = \"outputs/resnet.pth\"\n",
    "# See https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html\n",
    "resnet_url = \"https://download.pytorch.org/models/resnet50-0676ba61.pth\"\n",
    "# resnet_url = \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\"\n",
    "\n",
    "\n",
    "if not os.path.exists(resnet_file):\n",
    "    os.system(f\"wget -O {resnet_file} {resnet_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d385a0ff-69a5-4a07-acaf-df13948bc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 ms ± 7.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Baseline method: Instantiate the model and call load_state_dict()\n",
    "%timeit resnet_func(pretrained=True)\n",
    "resnet = resnet_func(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65bfd36b-d5a7-443d-b75f-4382c3794ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 ms ± 1.52 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Baseline 2: torch.load()\n",
    "torch.save(resnet, \"outputs/resnet.torch\")\n",
    "%timeit torch.load(\"outputs/resnet.torch\")\n",
    "resnet_2 = torch.load(\"outputs/resnet.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64bcce39-fc34-4b93-a877-d2e6e93bbb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.8 ms ± 5.64 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Baseline 3: ray.get()\n",
    "resnet_ref = ray.put(resnet)\n",
    "\n",
    "# Ray.put() actually returns before things have completely settled down.\n",
    "time.sleep(1)\n",
    "\n",
    "%timeit ray.get(resnet_ref)\n",
    "resnet_3 = ray.get(resnet_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "084002cf-ddc3-47d0-830c-2e710973e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.98 ms ± 155 µs per loop (mean ± std. dev. of 20 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "resnet_ref = ray.put(extract_tensors(resnet))\n",
    "\n",
    "# Ray.put() actually returns before things have completely settled down.\n",
    "time.sleep(1)\n",
    "\n",
    "%timeit -r 20 restore_from_plasma(resnet_ref)\n",
    "resnet_4 = restore_from_plasma(resnet_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0847ff4-b392-4c3a-bd98-e97b40254ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vs 2: []\n",
      "1 vs 3: []\n",
      "1 vs 4: []\n"
     ]
    }
   ],
   "source": [
    "# Compare parameters to verify that ResNet was loaded properly.\n",
    "params_1 = list(resnet.parameters())\n",
    "params_2 = list(resnet_2.parameters())\n",
    "params_3 = list(resnet_3.parameters())\n",
    "params_4 = list(resnet_4.parameters())\n",
    "\n",
    "\n",
    "def compare_lists(l1, l2):\n",
    "    different_indices = []\n",
    "    for i in range(len(l1)):\n",
    "        if not torch.equal(l1[i], l2[i]):\n",
    "            different_indices.append(i)\n",
    "    return different_indices\n",
    "\n",
    "\n",
    "print(f\"1 vs 2: {compare_lists(params_1, params_2)}\")\n",
    "print(f\"1 vs 3: {compare_lists(params_1, params_2)}\")\n",
    "print(f\"1 vs 4: {compare_lists(params_1, params_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0d68321-3245-4e5b-b004-61af387cf6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vs 2: []\n",
      "1 vs 3: []\n",
      "1 vs 4: []\n"
     ]
    }
   ],
   "source": [
    "# Compare buffers to verify that ResNet was loaded properly.\n",
    "bufs_1 = list(resnet.buffers())\n",
    "bufs_2 = list(resnet_2.buffers())\n",
    "bufs_3 = list(resnet_3.buffers())\n",
    "bufs_4 = list(resnet_4.buffers())\n",
    "\n",
    "print(f\"1 vs 2: {compare_lists(bufs_1, bufs_2)}\")\n",
    "print(f\"1 vs 3: {compare_lists(bufs_1, bufs_3)}\")\n",
    "print(f\"1 vs 4: {compare_lists(bufs_1, bufs_4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdeaa3bb-1f6e-4c96-bc58-cf7ff52a468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"outputs/dog.jpg\")\n",
    "try:\n",
    "    urllib.URLopener().retrieve(url, filename)\n",
    "except BaseException:\n",
    "    # Different versions of urllib have different APIs\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "def run_image_through_resnet(model):\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "    input_image = Image.open(filename)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    # create a mini-batch as expected by the model\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Make sure the model is not in training mode.\n",
    "    model.eval()\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model(input_batch)\n",
    "    # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "    # The output has unnormalized scores. To get probabilities, you can run a\n",
    "    # softmax on it.\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    return(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3cbe522-7f10-4c1a-bcc5-b77ac1100af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.24 msec elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/freiss/pd/zero-copy-model-loading/env/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2477e-06, 5.7400e-07, 7.9565e-07, 4.7353e-07, 1.4483e-07, 3.1733e-06,\n",
       "        8.5920e-07, 5.1084e-05, 3.1556e-04, 2.8510e-06])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the models still run\n",
    "before_sec = time.time()\n",
    "result = run_image_through_resnet(resnet)[0:10]\n",
    "print(f\"{1000 * (time.time() - before_sec):1.2f} msec elapsed\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c5299d7-18ed-435e-b69d-7200c84436da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.80 msec elapsed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2477e-06, 5.7400e-07, 7.9565e-07, 4.7353e-07, 1.4483e-07, 3.1733e-06,\n",
       "        8.5920e-07, 5.1084e-05, 3.1556e-04, 2.8510e-06])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_sec = time.time()\n",
    "result = run_image_through_resnet(resnet_2)[0:10]\n",
    "print(f\"{1000 * (time.time() - before_sec):1.2f} msec elapsed\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a9cbb0c-28e6-49e2-b4d1-2138fc5b1af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164.47 msec elapsed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2477e-06, 5.7400e-07, 7.9565e-07, 4.7353e-07, 1.4483e-07, 3.1733e-06,\n",
       "        8.5920e-07, 5.1084e-05, 3.1556e-04, 2.8510e-06])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_sec = time.time()\n",
    "result = run_image_through_resnet(resnet_3)[0:10]\n",
    "print(f\"{1000 * (time.time() - before_sec):1.2f} msec elapsed\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8facadb-39b3-4c46-a354-e2831fa61206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159.69 msec elapsed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2477e-06, 5.7400e-07, 7.9565e-07, 4.7353e-07, 1.4483e-07, 3.1733e-06,\n",
       "        8.5920e-07, 5.1084e-05, 3.1556e-04, 2.8510e-06])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_sec = time.time()\n",
    "result = run_image_through_resnet(resnet_4)[0:10]\n",
    "print(f\"{1000 * (time.time() - before_sec):1.2f} msec elapsed\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b143e2-2400-4795-9c01-8fad4acd75b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
