{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8270a16d",
   "metadata": {},
   "source": [
    "# benchmark.ipynb\n",
    "\n",
    "This notebook contains the text and code for the next blog post in the zero-copy model series, \n",
    "title TBD.\n",
    "\n",
    "The first post explained how to load PyTorch models for inference extremely fast by leveraging the Plasma object store's ability to load numeric data directly from shared memory.\n",
    "\n",
    "In this post, we talk in more concrete terms about how to use this zero-copy model loading for model serving. We put together a simple model serving system, then set up a microbenchmark that simulates a heavy-tailed traffic pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbf7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import copy\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "\n",
    "from typing import Dict, Any, Callable, Tuple, List\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import zerocopy\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69fefd",
   "metadata": {},
   "source": [
    "# Title of new blog post goes here\n",
    "\n",
    "*Recap of previous blog post goes here.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eccd9",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "The end-to-end scenario for our benchmark involves supporting an AI chatbot.\n",
    "The chatbot's conversational AI runs off of a conversation tree. Some of the \n",
    "nodes of this tree invoke models.\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "The end-to-end scenario for our benchmark involves supporting an AI chatbot.\n",
    "The chatbot's conversational AI runs off of a conversation tree (**TODO:** What's the best term for this tree?). Some of the nodes of this tree invoke question answering models.\n",
    "\n",
    "Our benchmark will cover the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs question answering (QA) models on behalf of the \n",
    "chatbot's conversational AI. The chatbot's conversation tree leads to 4 very different\n",
    "question answering scenarios, and each scenario has its own dedicated QA\n",
    "model. Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models to cover the 4 question types and 3 languages.\n",
    "-->\n",
    "\n",
    "> **TODO:** Cartoon block diagram of the end-to-end scenario. \n",
    "> Diagram should show a user interacting with a chatbot. The chatbot runs off of a conversation tree. \n",
    "> Some of the nodes of the conversation tree have question answering models hanging off of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc5ddc-35bc-4c70-b3c8-c9e0debbc8bb",
   "metadata": {},
   "source": [
    "## The Models\n",
    "Our benchmark will cover the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs four different types of models:\n",
    "* *Intent classification* models that determine what category of information the user is interested in.\n",
    "* *Question answering* models that provide the answers to specific factual questions.\n",
    "* *Sentiment analysis* models that monitor the user's mood and frustration level.\n",
    "* *Natural language generation* models that give the chatbot's responses a less scripted flavor.\n",
    "\n",
    "Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models to cover the 4 model types and 3 languages.\n",
    "\n",
    "We'll simulate each of these four model types by drawing the most popular model from each category from the [Huggingface model marketplace](https://huggingface.co/models).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02179a50",
   "metadata": {},
   "source": [
    "### Intent Model\n",
    "\n",
    "For our intent classification models, we'll use the model [`mrm8488/t5-base-finetuned-e2m-intent`](https://huggingface.co/mrm8488/t5-base-finetuned-e2m-intent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57eb7bac-c341-4eb8-9355-d14945e1eb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load intent model with standard method: 3.04 s ± 66.1 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "\n",
    "# Measure loading time from local disk.\n",
    "print('Time to load intent model with standard method: ', end='')\n",
    "%timeit -r3 transformers.AutoModelForSeq2SeqLM.from_pretrained(INTENT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8e6a6",
   "metadata": {},
   "source": [
    "The intent model comes as three parts: A *tokenizer* that converts raw text into a sequence numeric token IDs, a core *model* that transforms these token sequences, and a *preprocessing and postprocessing code* to choreograph the usage of the first two parts. For convenience, we group these components into a single Python class with a `__call__()` method that runs the end-to-end inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55f2dd0-169e-461e-b4c0-1b5c532214df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'to eat'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IntentPipeline:\n",
    "    '''\n",
    "    Serving pipeline implementation for the intent model.\n",
    "\n",
    "    Runs preprocessing, model invocation, and postprocessing steps.\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Tokenizer loading code from the model zoo doesn't work, so we \n",
    "        # explicitly specify the t5-base tokenizer.\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "        self._model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            INTENT_MODEL_NAME)\n",
    "        self._max_length = 128  # Max sequence length, input + output, in tokens\n",
    "\n",
    "    def __call__(self, context: str) -> str:\n",
    "        input_text = f'{context} </s>'\n",
    "        features = self._tokenizer([input_text], return_tensors='pt')\n",
    "        output = self._model.generate(\n",
    "            input_ids=features['input_ids'], \n",
    "            attention_mask=features['attention_mask'],\n",
    "            max_length=self._max_length)\n",
    "        result_string = self._tokenizer.decode(output[0])\n",
    "\n",
    "        # Direct model result is in the form <pad> [intent]</s>. Strip off the\n",
    "        # prefix and suffix\n",
    "        return result_string[len('<pad> '):-len('</s>')]\n",
    "\n",
    "\n",
    "intent_input = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and kick butt, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "intent_result = {'intent': IntentPipeline()(intent_input['context'])}\n",
    "intent_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d14f7-6e5b-46dd-80af-ed7d6c669755",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Model\n",
    "\n",
    "For our sentiment analysis models, we'll use model [`cardiffnlp/twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e487cd-ce01-436e-bb28-818711b048b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load sentiment model with standard method: 1.77 s ± 68.9 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "\n",
    "print('Time to load sentiment model with standard method: ', end='')\n",
    "%timeit -r3 transformers.AutoModelForSequenceClassification.from_pretrained(SENTIMENT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b97c8",
   "metadata": {},
   "source": [
    "Like the intent model, the sentiment model doesn't come with a prebuilt pipeline, so we create our own wrapper class for end-to-end inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3226375-121b-4072-a7fe-b042151a2887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive', 0.5419477820396423),\n",
       " ('neutral', 0.38251084089279175),\n",
       " ('negative', 0.07554134726524353)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentPipeline:\n",
    "    '''\n",
    "    Serving pipeline implementation for the sentiment model.\n",
    "\n",
    "    Runs preprocessing, model invocation, and postprocessing steps.\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Tokenizer loading code from the model zoo doesn't work, so we\n",
    "        # explicitly specify the t5-base tokenizer.\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            SENTIMENT_MODEL_NAME)\n",
    "        self._model = (transformers.AutoModelForSequenceClassification\n",
    "                       .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "\n",
    "    def __call__(self, context: str) -> str:\n",
    "        encoded_input = self._tokenizer(context, return_tensors='pt')\n",
    "        output = self._model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = scipy.special.softmax(scores)\n",
    "        scores = [float(s) for s in scores]\n",
    "        return list(zip(['positive', 'neutral', 'negative'], scores))\n",
    "\n",
    "\n",
    "sentiment_input = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "\n",
    "sentiment_result = SentimentPipeline()(sentiment_input['context'])\n",
    "sentiment_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e4dd5-087d-48e8-9cbc-a1e04367294f",
   "metadata": {},
   "source": [
    "### Question Answering Model\n",
    "\n",
    "For our question answering models, we'll use the model [`deepset/roberta-base-squad2`](https://huggingface.co/deepset/roberta-base-squad2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37dae247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load QA model with standard method: 6.01 s ± 6.92 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "\n",
    "# Strip out this timing code for the blog version.\n",
    "print('Time to load QA model with standard method: ', end='')\n",
    "%timeit -r3 transformers.pipeline('question-answering', model=QA_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3eaf3",
   "metadata": {},
   "source": [
    "Unlike the intent and sentiment models, the question answering model comes prepackaged as a `question-answering` pipeline via the `tokenizers` library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines). So we can load and run end to end inference by creating an instance of the pipeline class and invoking that object's `__call__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88c6a30-a5c1-4260-92fe-2ef117a617a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input for the QA model\n",
    "qa_input = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "qa_result = transformers.pipeline('question-answering',\n",
    "                                  model=QA_MODEL_NAME)(qa_input)\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd548d2f-824f-4f22-8c63-91c80a5fb633",
   "metadata": {},
   "source": [
    "### Natural Language Generation Model\n",
    "\n",
    "For natural language generation, we'll use the model [`gpt2`](https://huggingface.co/gpt2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec19d4c-ca29-48a4-be25-e3e01de8dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load natural language generation model with standard method: 6.02 s ± 50.6 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "GENERATE_MODEL_NAME = 'gpt2'\n",
    "GPT2_TOKEN_PAD_ID = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\").eos_token_id\n",
    "\n",
    "print('Time to load natural language generation model with standard method: ',\n",
    "      end='')\n",
    "%timeit -r3 transformers.pipeline('text-generation', model=GENERATE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a63dc9",
   "metadata": {},
   "source": [
    "Like the question answering model, this natural language generation model comes wrapped in a `tokenizers` pipeline class. The class's `__call__()` method performs all the steps necessary to run end-to-end inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85336a8c-685c-4d44-bec7-101cdb3fd3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'All your base are your friends, and they are all with you.\\n\\n3) The game will give your character some abilities, just like in'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_input = {\n",
    "    'context': 'All your base are'\n",
    "}\n",
    "\n",
    "generate_result = transformers.pipeline(\n",
    "    'text-generation', model=GENERATE_MODEL_NAME, \n",
    "    pad_token_id=GPT2_TOKEN_PAD_ID)(\n",
    "        generate_input['context'], max_length=30, \n",
    "        num_return_sequences=1)\n",
    "generate_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b1aa",
   "metadata": {},
   "source": [
    "## Baseline implementation\n",
    "\n",
    "Let's start with a baseline implementation of model serving for this model. This baseline implementation emulates running each model in a separate container. The server has 12 CPUs, so each container gets 1 CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c497c62-44af-4f52-add0-98ffa540c9f7",
   "metadata": {},
   "source": [
    "### Model deployment\n",
    "\n",
    "We implement this baseline configuration with a pool of Ray actors.\n",
    "\n",
    "*TODO: Redo this part with a model serving framework that is not Ray.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f538ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 16:58:27,347\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/freiss/ray/zero-copy-model-loading/env/lib/python3.8/site-packages/ray/dashboard/agent.py:152: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   if LooseVersion(aiohttp.__version__) < LooseVersion(\"4.0.0\"):\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:31,044\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:31,054\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:XFVewt:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-02-09 16:58:31,731\tINFO api.py:463 -- Started Serve instance in namespace 'f040bdfd-4a0b-490e-8f5d-6d9fe316c6a2'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7ff678c95220>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a190857-a836-47b9-8c60-c84289eb10f4",
   "metadata": {},
   "source": [
    "We wrap each model's inference pipeline in a Python class that handles translating arguments and results to and from JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8a6ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66800)\u001b[0m INFO:     Started server process [66800]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Generate at 0x7ff674031ee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Intent:\n",
    "    def __init__(self):\n",
    "        self._pipeline = IntentPipeline()\n",
    "\n",
    "    def __call__(self, request: starlette.requests.Request):\n",
    "        return {\n",
    "            \"intent\": self._pipeline(request.query_params['context'])\n",
    "        }\n",
    "\n",
    "\n",
    "class Sentiment:\n",
    "    def __init__(self):\n",
    "        self._pipeline = SentimentPipeline()\n",
    "\n",
    "    def __call__(self, request: starlette.requests.Request):\n",
    "        return self._pipeline(request.query_params['context'])\n",
    "\n",
    "\n",
    "class QA:\n",
    "    def __init__(self):\n",
    "        self._pipeline = transformers.pipeline('question-answering', \n",
    "                                               model=QA_MODEL_NAME)\n",
    "\n",
    "    def __call__(self, request: starlette.requests.Request):\n",
    "        model_input = {\n",
    "            'question': request.query_params['question'],\n",
    "            'context': request.query_params['context']\n",
    "        }\n",
    "        return self._pipeline(model_input)\n",
    "\n",
    "\n",
    "class Generate:\n",
    "    def __init__(self):\n",
    "        self._pipeline = transformers.pipeline(\n",
    "            'text-generation',\n",
    "            model=GENERATE_MODEL_NAME, \n",
    "            pad_token_id=GPT2_TOKEN_PAD_ID)\n",
    "        self._max_length = 30\n",
    "        self._num_return_sequences = 1\n",
    "\n",
    "    def __call__(self, request: starlette.requests.Request):\n",
    "        model_input = request.query_params['context']\n",
    "        return self._pipeline(\n",
    "            model_input,\n",
    "            max_length=self._max_length,\n",
    "            num_return_sequences=self._num_return_sequences)\n",
    "\n",
    "\n",
    "# Verify that all the constructors for these classes work\n",
    "Intent()\n",
    "Sentiment()\n",
    "QA()\n",
    "Generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c18e8c-76e4-452d-8b50-cafe624e96f0",
   "metadata": {},
   "source": [
    "Then we can wrap these classes directly in Ray Serve deployments, one deployment for each model. Recall that we have a model for each combination of task and target language, for a total of 12 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757d546e-f27d-4951-a4a2-0c6207d198f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 16:58:57,346\tINFO api.py:242 -- Updating deployment 'en_intent'. component=serve deployment=en_intent\n",
      "2022-02-09 16:58:57,355\tINFO api.py:242 -- Updating deployment 'en_sentiment'. component=serve deployment=en_sentiment\n",
      "2022-02-09 16:58:57,364\tINFO api.py:242 -- Updating deployment 'en_qa'. component=serve deployment=en_qa\n",
      "2022-02-09 16:58:57,378\tINFO api.py:242 -- Updating deployment 'en_generate'. component=serve deployment=en_generate\n",
      "2022-02-09 16:58:57,387\tINFO api.py:242 -- Updating deployment 'es_intent'. component=serve deployment=es_intent\n",
      "2022-02-09 16:58:57,405\tINFO api.py:242 -- Updating deployment 'es_sentiment'. component=serve deployment=es_sentiment\n",
      "2022-02-09 16:58:57,415\tINFO api.py:242 -- Updating deployment 'es_qa'. component=serve deployment=es_qa\n",
      "2022-02-09 16:58:57,428\tINFO api.py:242 -- Updating deployment 'es_generate'. component=serve deployment=es_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,429\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_intent'. component=serve deployment=en_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,450\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_sentiment'. component=serve deployment=en_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,469\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_qa'. component=serve deployment=en_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,487\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_generate'. component=serve deployment=en_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,509\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_intent'. component=serve deployment=es_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,531\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_sentiment'. component=serve deployment=es_sentiment\n",
      "2022-02-09 16:58:57,632\tINFO api.py:242 -- Updating deployment 'zh_intent'. component=serve deployment=zh_intent\n",
      "2022-02-09 16:58:57,658\tINFO api.py:242 -- Updating deployment 'zh_sentiment'. component=serve deployment=zh_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,555\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_qa'. component=serve deployment=es_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,583\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_generate'. component=serve deployment=es_generate\n",
      "2022-02-09 16:58:57,696\tINFO api.py:242 -- Updating deployment 'zh_qa'. component=serve deployment=zh_qa\n",
      "2022-02-09 16:58:57,735\tINFO api.py:242 -- Updating deployment 'zh_generate'. component=serve deployment=zh_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,742\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_intent'. component=serve deployment=zh_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,769\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_sentiment'. component=serve deployment=zh_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,800\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_qa'. component=serve deployment=zh_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 16:58:57,830\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_generate'. component=serve deployment=zh_generate\n"
     ]
    }
   ],
   "source": [
    "# Define endpoints\n",
    "LANGUAGES = ['en', 'es', 'zh']\n",
    "\n",
    "deployments = {}\n",
    "for lang in LANGUAGES:\n",
    "    deployments[(lang, 'intent')] = (\n",
    "        serve.deployment(Intent, f'{lang}_intent'))\n",
    "    deployments[(lang, 'sentiment')] = (\n",
    "        serve.deployment(Sentiment, f'{lang}_sentiment'))\n",
    "    deployments[(lang, 'qa')] = (\n",
    "        serve.deployment(QA, f'{lang}_qa'))\n",
    "    deployments[(lang, 'generate')] = (\n",
    "        serve.deployment(Generate, f'{lang}_generate'))\n",
    "\n",
    "\n",
    "for d in deployments.values():\n",
    "    d.deploy(_blocking=False)\n",
    "\n",
    "# Wait a moment so log output doesn't go to the next cell's output\n",
    "time.sleep(5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f4b4b-015e-4c3d-ae71-b954eceb5b4a",
   "metadata": {},
   "source": [
    "Now these models are callable via HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736c66fd-cb4a-4886-a40f-f66872bb5c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(en_intent pid=66808)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(es_intent pid=66804)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(zh_intent pid=66798)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intent': 'to eat'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the intent model deployments are working\n",
    "# This cell may be delayed until async deployments from the previous cell are\n",
    "# complete.\n",
    "params = urllib.parse.urlencode(intent_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_intent?{params}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee755848-b005-4a7a-9a10-7a0ba8ca57d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['positive', 0.5419477820396423],\n",
       " ['neutral', 0.38251084089279175],\n",
       " ['negative', 0.07554134726524353]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the sentiment model deployments are working\n",
    "params = urllib.parse.urlencode(sentiment_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_sentiment?{params}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fcdd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the QA model deployments are working\n",
    "params = urllib.parse.urlencode(qa_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_qa?{params}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dabf488-f9b3-4fee-905f-ac53e9f19920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"All your base are ready for your next set of rules...\\nThere's two ways you can modify the rules to suit your game play style: use\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the NLG model deployments are working\n",
    "params = urllib.parse.urlencode(generate_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_generate?{params}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4316-86af-481f-8cc0-887a88c30c0d",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "\n",
    "Now that we have deployed each of our models with a web service front end, we can define a benchmark that sends inference traffic to these web service endpoints and measures response time.\n",
    "\n",
    "We start by wrapping all the web services in a single callback function that calls a model, retrieves the result, verifies the result, and returns elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "079ad698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"intent\" model takes 0.288 seconds.\n",
      "The \"sentiment\" model takes 0.082 seconds.\n",
      "The \"qa\" model takes 0.522 seconds.\n",
      "The \"generate\" model takes 1.042 seconds.\n"
     ]
    }
   ],
   "source": [
    "# For now, we have a single canned input for each model type.\n",
    "MODEL_INPUTS = {\n",
    "    'intent': intent_input,\n",
    "    'sentiment': sentiment_input,\n",
    "    'qa': qa_input,\n",
    "    'generate': generate_input\n",
    "}\n",
    "\n",
    "# Expected results by model type\n",
    "MODEL_RESULTS = {\n",
    "    'intent': intent_result,\n",
    "    'sentiment': sentiment_result,\n",
    "    'qa': qa_result,\n",
    "    # Generate model produces a different result each time it's called.\n",
    "    # 'generate': generate_result\n",
    "}\n",
    "\n",
    "MODEL_TYPES = list(MODEL_INPUTS.keys())\n",
    "\n",
    "\n",
    "def call_model(model_type: str, language: str) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Callack function that calls the model deployment, retrieves and\n",
    "    validates the result, and returns elapsed time.\n",
    "\n",
    "    :param model_type: Type of model to call; must be one of\n",
    "                       'intent', 'sentiment', 'qa', or 'generate'\n",
    "    :param language: Two-letter language code; must be one of\n",
    "                     'en', 'es', 'zh'\n",
    "\n",
    "    :returns: Tuple of start and end times of the web service call\n",
    "    '''\n",
    "    if model_type not in MODEL_TYPES:\n",
    "        raise ValueError(f'Unexpected model type \"{model_type}\" '\n",
    "                         f'(expected {MODEL_TYPES}')\n",
    "    if language not in LANGUAGES:\n",
    "        raise ValueError(f'Unexpected language code \"{language}\" '\n",
    "                         f'(expected {LANGUAGES}')\n",
    "\n",
    "    # For now, use the same input every time\n",
    "    params = urllib.parse.urlencode(MODEL_INPUTS[model_type])\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = requests.get(\n",
    "        f'http://127.0.0.1:8000/{language}_{model_type}?{params}').json()\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Do some basic validation\n",
    "    if (model_type in MODEL_RESULTS.keys() and\n",
    "            json.dumps(result) != json.dumps(MODEL_RESULTS[model_type])):\n",
    "        raise ValueError(f'Unexpected result: {result}; '\n",
    "                         f'Expected: {MODEL_RESULTS[model_type]}')\n",
    "\n",
    "    return (start_time, end_time)\n",
    "\n",
    "\n",
    "# Test with each model type\n",
    "for model_type in MODEL_INPUTS.keys():\n",
    "    #print(f'Calling {model_type}')\n",
    "    times = call_model(model_type, 'en')\n",
    "    print(f'The \"{model_type}\" model takes {times[1] - times[0]:1.3f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92a1f5-3624-4f22-8a95-0b9d9c2f32d6",
   "metadata": {},
   "source": [
    "Our benchmark generates a trace of requests, then plays back the trace and measures the \n",
    "latency of each request. \n",
    "\n",
    "The request rate changes each second, with the rate of a particular 1-second window drawn from the Poisson\n",
    "distribution. Here's the code to generate the start times for the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110ace1f-9c9d-4b55-a50b-9c20d2d9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_start_times(requests_per_sec: float, num_sec: int,\n",
    "                    seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a trace of inference request start times. Divides the trace\n",
    "    into 1-second intervals. Each interval gets a number of requests drawn\n",
    "    from a Poissson distribution. These requests are evenly spread through the\n",
    "    interval.\n",
    "\n",
    "    :param requests_per_sec: Average requests per second overall\n",
    "    :param num_sec: Number of seconds of trace to generate\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Numpy array of timestamps (starting from 0) for the requests\n",
    "     in the trace\n",
    "    \"\"\"\n",
    "    trace = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Compute the number of requests in each 1-second window.\n",
    "    req_per_window = rng.poisson(requests_per_sec, size=num_sec)\n",
    "\n",
    "    for window_num in range(num_sec):\n",
    "        num_requests = req_per_window[window_num]\n",
    "        if num_requests > 0:\n",
    "            request_interval = 1.0 / num_requests\n",
    "            for i in range(num_requests):\n",
    "                trace.append(window_num + request_interval * i)\n",
    "\n",
    "    return np.array(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c681f9-e06e-4da8-a6e8-08ae5e54fa85",
   "metadata": {},
   "source": [
    "Each request goes to a randomly-selected model. The choice of models is\n",
    "weighted according to a truncated Poisson distribution. Here's the code to generate\n",
    "the list of model IDs for the requests in the trace. When we play back the trace,\n",
    "we'll map each integer model ID to a combination of a language code and a model type\n",
    "--- for example, `('en', 'sentiment')` for the English sentiment model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451f1281-6c46-45e9-b397-5d8898717e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en', 'intent'),\n",
       " ('en', 'sentiment'),\n",
       " ('en', 'qa'),\n",
       " ('en', 'generate'),\n",
       " ('es', 'intent'),\n",
       " ('es', 'sentiment'),\n",
       " ('es', 'qa'),\n",
       " ('es', 'generate'),\n",
       " ('zh', 'intent'),\n",
       " ('zh', 'sentiment'),\n",
       " ('zh', 'qa'),\n",
       " ('zh', 'generate')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_model_ids(lambda_: float, num_models: int, num_points: int,\n",
    "                  seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw integer model IDs at random from a truncated Poisson distribution.\n",
    "\n",
    "    :param lambda_: Primary parameter of the distribution, which also happens to \n",
    "     be the mean value of the (untruncated) distribution.\n",
    "    :param num_models: Number of models; generated IDs will range from 0 to\n",
    "                       `num_models - 1`, inclusive.\n",
    "    :param num_points: Number of random model IDs to return.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Randomly generated model IDs for a series of requests, as a\n",
    "     1D Numpy array of integers.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Draw integers from a truncated Poisson distribution. Start with a \n",
    "    # non-truncated distribution, then resample for\n",
    "    # any values that went over the limit.\n",
    "    int_ids = rng.poisson(lambda_, size=num_points)\n",
    "    while np.any(int_ids >= num_models):\n",
    "        new_values = rng.poisson(lambda_, size=np.sum(int_ids >= num_models))\n",
    "        int_ids[int_ids >= num_models] = new_values\n",
    "    return int_ids\n",
    "\n",
    "# Map the integer model IDs from the trace to pairs of language code and\n",
    "# model type.\n",
    "MODEL_ID_TO_PARAMS = [\n",
    "    (lang_code, model_name)\n",
    "    for lang_code in LANGUAGES\n",
    "    for model_name in MODEL_TYPES\n",
    "]\n",
    "\n",
    "MODEL_ID_TO_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61453629-426e-4b09-8430-4b2c11b71a64",
   "metadata": {},
   "source": [
    "The benchmark itself generates and then plays back the trace, measuring the end-to-end latency of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfa55ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>model_type</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.084730</td>\n",
       "      <td>0.083161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125163</td>\n",
       "      <td>0.206862</td>\n",
       "      <td>0.081699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250241</td>\n",
       "      <td>0.537558</td>\n",
       "      <td>0.287317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.376230</td>\n",
       "      <td>0.905310</td>\n",
       "      <td>0.529080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.504184</td>\n",
       "      <td>0.822487</td>\n",
       "      <td>0.318303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.627655</td>\n",
       "      <td>1.107790</td>\n",
       "      <td>0.480135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.751775</td>\n",
       "      <td>1.386082</td>\n",
       "      <td>0.634307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.879065</td>\n",
       "      <td>0.965719</td>\n",
       "      <td>0.086654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.004035</td>\n",
       "      <td>1.090324</td>\n",
       "      <td>0.086289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.127867</td>\n",
       "      <td>1.945572</td>\n",
       "      <td>0.817705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.250648</td>\n",
       "      <td>1.946135</td>\n",
       "      <td>0.695487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.380407</td>\n",
       "      <td>2.501325</td>\n",
       "      <td>1.120918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.501794</td>\n",
       "      <td>1.587009</td>\n",
       "      <td>0.085215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.625900</td>\n",
       "      <td>1.714802</td>\n",
       "      <td>0.088902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.750388</td>\n",
       "      <td>2.275705</td>\n",
       "      <td>0.525317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.879228</td>\n",
       "      <td>2.501481</td>\n",
       "      <td>0.622253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.002623</td>\n",
       "      <td>3.045272</td>\n",
       "      <td>1.042649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>2.114994</td>\n",
       "      <td>3.045860</td>\n",
       "      <td>0.930866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>2.223887</td>\n",
       "      <td>3.592544</td>\n",
       "      <td>1.368657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.338568</td>\n",
       "      <td>3.593138</td>\n",
       "      <td>1.254570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>2.447981</td>\n",
       "      <td>2.533278</td>\n",
       "      <td>0.085297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>2.558925</td>\n",
       "      <td>3.877389</td>\n",
       "      <td>1.318464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.670292</td>\n",
       "      <td>6.128211</td>\n",
       "      <td>3.457919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>2.779707</td>\n",
       "      <td>6.128913</td>\n",
       "      <td>3.349206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>2.891834</td>\n",
       "      <td>6.129573</td>\n",
       "      <td>3.237739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000653</td>\n",
       "      <td>6.130708</td>\n",
       "      <td>3.130055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>3.144771</td>\n",
       "      <td>6.130505</td>\n",
       "      <td>2.985734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>3.288667</td>\n",
       "      <td>6.131278</td>\n",
       "      <td>2.842611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>3.432834</td>\n",
       "      <td>6.132442</td>\n",
       "      <td>2.699608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>3.572371</td>\n",
       "      <td>6.132652</td>\n",
       "      <td>2.560281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.719234</td>\n",
       "      <td>3.808971</td>\n",
       "      <td>0.089737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.859016</td>\n",
       "      <td>6.672758</td>\n",
       "      <td>2.813742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.001131</td>\n",
       "      <td>4.087514</td>\n",
       "      <td>0.086383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.171170</td>\n",
       "      <td>6.673302</td>\n",
       "      <td>2.502132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.336855</td>\n",
       "      <td>7.765454</td>\n",
       "      <td>3.428599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.506967</td>\n",
       "      <td>7.766037</td>\n",
       "      <td>3.259070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.669845</td>\n",
       "      <td>7.767299</td>\n",
       "      <td>3.097454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>4.836232</td>\n",
       "      <td>7.767128</td>\n",
       "      <td>2.930896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    request_id  model_num lang_code model_type  desired_start  actual_start  \\\n",
       "0            0          1        en  sentiment       0.000000      0.001569   \n",
       "1            1          1        en  sentiment       0.125000      0.125163   \n",
       "2            2          0        en     intent       0.250000      0.250241   \n",
       "3            3          2        en         qa       0.375000      0.376230   \n",
       "4            4          0        en     intent       0.500000      0.504184   \n",
       "5            5          0        en     intent       0.625000      0.627655   \n",
       "6            6          0        en     intent       0.750000      0.751775   \n",
       "7            7          1        en  sentiment       0.875000      0.879065   \n",
       "8            8          1        en  sentiment       1.000000      1.004035   \n",
       "9            9          0        en     intent       1.125000      1.127867   \n",
       "10          10          0        en     intent       1.250000      1.250648   \n",
       "11          11          0        en     intent       1.375000      1.380407   \n",
       "12          12          1        en  sentiment       1.500000      1.501794   \n",
       "13          13          1        en  sentiment       1.625000      1.625900   \n",
       "14          14          2        en         qa       1.750000      1.750388   \n",
       "15          15          0        en     intent       1.875000      1.879228   \n",
       "16          16          0        en     intent       2.000000      2.002623   \n",
       "17          17          0        en     intent       2.111111      2.114994   \n",
       "18          18          0        en     intent       2.222222      2.223887   \n",
       "19          19          0        en     intent       2.333333      2.338568   \n",
       "20          20          1        en  sentiment       2.444444      2.447981   \n",
       "21          21          0        en     intent       2.555556      2.558925   \n",
       "22          22          0        en     intent       2.666667      2.670292   \n",
       "23          23          0        en     intent       2.777778      2.779707   \n",
       "24          24          0        en     intent       2.888889      2.891834   \n",
       "25          25          0        en     intent       3.000000      3.000653   \n",
       "26          26          0        en     intent       3.142857      3.144771   \n",
       "27          27          0        en     intent       3.285714      3.288667   \n",
       "28          28          0        en     intent       3.428571      3.432834   \n",
       "29          29          0        en     intent       3.571429      3.572371   \n",
       "30          30          1        en  sentiment       3.714286      3.719234   \n",
       "31          31          0        en     intent       3.857143      3.859016   \n",
       "32          32          1        en  sentiment       4.000000      4.001131   \n",
       "33          33          0        en     intent       4.166667      4.171170   \n",
       "34          34          0        en     intent       4.333333      4.336855   \n",
       "35          35          0        en     intent       4.500000      4.506967   \n",
       "36          36          0        en     intent       4.666667      4.669845   \n",
       "37          37          0        en     intent       4.833333      4.836232   \n",
       "\n",
       "         end   latency  \n",
       "0   0.084730  0.083161  \n",
       "1   0.206862  0.081699  \n",
       "2   0.537558  0.287317  \n",
       "3   0.905310  0.529080  \n",
       "4   0.822487  0.318303  \n",
       "5   1.107790  0.480135  \n",
       "6   1.386082  0.634307  \n",
       "7   0.965719  0.086654  \n",
       "8   1.090324  0.086289  \n",
       "9   1.945572  0.817705  \n",
       "10  1.946135  0.695487  \n",
       "11  2.501325  1.120918  \n",
       "12  1.587009  0.085215  \n",
       "13  1.714802  0.088902  \n",
       "14  2.275705  0.525317  \n",
       "15  2.501481  0.622253  \n",
       "16  3.045272  1.042649  \n",
       "17  3.045860  0.930866  \n",
       "18  3.592544  1.368657  \n",
       "19  3.593138  1.254570  \n",
       "20  2.533278  0.085297  \n",
       "21  3.877389  1.318464  \n",
       "22  6.128211  3.457919  \n",
       "23  6.128913  3.349206  \n",
       "24  6.129573  3.237739  \n",
       "25  6.130708  3.130055  \n",
       "26  6.130505  2.985734  \n",
       "27  6.131278  2.842611  \n",
       "28  6.132442  2.699608  \n",
       "29  6.132652  2.560281  \n",
       "30  3.808971  0.089737  \n",
       "31  6.672758  2.813742  \n",
       "32  4.087514  0.086383  \n",
       "33  6.673302  2.502132  \n",
       "34  7.765454  3.428599  \n",
       "35  7.766037  3.259070  \n",
       "36  7.767299  3.097454  \n",
       "37  7.767128  2.930896  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_benchmark(model_callback: Callable, requests_per_sec: float, \n",
    "                  num_sec: int, \n",
    "                  model_id_to_params: List[Tuple[str, str]],\n",
    "                  model_lambda: float = 0.3,\n",
    "                  seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A simple benchmark in Python.\n",
    "\n",
    "    Sends a stream of requests to multiple models, with the rate varying\n",
    "    according to a Poisson distribution and division of traffic among models\n",
    "    following a truncated Poisson distribution.\n",
    "\n",
    "    :param model_callback: Thread-safe callback function that makes a \n",
    "     single request and returns elapsed time. Should have the signature\n",
    "     `f(model_type: str, language: str)`\n",
    "    :param request_per_sec: Mean of the Poisson distribution that determines\n",
    "     the number of requests in each 1-second window.\n",
    "    :param num_sec: Seconds of traffic to generate at the requested rate.\n",
    "     The actual session will extend past this window until all open requests\n",
    "     have finished.\n",
    "    :param model_lambda: Primary parameter of the truncated Poisson\n",
    "     distribution used to split requests among models. Approximately\n",
    "     equal to the mean of the distribution. The default value of 0.3 sends\n",
    "     70% of traffic to model 0.\n",
    "    :param model_id_to_params: List that maps integer model ID to a tuple of \n",
    "     (language code, model name) for each of the models.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: DataFrame of benchmark results at per-request granularity\n",
    "    \"\"\"\n",
    "    # Preallocate the trace as a set of lists.\n",
    "    benchmark_start_time = time.time()\n",
    "    desired_start_times = (\n",
    "        gen_start_times(requests_per_sec, num_sec, seed)\n",
    "        + benchmark_start_time)\n",
    "    num_requests = desired_start_times.shape[0]\n",
    "    model_nums = gen_model_ids(model_lambda, len(model_id_to_params),\n",
    "                               num_requests, seed)\n",
    "    language_codes = [model_id_to_params[num][0] for num in model_nums]\n",
    "    model_types = [model_id_to_params[num][1] for num in model_nums]\n",
    "    actual_start_times = [None] * num_requests\n",
    "    end_times = [None] * num_requests\n",
    "\n",
    "    # Because some notebook servers (i.e. VSCode) don't play well with\n",
    "    # asyncio, we use threads to manage concurrent requests.\n",
    "    thread_pool = concurrent.futures.ThreadPoolExecutor(1000)\n",
    "\n",
    "    # Map from request object to request number\n",
    "    active_requests = {}  # type: Dict[concurrent.futures.Future, int]\n",
    "\n",
    "    # Main event loop: Spawn background requests, get their responses.\n",
    "    request_num = 0\n",
    "    while request_num < num_requests or len(active_requests) > 0:\n",
    "        sec_to_next = (\n",
    "            1.0 if request_num >= num_requests\n",
    "            else desired_start_times[request_num] - time.time()\n",
    "        )\n",
    "        if sec_to_next <= 0:\n",
    "            # Time to send the next request\n",
    "            lang_code = language_codes[request_num]\n",
    "            model_type = model_types[request_num]\n",
    "            future = thread_pool.submit(\n",
    "                model_callback, model_type, lang_code)\n",
    "            active_requests[future] = request_num\n",
    "            request_num += 1\n",
    "        else:\n",
    "            # Block until it's time to send the next request or a previous\n",
    "            # request is done.\n",
    "            ready_set, _ = concurrent.futures.wait(\n",
    "                list(active_requests.keys()),\n",
    "                timeout=sec_to_next)\n",
    "\n",
    "            # Record timings from any open requests that have completed.\n",
    "            for future in ready_set:\n",
    "                request_id = active_requests.pop(future)\n",
    "                start_time, end_time = future.result()\n",
    "                actual_start_times[request_id] = start_time\n",
    "                end_times[request_id] = end_time\n",
    "\n",
    "    # Collate results as a DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        \"request_id\": range(num_requests),\n",
    "        \"model_num\": model_nums,\n",
    "        \"lang_code\": language_codes,\n",
    "        \"model_type\": model_types,\n",
    "        \"desired_start\": desired_start_times,\n",
    "        \"actual_start\": actual_start_times,\n",
    "        \"end\": end_times\n",
    "    })\n",
    "\n",
    "    # Make all times relative to start of the trace\n",
    "    for key in (\"desired_start\", \"actual_start\", \"end\"):\n",
    "        result[key] -= benchmark_start_time\n",
    "    result[\"latency\"] = result[\"end\"] - result[\"actual_start\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Quick test run\n",
    "run_benchmark(call_model, 6, 5, MODEL_ID_TO_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5047d",
   "metadata": {},
   "source": [
    "Let's run the benchmark with our baseline model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccb1d361-41b6-481c-a770-6f802d1b782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at 2 requests/sec.\n",
      "Running at 3 requests/sec.\n",
      "Running at 4 requests/sec.\n",
      "Running at 5 requests/sec.\n",
      "Running at 6 requests/sec.\n",
      "Running at 8 requests/sec.\n",
      "Running at 10 requests/sec.\n",
      "Running at 12 requests/sec.\n",
      "Running at 14 requests/sec.\n"
     ]
    }
   ],
   "source": [
    "# Run the benchmark at multiple different request rates\n",
    "REQUEST_RATES = (2, 3, 4, 5, 6,\n",
    "                 8, 10, 12, 14\n",
    "                )\n",
    "RUNNING_TIME_SEC = 60\n",
    "to_concat = []\n",
    "for request_rate in REQUEST_RATES:\n",
    "    print(f\"Running at {request_rate} requests/sec.\")\n",
    "    times = run_benchmark(call_model, request_rate, RUNNING_TIME_SEC,\n",
    "                          MODEL_ID_TO_PARAMS)\n",
    "    times.insert(0, \"request_rate\", request_rate)\n",
    "    to_concat.append(times)\n",
    "\n",
    "results = pd.concat(to_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e96396d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_rate</th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>model_type</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.082156</td>\n",
       "      <td>0.080399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125153</td>\n",
       "      <td>0.205616</td>\n",
       "      <td>0.080463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250165</td>\n",
       "      <td>0.532721</td>\n",
       "      <td>0.282556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.381416</td>\n",
       "      <td>0.910457</td>\n",
       "      <td>0.529041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.509224</td>\n",
       "      <td>0.814528</td>\n",
       "      <td>0.305304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>6</td>\n",
       "      <td>358</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.007938</td>\n",
       "      <td>72.753154</td>\n",
       "      <td>13.745216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>6</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>59.200</td>\n",
       "      <td>59.209708</td>\n",
       "      <td>72.753283</td>\n",
       "      <td>13.543575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>6</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>59.400</td>\n",
       "      <td>59.405045</td>\n",
       "      <td>73.001094</td>\n",
       "      <td>13.596049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>6</td>\n",
       "      <td>361</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>59.600</td>\n",
       "      <td>59.608114</td>\n",
       "      <td>73.544107</td>\n",
       "      <td>13.935993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>6</td>\n",
       "      <td>362</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>59.800</td>\n",
       "      <td>59.802969</td>\n",
       "      <td>73.544683</td>\n",
       "      <td>13.741714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     request_rate  request_id  model_num lang_code model_type  desired_start  \\\n",
       "0               6           0          1        en  sentiment          0.000   \n",
       "1               6           1          1        en  sentiment          0.125   \n",
       "2               6           2          0        en     intent          0.250   \n",
       "3               6           3          2        en         qa          0.375   \n",
       "4               6           4          0        en     intent          0.500   \n",
       "..            ...         ...        ...       ...        ...            ...   \n",
       "358             6         358          0        en     intent         59.000   \n",
       "359             6         359          0        en     intent         59.200   \n",
       "360             6         360          0        en     intent         59.400   \n",
       "361             6         361          0        en     intent         59.600   \n",
       "362             6         362          0        en     intent         59.800   \n",
       "\n",
       "     actual_start        end    latency  \n",
       "0        0.001757   0.082156   0.080399  \n",
       "1        0.125153   0.205616   0.080463  \n",
       "2        0.250165   0.532721   0.282556  \n",
       "3        0.381416   0.910457   0.529041  \n",
       "4        0.509224   0.814528   0.305304  \n",
       "..            ...        ...        ...  \n",
       "358     59.007938  72.753154  13.745216  \n",
       "359     59.209708  72.753283  13.543575  \n",
       "360     59.405045  73.001094  13.596049  \n",
       "361     59.608114  73.544107  13.935993  \n",
       "362     59.802969  73.544683  13.741714  \n",
       "\n",
       "[363 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results[\"request_rate\"] == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41097a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">latency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request_rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.274231</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>0.524970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.301546</td>\n",
       "      <td>0.283806</td>\n",
       "      <td>1.087152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.430170</td>\n",
       "      <td>0.362979</td>\n",
       "      <td>1.321989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.346275</td>\n",
       "      <td>2.663348</td>\n",
       "      <td>6.004576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.528451</td>\n",
       "      <td>8.249494</td>\n",
       "      <td>24.716391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51.185530</td>\n",
       "      <td>54.802123</td>\n",
       "      <td>124.972180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>538.575115</td>\n",
       "      <td>191.506339</td>\n",
       "      <td>1312.499908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>513.832799</td>\n",
       "      <td>420.791894</td>\n",
       "      <td>1070.662853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>877.295472</td>\n",
       "      <td>738.462829</td>\n",
       "      <td>1856.460086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latency                         \n",
       "                    mean      median          max\n",
       "request_rate                                     \n",
       "2               0.274231    0.281500     0.524970\n",
       "3               0.301546    0.283806     1.087152\n",
       "4               0.430170    0.362979     1.321989\n",
       "5               2.346275    2.663348     6.004576\n",
       "6               8.528451    8.249494    24.716391\n",
       "8              51.185530   54.802123   124.972180\n",
       "10            538.575115  191.506339  1312.499908\n",
       "12            513.832799  420.791894  1070.662853\n",
       "14            877.295472  738.462829  1856.460086"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_results = results.groupby(\"request_rate\").aggregate({\"latency\": [\"mean\", \"median\", \"max\"]})\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9496843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average Latency (sec)')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(agg_results.index, agg_results[\"latency\", \"mean\"])\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Average Latency (sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d457420",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optimized implementation\n",
    "\n",
    "Now let's redo this baseline using zero-copy model loading.\n",
    "First we'll need to convert the models into a format that can be loaded without copying\n",
    "data. As we saw earlier, each model is actually a pipeline of multiple operations. However, the transformer-based model\n",
    "at the center of this pipeline is orders of magnitude larger and more CPU-intensive than everything else, so we'll only apply zero-copy loading to that part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7add2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,767\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'en_intent'. component=serve deployment=en_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,774\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'en_sentiment'. component=serve deployment=en_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,778\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'en_qa'. component=serve deployment=en_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,781\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'en_generate'. component=serve deployment=en_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,784\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'es_intent'. component=serve deployment=es_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,787\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'es_sentiment'. component=serve deployment=es_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,791\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'es_qa'. component=serve deployment=es_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,794\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'es_generate'. component=serve deployment=es_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,797\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'zh_intent'. component=serve deployment=zh_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,801\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'zh_sentiment'. component=serve deployment=zh_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,804\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'zh_qa'. component=serve deployment=zh_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=66803)\u001b[0m 2022-02-09 18:18:52,808\tINFO deployment_state.py:932 -- Removing 1 replicas from deployment 'zh_generate'. component=serve deployment=zh_generate\n",
      "2022-02-09 18:19:00,522\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:19:04,787\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:19:04,797\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:dCkaGy:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-02-09 18:19:05,517\tINFO api.py:463 -- Started Serve instance in namespace 'dd9a116c-1a8d-488b-92fd-b9ebda41267d'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=68076)\u001b[0m INFO:     Started server process [68076]\n"
     ]
    }
   ],
   "source": [
    "# Restart Ray to prevent the actors from the baseline run from\n",
    "# reserving CPUs.\n",
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()\n",
    "\n",
    "# Wait a moment to make sure that all log output goes to this cell\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6565396",
   "metadata": {},
   "source": [
    "### Introducing `zerocopy`\n",
    "\n",
    "We've created a Python package, `zerocopy`, with the model rewrite code from our previous post (TODO: Publish the package to PyPI).\n",
    "\n",
    "To use that package, you'll need to install it with `pip`, then import it into your script.\n",
    "\n",
    "```python\n",
    "import zerocopy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c219633-6ab1-4f6e-b02f-4a3a9a55ec40",
   "metadata": {},
   "source": [
    "(TODO: Insert description of how `zero_copy` strips off the weights of a model\n",
    "and provides a way to reconsitute them from Plasma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a6334-6837-4c2e-825c-68d87adcaf3a",
   "metadata": {},
   "source": [
    "### Applying zero-copy model loading to our models\n",
    "\n",
    "The low-level process works for all of our example models. Here it is in action with the question answering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e7941cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 3.7534, -8.7741, -9.1023, -6.6326, -8.7627, -8.6893, -9.7706, -9.6873,\n",
       "         -9.6512, -4.1902, -7.6596, -8.5491, -7.9478, -7.5671, -9.0784, -8.2897,\n",
       "         -7.5410, -5.9760, -7.2943, -6.4249, -9.3297, -8.9922, -7.9955, -8.9485,\n",
       "         -8.7506, -6.1065, -8.2273, -8.4471, -8.9334, -2.9227, -2.8201, -4.6316,\n",
       "         -5.5576, -5.7634, -5.8722, -5.9716, -6.2091, -6.0458, -5.3727, -9.8484,\n",
       "         -8.4069, -7.6116, -5.1946, -8.7245, -4.3297, -8.3712, -8.5934, -4.6927,\n",
       "         -8.6297, -5.4113, -8.5218, -6.5587, -6.0347, -9.0888, -3.9168, -3.7715,\n",
       "         -4.0178, -5.5528, -6.2156, -6.1800, -6.1387, -6.2229, -6.4473, -6.3201,\n",
       "         -6.2259, -8.2617, -8.0752, -8.5442, -7.0337, -5.3172, -7.1967, -8.6134,\n",
       "         -5.5957, -9.0524, -8.1817, -7.5553, -7.3491, -8.5218, -7.5425, -7.8877,\n",
       "         -8.3360, -8.1147, -5.0947, -5.1871, -6.5926, -6.7610, -6.7855, -6.8672,\n",
       "         -6.9245, -6.7498, -7.7118, -8.6336, -8.0724, -7.3320, -7.0676, -8.8112,\n",
       "         -4.9942, -5.8767, -9.1029, -7.2982, -6.3021, -9.7423, -7.1145, -7.1081,\n",
       "         -6.9473, -8.5740, -3.0519, -6.5594, -8.5219, -4.1875, -3.3123, -4.7947,\n",
       "         -6.0620, -6.2542, -6.3127, -6.3890, -6.4033, -5.8774, -8.3484, -8.4814,\n",
       "         -9.0725, -8.4167, -9.2634, -8.2589, -7.3816, -8.6027, -5.9291, -2.9634,\n",
       "         -7.5559, -5.7752, -5.7358, -2.7408, -9.7851, -8.8763, -8.4199, -9.4240,\n",
       "         -9.2364, -6.2053, -3.3876, -7.0878, -5.4361, -4.9418, -4.3873, -6.6898,\n",
       "         -6.8614, -6.9100, -6.7906, -6.6952, -6.9213, -7.2896, -7.4180, -5.0012,\n",
       "         -7.7606, -4.3374, -9.5112, -3.4501, -3.6575, -5.4360, -5.9422, -6.0169,\n",
       "         -6.0962, -6.1794, -6.1066, -5.3584, -9.5739]],\n",
       "       grad_fn=<CloneBackward0>), end_logits=tensor([[ 4.4085, -9.0466, -8.8865, -8.7139, -8.4380, -6.1985, -8.0720, -8.6342,\n",
       "         -8.5613, -8.2167, -3.1857, -8.5013, -9.1181, -9.5205, -8.6247, -9.3529,\n",
       "         -9.5464, -7.6825, -6.5827, -5.3342, -7.0454, -8.8780, -9.4053, -8.9716,\n",
       "         -9.1811, -5.5211, -8.8220, -7.2581, -8.8071, -3.8688, -2.2351, -4.3839,\n",
       "         -4.6088, -4.6001, -4.5076, -4.4578, -4.5339, -4.5748, -3.9645, -7.4804,\n",
       "         -9.4568, -9.2952, -5.0298, -9.1183, -8.2703, -6.1834, -8.8689, -4.8877,\n",
       "         -9.0013, -3.7470, -5.0784, -9.5459, -7.1008, -8.6576, -4.8155, -4.2921,\n",
       "         -2.7303, -4.5583, -4.8196, -4.7327, -4.6521, -4.6179, -4.7436, -4.8456,\n",
       "         -8.8704, -5.5502, -9.1086, -9.2351, -9.6451, -7.8936, -5.3242, -8.5684,\n",
       "         -5.8514, -8.7893, -9.1167, -6.6691, -4.8255, -5.0784, -9.5671, -8.4143,\n",
       "         -9.0938, -8.9134, -4.9596, -5.5421, -5.9617, -5.8092, -5.7694, -5.6697,\n",
       "         -5.1522, -4.9800, -7.5636, -6.4838, -9.3509, -9.7418, -8.4976, -9.0644,\n",
       "         -7.2498, -4.1803, -8.6530, -8.6244, -4.0320, -7.6777, -9.3713, -9.6320,\n",
       "         -8.6084, -8.9086, -4.2255, -2.6947, -5.0785, -4.4284, -2.9733, -5.2723,\n",
       "         -5.7548, -5.7846, -5.6469, -5.3465, -4.7512, -4.2537, -9.1724, -8.5388,\n",
       "         -9.1766, -9.1957, -8.9629, -9.5458, -9.0349, -7.9539, -9.7081, -5.0877,\n",
       "         -7.9361, -6.1761, -8.4385, -1.4253, -4.2468, -8.7981, -9.2489, -8.5449,\n",
       "         -9.0143, -9.6156, -5.2834, -7.8561, -4.6573, -4.8583, -3.0356, -5.0068,\n",
       "         -5.2084, -5.2549, -5.0224, -4.8224, -4.8028, -5.2435, -9.3175, -8.6061,\n",
       "         -8.7362, -2.4815, -5.2484, -3.2335, -4.3040, -5.2170, -5.4810, -5.4640,\n",
       "         -5.3760, -5.0714, -4.5941, -3.6325, -8.8697]],\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the QA model directly\n",
    "qa = transformers.pipeline('question-answering', model=QA_MODEL_NAME)\n",
    "qa_tokens = qa.tokenizer(qa_input[\"question\"], qa_input[\"context\"], \n",
    "                         return_tensors=\"pt\")\n",
    "qa.model(**qa_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6157e447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 3.7534, -8.7741, -9.1023, -6.6326, -8.7627, -8.6893, -9.7706, -9.6873,\n",
       "         -9.6512, -4.1902, -7.6596, -8.5491, -7.9478, -7.5671, -9.0784, -8.2897,\n",
       "         -7.5410, -5.9760, -7.2943, -6.4249, -9.3297, -8.9922, -7.9955, -8.9485,\n",
       "         -8.7506, -6.1065, -8.2273, -8.4471, -8.9334, -2.9227, -2.8201, -4.6316,\n",
       "         -5.5576, -5.7634, -5.8722, -5.9716, -6.2091, -6.0458, -5.3727, -9.8484,\n",
       "         -8.4069, -7.6116, -5.1946, -8.7245, -4.3297, -8.3712, -8.5934, -4.6927,\n",
       "         -8.6297, -5.4113, -8.5218, -6.5587, -6.0347, -9.0888, -3.9168, -3.7715,\n",
       "         -4.0178, -5.5528, -6.2156, -6.1800, -6.1387, -6.2229, -6.4473, -6.3201,\n",
       "         -6.2259, -8.2617, -8.0752, -8.5442, -7.0337, -5.3172, -7.1967, -8.6134,\n",
       "         -5.5957, -9.0524, -8.1817, -7.5553, -7.3491, -8.5218, -7.5425, -7.8877,\n",
       "         -8.3360, -8.1147, -5.0947, -5.1871, -6.5926, -6.7610, -6.7855, -6.8672,\n",
       "         -6.9245, -6.7498, -7.7118, -8.6336, -8.0724, -7.3320, -7.0676, -8.8112,\n",
       "         -4.9942, -5.8767, -9.1029, -7.2982, -6.3021, -9.7423, -7.1145, -7.1081,\n",
       "         -6.9473, -8.5740, -3.0519, -6.5594, -8.5219, -4.1875, -3.3123, -4.7947,\n",
       "         -6.0620, -6.2542, -6.3127, -6.3890, -6.4033, -5.8774, -8.3484, -8.4814,\n",
       "         -9.0725, -8.4167, -9.2634, -8.2589, -7.3816, -8.6027, -5.9291, -2.9634,\n",
       "         -7.5559, -5.7752, -5.7358, -2.7408, -9.7851, -8.8763, -8.4199, -9.4240,\n",
       "         -9.2364, -6.2053, -3.3876, -7.0878, -5.4361, -4.9418, -4.3873, -6.6898,\n",
       "         -6.8614, -6.9100, -6.7906, -6.6952, -6.9213, -7.2896, -7.4180, -5.0012,\n",
       "         -7.7606, -4.3374, -9.5112, -3.4501, -3.6575, -5.4360, -5.9422, -6.0169,\n",
       "         -6.0962, -6.1794, -6.1066, -5.3584, -9.5739]]), end_logits=tensor([[ 4.4085, -9.0466, -8.8865, -8.7139, -8.4380, -6.1985, -8.0720, -8.6342,\n",
       "         -8.5613, -8.2167, -3.1857, -8.5013, -9.1181, -9.5205, -8.6247, -9.3529,\n",
       "         -9.5464, -7.6825, -6.5827, -5.3342, -7.0454, -8.8780, -9.4053, -8.9716,\n",
       "         -9.1811, -5.5211, -8.8220, -7.2581, -8.8071, -3.8688, -2.2351, -4.3839,\n",
       "         -4.6088, -4.6001, -4.5076, -4.4578, -4.5339, -4.5748, -3.9645, -7.4804,\n",
       "         -9.4568, -9.2952, -5.0298, -9.1183, -8.2703, -6.1834, -8.8689, -4.8877,\n",
       "         -9.0013, -3.7470, -5.0784, -9.5459, -7.1008, -8.6576, -4.8155, -4.2921,\n",
       "         -2.7303, -4.5583, -4.8196, -4.7327, -4.6521, -4.6179, -4.7436, -4.8456,\n",
       "         -8.8704, -5.5502, -9.1086, -9.2351, -9.6451, -7.8936, -5.3242, -8.5684,\n",
       "         -5.8514, -8.7893, -9.1167, -6.6691, -4.8255, -5.0784, -9.5671, -8.4143,\n",
       "         -9.0938, -8.9134, -4.9596, -5.5421, -5.9617, -5.8092, -5.7694, -5.6697,\n",
       "         -5.1522, -4.9800, -7.5636, -6.4838, -9.3509, -9.7418, -8.4976, -9.0644,\n",
       "         -7.2498, -4.1803, -8.6530, -8.6244, -4.0320, -7.6777, -9.3713, -9.6320,\n",
       "         -8.6084, -8.9086, -4.2255, -2.6947, -5.0785, -4.4284, -2.9733, -5.2723,\n",
       "         -5.7548, -5.7846, -5.6469, -5.3465, -4.7512, -4.2537, -9.1724, -8.5388,\n",
       "         -9.1766, -9.1957, -8.9629, -9.5458, -9.0349, -7.9539, -9.7081, -5.0877,\n",
       "         -7.9361, -6.1761, -8.4385, -1.4253, -4.2468, -8.7981, -9.2489, -8.5449,\n",
       "         -9.0143, -9.6156, -5.2834, -7.8561, -4.6573, -4.8583, -3.0356, -5.0068,\n",
       "         -5.2084, -5.2549, -5.0224, -4.8224, -4.8028, -5.2435, -9.3175, -8.6061,\n",
       "         -8.7362, -2.4815, -5.2484, -3.2335, -4.3040, -5.2170, -5.4810, -5.4640,\n",
       "         -5.3760, -5.0714, -4.5941, -3.6325, -8.8697]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the QA model via `call_model`. Results should be the same as \n",
    "# the previous cell.\n",
    "qa_model_ref = ray.put(zerocopy.extract_tensors(qa.model))\n",
    "ray.get(zerocopy.call_model.remote(qa_model_ref, [], qa_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56637744-182c-4c06-95fa-75e5245c4aa5",
   "metadata": {},
   "source": [
    "For convenience, the `zerocopy` library includes a function `rewrite_pipeline` that takes  transforms any models embedded into a model pipeline into Ray tasks that use zero-copy model loading to load weights. If we apply this function to our question answering pipeline, the resulting rewritten pipeline faithfully performs all the pre- and post-processing that the original pipeline performed. However, this rewritten pipeline runs the embedded PyTorch model in remote Ray tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10fd36e6-347b-422c-b558-bea0a0e9c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before rewrite: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      " After rewrite: {'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}\n"
     ]
    }
   ],
   "source": [
    "zero_copy_qa = zerocopy.rewrite_pipeline(qa)\n",
    "print(f\"Before rewrite: {qa(qa_input)}\")\n",
    "print(f\" After rewrite: {zero_copy_qa(qa_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae691c",
   "metadata": {},
   "source": [
    "The time to invoke the rewritten model once is almost the same as running the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca2416e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time to run locally: 533 ms ± 5.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Time to run with zero-copy: 551 ms ± 18.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Compare timings\n",
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit qa(qa_input)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit zero_copy_qa(qa_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf74bd-4e62-4031-965b-0ef9f4b405de",
   "metadata": {},
   "source": [
    "If we run inference multiple times, `zero_copy.call_model()` can send those inference requests to separate Ray tasks that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2b7b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time to run 16 times locally: 8.64 s ± 17.3 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Time to run 16 times with zero-copy: 3.98 s ± 104 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        qa.model(**qa_tokens)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [zerocopy.call_model.remote(zero_copy_qa.model._model_ref, [], qa_tokens) \n",
    "               for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 16\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9de08b8-f01e-4ad3-8d44-7bfd16c61e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to eat'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the same thing with the intent model\n",
    "intent = IntentPipeline()\n",
    "intent(intent_input['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc2820e3-57cb-4933-bd86-80071ec163db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_copy_intent = zerocopy.rewrite_pipeline(intent, ('__call__', 'generate'))\n",
    "zero_copy_intent(intent_input['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36429095-4c64-42bb-af7c-f58b5939a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time to run locally: 299 ms ± 3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Time to run with zero-copy: 306 ms ± 3.69 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit intent(intent_input['context'])\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit zero_copy_intent(intent_input['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17fec81f-f1c0-434e-abb7-2f4aacf5449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 100 times with zero-copy: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(call_model pid=68117)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.5 s ± 87.4 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "       Time to run 100 times locally: 33.9 s ± 107 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "intent_tokens = intent._tokenizer([intent_input['context']], return_tensors='pt')\n",
    "\n",
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        intent._model.generate(\n",
    "            input_ids=intent_tokens['input_ids'], \n",
    "            attention_mask=intent_tokens['attention_mask'],\n",
    "            max_length=128)\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [zerocopy.call_model.remote(\n",
    "        zero_copy_intent._model._model_ref, [], \n",
    "            {\n",
    "                'input_ids': intent_tokens['input_ids'],\n",
    "                'attention_mask': intent_tokens['attention_mask'],\n",
    "                'max_length': 128\n",
    "            },\n",
    "        'generate') \n",
    "            for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 100\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee421f-3d53-40b9-a065-b86f664e339e",
   "metadata": {},
   "source": [
    "As in the baseline implementation, we can wrap the question answering pipeline in a Ray Serve endpoint with just a few lines of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4dd0832-1859-4c24-9cc2-f67584f928ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroCopyQA:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            transformers.pipeline('question-answering', \n",
    "                                  model=QA_MODEL_NAME))\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        model_input = {\n",
    "            'question': request.query_params['question'],\n",
    "            'context': request.query_params['context']\n",
    "        }\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(model_input))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1fe78-e603-424b-8865-5fca6b0c8ed2",
   "metadata": {},
   "source": [
    "We can apply the same transformation to the other three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26273dbb-a49c-4979-b753-bd71516a3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroCopyIntent:\n",
    "    def __init__(self):\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            IntentPipeline(), ('__call__', 'generate'))\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, \n",
    "            lambda: self._pipeline(request.query_params['context']))\n",
    "        return {\n",
    "            \"intent\": result\n",
    "        }\n",
    "\n",
    "\n",
    "class ZeroCopySentiment:\n",
    "    def __init__(self):\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(SentimentPipeline())\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "            self._threadpool, \n",
    "            lambda: self._pipeline(request.query_params['context']))\n",
    "        return result\n",
    "\n",
    "class ZeroCopyGenerate:\n",
    "    def __init__(self):\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(transformers.pipeline(\n",
    "            'text-generation',\n",
    "            model=GENERATE_MODEL_NAME, \n",
    "            pad_token_id=GPT2_TOKEN_PAD_ID),\n",
    "            ('__call__', 'generate'))\n",
    "        self._max_length = 30\n",
    "        self._num_return_sequences = 1\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        model_input = request.query_params['context']\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "            self._threadpool, \n",
    "            lambda: self._pipeline(\n",
    "                model_input,\n",
    "                max_length=self._max_length,\n",
    "                num_return_sequences=self._num_return_sequences))\n",
    "        return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48325449-b620-43d3-8231-a4023572ee27",
   "metadata": {},
   "source": [
    "As before, we can wrap these classes as Ray Serve deployments, one deployment per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "049463e1-33df-494a-8420-b7c0247b872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 18:23:49,101\tINFO api.py:242 -- Updating deployment 'en_intent'. component=serve deployment=en_intent\n",
      "2022-02-09 18:23:49,115\tINFO api.py:242 -- Updating deployment 'en_sentiment'. component=serve deployment=en_sentiment\n",
      "2022-02-09 18:23:49,172\tINFO api.py:242 -- Updating deployment 'en_qa'. component=serve deployment=en_qa\n",
      "2022-02-09 18:23:49,186\tINFO api.py:242 -- Updating deployment 'en_generate'. component=serve deployment=en_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,120\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_intent'. component=serve deployment=en_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,143\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_sentiment'. component=serve deployment=en_sentiment\n",
      "2022-02-09 18:23:49,199\tINFO api.py:242 -- Updating deployment 'es_intent'. component=serve deployment=es_intent\n",
      "2022-02-09 18:23:49,218\tINFO api.py:242 -- Updating deployment 'es_sentiment'. component=serve deployment=es_sentiment\n",
      "2022-02-09 18:23:49,233\tINFO api.py:242 -- Updating deployment 'es_qa'. component=serve deployment=es_qa\n",
      "2022-02-09 18:23:49,250\tINFO api.py:242 -- Updating deployment 'es_generate'. component=serve deployment=es_generate\n",
      "2022-02-09 18:23:49,268\tINFO api.py:242 -- Updating deployment 'zh_intent'. component=serve deployment=zh_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,270\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_qa'. component=serve deployment=en_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,293\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'en_generate'. component=serve deployment=en_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,321\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_intent'. component=serve deployment=es_intent\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,344\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_sentiment'. component=serve deployment=es_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,372\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_qa'. component=serve deployment=es_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,396\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'es_generate'. component=serve deployment=es_generate\n",
      "2022-02-09 18:23:49,482\tINFO api.py:242 -- Updating deployment 'zh_sentiment'. component=serve deployment=zh_sentiment\n",
      "2022-02-09 18:23:49,514\tINFO api.py:242 -- Updating deployment 'zh_qa'. component=serve deployment=zh_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,427\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_intent'. component=serve deployment=zh_intent\n",
      "2022-02-09 18:23:49,550\tINFO api.py:242 -- Updating deployment 'zh_generate'. component=serve deployment=zh_generate\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,570\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_sentiment'. component=serve deployment=zh_sentiment\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,599\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_qa'. component=serve deployment=zh_qa\n",
      "\u001b[2m\u001b[36m(ServeController pid=68077)\u001b[0m 2022-02-09 18:23:49,636\tINFO deployment_state.py:912 -- Adding 1 replicas to deployment 'zh_generate'. component=serve deployment=zh_generate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(en_qa pid=68139)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(en_intent pid=68128)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(es_intent pid=68116)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(zh_intent pid=68072)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "zero_copy_deployments = {}  \n",
    "for lang in LANGUAGES:\n",
    "    zero_copy_deployments[(lang, 'intent')] = (\n",
    "        serve.deployment(ZeroCopyIntent, f'{lang}_intent',\n",
    "                         ray_actor_options={\"num_cpus\": 0.1}))\n",
    "    zero_copy_deployments[(lang, 'sentiment')] = (\n",
    "        serve.deployment(ZeroCopySentiment, f'{lang}_sentiment',\n",
    "                         ray_actor_options={\"num_cpus\": 0.1}))\n",
    "    zero_copy_deployments[(lang, 'qa')] = (\n",
    "        serve.deployment(ZeroCopyQA, f'{lang}_qa',\n",
    "                         ray_actor_options={\"num_cpus\": 0.1}))\n",
    "    zero_copy_deployments[(lang, 'generate')] = (\n",
    "        serve.deployment(ZeroCopyGenerate, f'{lang}_generate',\n",
    "                         ray_actor_options={\"num_cpus\": 0.1}))\n",
    "    \n",
    "for d in zero_copy_deployments.values():\n",
    "    d.deploy(_blocking=False)\n",
    "    \n",
    "# Wait a moment so log output doesn't go to the next cell's output\n",
    "time.sleep(12.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5e731e9-5111-4cbc-980b-2ea187b63239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'to eat'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure all these models are working\n",
    "params = urllib.parse.urlencode(intent_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_intent?{params}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f232f531-4680-4792-93c1-2ff646ec5f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['positive', 0.5419477820396423],\n",
       " ['neutral', 0.38251084089279175],\n",
       " ['negative', 0.07554134726524353]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = urllib.parse.urlencode(sentiment_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_sentiment?{params}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8d6f0-40de-444d-9201-ac210fa0d2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = urllib.parse.urlencode(qa_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_qa?{params}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b24ec-8029-4655-98fd-58f15e2001e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'All your base are going to need to be the most expensive available on the marketplace.\\n\\nIf you like to have a little more flexibility, then'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = urllib.parse.urlencode(generate_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/en_generate?{params}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c04338",
   "metadata": {},
   "source": [
    "### Benchmarking the zero-copy version\n",
    "\n",
    "We've deployed these models to the same URLs, so the benchmark code from before should work without\n",
    "any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922d3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>model_type</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.110922</td>\n",
       "      <td>0.109745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.222170</td>\n",
       "      <td>0.096970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250172</td>\n",
       "      <td>0.587223</td>\n",
       "      <td>0.337051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.379270</td>\n",
       "      <td>1.181423</td>\n",
       "      <td>0.802153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.503959</td>\n",
       "      <td>0.947586</td>\n",
       "      <td>0.443627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.628806</td>\n",
       "      <td>6.416492</td>\n",
       "      <td>5.787686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.754164</td>\n",
       "      <td>6.505053</td>\n",
       "      <td>5.750889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.879503</td>\n",
       "      <td>5.283766</td>\n",
       "      <td>4.404263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.003898</td>\n",
       "      <td>7.087968</td>\n",
       "      <td>6.084070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.143836</td>\n",
       "      <td>6.429772</td>\n",
       "      <td>5.285936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.290993</td>\n",
       "      <td>2.977008</td>\n",
       "      <td>1.686015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.433320</td>\n",
       "      <td>2.137586</td>\n",
       "      <td>0.704266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.574889</td>\n",
       "      <td>7.048429</td>\n",
       "      <td>5.473540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.718735</td>\n",
       "      <td>7.050960</td>\n",
       "      <td>5.332225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>qa</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>1.860369</td>\n",
       "      <td>8.207011</td>\n",
       "      <td>6.346642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.004302</td>\n",
       "      <td>2.814985</td>\n",
       "      <td>0.810683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>2.144284</td>\n",
       "      <td>8.898565</td>\n",
       "      <td>6.754281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>2.289502</td>\n",
       "      <td>8.885400</td>\n",
       "      <td>6.595898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>2.429122</td>\n",
       "      <td>4.205930</td>\n",
       "      <td>1.776808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>2.578348</td>\n",
       "      <td>4.210163</td>\n",
       "      <td>1.631815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>2.716303</td>\n",
       "      <td>7.019650</td>\n",
       "      <td>4.303347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.858146</td>\n",
       "      <td>10.974136</td>\n",
       "      <td>8.115990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.004069</td>\n",
       "      <td>10.973293</td>\n",
       "      <td>7.969224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.504354</td>\n",
       "      <td>5.915458</td>\n",
       "      <td>2.411104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.001140</td>\n",
       "      <td>7.111101</td>\n",
       "      <td>3.109961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.168564</td>\n",
       "      <td>6.893856</td>\n",
       "      <td>2.725292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.337759</td>\n",
       "      <td>7.386896</td>\n",
       "      <td>3.049137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.503343</td>\n",
       "      <td>7.445590</td>\n",
       "      <td>2.942247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.669051</td>\n",
       "      <td>7.974792</td>\n",
       "      <td>3.305741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>4.834285</td>\n",
       "      <td>8.127341</td>\n",
       "      <td>3.293056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.005043</td>\n",
       "      <td>5.205346</td>\n",
       "      <td>0.200303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>5.334976</td>\n",
       "      <td>9.348706</td>\n",
       "      <td>4.013730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>5.670392</td>\n",
       "      <td>7.025441</td>\n",
       "      <td>1.355049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000842</td>\n",
       "      <td>9.374164</td>\n",
       "      <td>3.373322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>6.126382</td>\n",
       "      <td>8.232839</td>\n",
       "      <td>2.106457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>6.254218</td>\n",
       "      <td>9.871961</td>\n",
       "      <td>3.617743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>6.375000</td>\n",
       "      <td>6.378521</td>\n",
       "      <td>9.716076</td>\n",
       "      <td>3.337555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.509296</td>\n",
       "      <td>8.625944</td>\n",
       "      <td>2.116648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>6.625000</td>\n",
       "      <td>6.628660</td>\n",
       "      <td>8.715590</td>\n",
       "      <td>2.086930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>6.753768</td>\n",
       "      <td>7.354486</td>\n",
       "      <td>0.600718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>6.881815</td>\n",
       "      <td>9.238008</td>\n",
       "      <td>2.356193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.003004</td>\n",
       "      <td>7.345962</td>\n",
       "      <td>0.342958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.339917</td>\n",
       "      <td>9.464551</td>\n",
       "      <td>2.124634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>7.669619</td>\n",
       "      <td>10.812951</td>\n",
       "      <td>3.143332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.004306</td>\n",
       "      <td>9.568134</td>\n",
       "      <td>1.563828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>8.142857</td>\n",
       "      <td>8.147907</td>\n",
       "      <td>9.913116</td>\n",
       "      <td>1.765209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>8.287180</td>\n",
       "      <td>10.147072</td>\n",
       "      <td>1.859892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>8.428571</td>\n",
       "      <td>8.430813</td>\n",
       "      <td>10.231648</td>\n",
       "      <td>1.800835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>8.574003</td>\n",
       "      <td>9.495189</td>\n",
       "      <td>0.921186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>8.714286</td>\n",
       "      <td>8.718625</td>\n",
       "      <td>10.590658</td>\n",
       "      <td>1.872033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>8.858674</td>\n",
       "      <td>10.766737</td>\n",
       "      <td>1.908063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.001923</td>\n",
       "      <td>10.895901</td>\n",
       "      <td>1.893978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>9.125000</td>\n",
       "      <td>9.130658</td>\n",
       "      <td>10.830520</td>\n",
       "      <td>1.699862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>9.254448</td>\n",
       "      <td>10.840318</td>\n",
       "      <td>1.585870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>9.375000</td>\n",
       "      <td>9.377046</td>\n",
       "      <td>10.921635</td>\n",
       "      <td>1.544589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>9.504839</td>\n",
       "      <td>11.041942</td>\n",
       "      <td>1.537103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>9.625000</td>\n",
       "      <td>9.626493</td>\n",
       "      <td>10.396832</td>\n",
       "      <td>0.770339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>9.751511</td>\n",
       "      <td>11.041409</td>\n",
       "      <td>1.289898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>intent</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>9.879775</td>\n",
       "      <td>11.146737</td>\n",
       "      <td>1.266962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    request_id  model_num lang_code model_type  desired_start  actual_start  \\\n",
       "0            0          1        en  sentiment       0.000000      0.001177   \n",
       "1            1          1        en  sentiment       0.125000      0.125200   \n",
       "2            2          0        en     intent       0.250000      0.250172   \n",
       "3            3          2        en         qa       0.375000      0.379270   \n",
       "4            4          0        en     intent       0.500000      0.503959   \n",
       "5            5          0        en     intent       0.625000      0.628806   \n",
       "6            6          0        en     intent       0.750000      0.754164   \n",
       "7            7          1        en  sentiment       0.875000      0.879503   \n",
       "8            8          1        en  sentiment       1.000000      1.003898   \n",
       "9            9          0        en     intent       1.142857      1.143836   \n",
       "10          10          0        en     intent       1.285714      1.290993   \n",
       "11          11          0        en     intent       1.428571      1.433320   \n",
       "12          12          1        en  sentiment       1.571429      1.574889   \n",
       "13          13          1        en  sentiment       1.714286      1.718735   \n",
       "14          14          2        en         qa       1.857143      1.860369   \n",
       "15          15          0        en     intent       2.000000      2.004302   \n",
       "16          16          0        en     intent       2.142857      2.144284   \n",
       "17          17          0        en     intent       2.285714      2.289502   \n",
       "18          18          0        en     intent       2.428571      2.429122   \n",
       "19          19          0        en     intent       2.571429      2.578348   \n",
       "20          20          1        en  sentiment       2.714286      2.716303   \n",
       "21          21          0        en     intent       2.857143      2.858146   \n",
       "22          22          0        en     intent       3.000000      3.004069   \n",
       "23          23          0        en     intent       3.500000      3.504354   \n",
       "24          24          0        en     intent       4.000000      4.001140   \n",
       "25          25          0        en     intent       4.166667      4.168564   \n",
       "26          26          0        en     intent       4.333333      4.337759   \n",
       "27          27          0        en     intent       4.500000      4.503343   \n",
       "28          28          0        en     intent       4.666667      4.669051   \n",
       "29          29          0        en     intent       4.833333      4.834285   \n",
       "30          30          1        en  sentiment       5.000000      5.005043   \n",
       "31          31          0        en     intent       5.333333      5.334976   \n",
       "32          32          1        en  sentiment       5.666667      5.670392   \n",
       "33          33          0        en     intent       6.000000      6.000842   \n",
       "34          34          0        en     intent       6.125000      6.126382   \n",
       "35          35          0        en     intent       6.250000      6.254218   \n",
       "36          36          0        en     intent       6.375000      6.378521   \n",
       "37          37          0        en     intent       6.500000      6.509296   \n",
       "38          38          0        en     intent       6.625000      6.628660   \n",
       "39          39          1        en  sentiment       6.750000      6.753768   \n",
       "40          40          0        en     intent       6.875000      6.881815   \n",
       "41          41          1        en  sentiment       7.000000      7.003004   \n",
       "42          42          0        en     intent       7.333333      7.339917   \n",
       "43          43          0        en     intent       7.666667      7.669619   \n",
       "44          44          0        en     intent       8.000000      8.004306   \n",
       "45          45          0        en     intent       8.142857      8.147907   \n",
       "46          46          0        en     intent       8.285714      8.287180   \n",
       "47          47          0        en     intent       8.428571      8.430813   \n",
       "48          48          1        en  sentiment       8.571429      8.574003   \n",
       "49          49          0        en     intent       8.714286      8.718625   \n",
       "50          50          0        en     intent       8.857143      8.858674   \n",
       "51          51          0        en     intent       9.000000      9.001923   \n",
       "52          52          0        en     intent       9.125000      9.130658   \n",
       "53          53          0        en     intent       9.250000      9.254448   \n",
       "54          54          0        en     intent       9.375000      9.377046   \n",
       "55          55          0        en     intent       9.500000      9.504839   \n",
       "56          56          1        en  sentiment       9.625000      9.626493   \n",
       "57          57          0        en     intent       9.750000      9.751511   \n",
       "58          58          0        en     intent       9.875000      9.879775   \n",
       "\n",
       "          end   latency  \n",
       "0    0.110922  0.109745  \n",
       "1    0.222170  0.096970  \n",
       "2    0.587223  0.337051  \n",
       "3    1.181423  0.802153  \n",
       "4    0.947586  0.443627  \n",
       "5    6.416492  5.787686  \n",
       "6    6.505053  5.750889  \n",
       "7    5.283766  4.404263  \n",
       "8    7.087968  6.084070  \n",
       "9    6.429772  5.285936  \n",
       "10   2.977008  1.686015  \n",
       "11   2.137586  0.704266  \n",
       "12   7.048429  5.473540  \n",
       "13   7.050960  5.332225  \n",
       "14   8.207011  6.346642  \n",
       "15   2.814985  0.810683  \n",
       "16   8.898565  6.754281  \n",
       "17   8.885400  6.595898  \n",
       "18   4.205930  1.776808  \n",
       "19   4.210163  1.631815  \n",
       "20   7.019650  4.303347  \n",
       "21  10.974136  8.115990  \n",
       "22  10.973293  7.969224  \n",
       "23   5.915458  2.411104  \n",
       "24   7.111101  3.109961  \n",
       "25   6.893856  2.725292  \n",
       "26   7.386896  3.049137  \n",
       "27   7.445590  2.942247  \n",
       "28   7.974792  3.305741  \n",
       "29   8.127341  3.293056  \n",
       "30   5.205346  0.200303  \n",
       "31   9.348706  4.013730  \n",
       "32   7.025441  1.355049  \n",
       "33   9.374164  3.373322  \n",
       "34   8.232839  2.106457  \n",
       "35   9.871961  3.617743  \n",
       "36   9.716076  3.337555  \n",
       "37   8.625944  2.116648  \n",
       "38   8.715590  2.086930  \n",
       "39   7.354486  0.600718  \n",
       "40   9.238008  2.356193  \n",
       "41   7.345962  0.342958  \n",
       "42   9.464551  2.124634  \n",
       "43  10.812951  3.143332  \n",
       "44   9.568134  1.563828  \n",
       "45   9.913116  1.765209  \n",
       "46  10.147072  1.859892  \n",
       "47  10.231648  1.800835  \n",
       "48   9.495189  0.921186  \n",
       "49  10.590658  1.872033  \n",
       "50  10.766737  1.908063  \n",
       "51  10.895901  1.893978  \n",
       "52  10.830520  1.699862  \n",
       "53  10.840318  1.585870  \n",
       "54  10.921635  1.544589  \n",
       "55  11.041942  1.537103  \n",
       "56  10.396832  0.770339  \n",
       "57  11.041409  1.289898  \n",
       "58  11.146737  1.266962  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick test run\n",
    "run_benchmark(call_model, 5, 10, MODEL_ID_TO_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at 2 requests/sec.\n",
      "Running at 3 requests/sec.\n",
      "Running at 4 requests/sec.\n",
      "Running at 5 requests/sec.\n",
      "Running at 6 requests/sec.\n",
      "\u001b[2m\u001b[36m(call_model pid=68614)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(call_model pid=68662)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at 8 requests/sec.\n",
      "Running at 10 requests/sec.\n",
      "Running at 12 requests/sec.\n",
      "Running at 14 requests/sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(call_model pid=68858)\u001b[0m \n",
      "\u001b[2m\u001b[36m(call_model pid=68893)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(call_model pid=68939)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "# Run the benchmark at multiple different request rates\n",
    "to_concat = []\n",
    "for request_rate in REQUEST_RATES:\n",
    "    print(f\"Running at {request_rate} requests/sec.\")\n",
    "    times = run_benchmark(call_model, request_rate, RUNNING_TIME_SEC,\n",
    "                          MODEL_ID_TO_PARAMS)\n",
    "    times.insert(0, \"request_rate\", request_rate)\n",
    "    to_concat.append(times)\n",
    "\n",
    "results_zerocopy = pd.concat(to_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f129d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">latency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request_rate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.661407</td>\n",
       "      <td>0.332965</td>\n",
       "      <td>3.544607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.557090</td>\n",
       "      <td>0.333867</td>\n",
       "      <td>4.106008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.074884</td>\n",
       "      <td>0.891919</td>\n",
       "      <td>10.622166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.587614</td>\n",
       "      <td>0.905039</td>\n",
       "      <td>6.720857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.896304</td>\n",
       "      <td>1.306567</td>\n",
       "      <td>8.264216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.876169</td>\n",
       "      <td>2.832798</td>\n",
       "      <td>9.643580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.691245</td>\n",
       "      <td>2.840891</td>\n",
       "      <td>9.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.184824</td>\n",
       "      <td>9.026742</td>\n",
       "      <td>18.863758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>208.546370</td>\n",
       "      <td>106.358101</td>\n",
       "      <td>606.104136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latency                        \n",
       "                    mean      median         max\n",
       "request_rate                                    \n",
       "2               0.661407    0.332965    3.544607\n",
       "3               0.557090    0.333867    4.106008\n",
       "4               2.074884    0.891919   10.622166\n",
       "5               1.587614    0.905039    6.720857\n",
       "6               1.896304    1.306567    8.264216\n",
       "8               2.876169    2.832798    9.643580\n",
       "10              2.691245    2.840891    9.052174\n",
       "12              8.184824    9.026742   18.863758\n",
       "14            208.546370  106.358101  606.104136"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_results_zerocopy = results_zerocopy.groupby(\"request_rate\").aggregate({\n",
    "    \"latency\": [\"mean\", \"median\", \"max\"]})\n",
    "agg_results_zerocopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a02cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff5a0315c70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the two sets of results against each other.\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(agg_results.index, agg_results[\"latency\", \"mean\"],\n",
    "         \"-o\", label=\"Without zero-copy loading\")\n",
    "plt.plot(agg_results_zerocopy.index, \n",
    "         agg_results_zerocopy[\"latency\", \"mean\"],\n",
    "         \"-o\", label=\"With zero-copy loading\")\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Average Request Latency (sec)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac27d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code defined an actor\n",
    "\n",
    "# class ModelCallback:\n",
    "#     def __init__(self, model_ref: ray.ObjectRef):\n",
    "#         self._model_ref = model_ref\n",
    "\n",
    "#     def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "#         return ray.get(call_model.remote(self._model_ref, args, kwargs))\n",
    "\n",
    "# @ray.remote\n",
    "# class QAModelZeroCopyActor:\n",
    "#     def __init__(self):\n",
    "#         self._qa = transformers.pipeline(\"question-answering\", model=model_name)\n",
    "#         self._model_ref = ray.put(zerocopy.extract_tensors(self._qa.model))\n",
    "#         self._qa.model = ModelCallback(self._model_ref)\n",
    "\n",
    "#     def run_inference(self, input_: Dict[str, str]) -> Dict[str, Any]:\n",
    "#         return self._qa(input_)\n",
    "\n",
    "# zero_copy_actors = [QAModelZeroCopyActor.options(max_concurrency=8).remote() \n",
    "#                     for _ in range(NUM_QA_MODELS)]\n",
    "# ray.get(zero_copy_actors[0].run_inference.remote(qa_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b57958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version of code for zero-copy QA model\n",
    "\n",
    "# class ZeroCopyQAModel:\n",
    "#     def __init__(self):\n",
    "#         # TODO: Move this rewrite to the `zerocopy` library.\n",
    "#         # Load the entire pipeline, then copy the model portion to Plasma.\n",
    "#         self._qa = transformers.pipeline(\"question-answering\", model=model_name)\n",
    "#         model_ref = ray.put(zerocopy.extract_tensors(self._qa.model))\n",
    "\n",
    "#         # Replace the pipeline's model with a callback that farms out work to\n",
    "#         # Ray tasks.\n",
    "#         class _ModelCallback:\n",
    "#             def __call__(self, *args, **kwargs):\n",
    "#                 return ray.get(call_model_zero_copy.remote(model_ref, args, kwargs))\n",
    "#         self._qa.model = _ModelCallback()\n",
    "\n",
    "#         # Use a threadpool because the model is called from pre/postprocessing code\n",
    "#         # that is not asyncio-aware\n",
    "#         self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "#     async def __call__(self, request: starlette.requests.Request):\n",
    "#         # Pull model inputs from URL query parameters.\n",
    "#         # A production version of this code would sanitize these strings.\n",
    "#         model_input = {\n",
    "#             \"question\": request.query_params[\"question\"],\n",
    "#             \"context\": request.query_params[\"context\"]\n",
    "#         }\n",
    "#         result = await asyncio.get_running_loop().run_in_executor(\n",
    "#             self._threadpool, lambda: self._qa(model_input))\n",
    "#         return result\n",
    "\n",
    "#     def __del__(self):  # Ray Serve needs this callback\n",
    "#         pass\n",
    "\n",
    "\n",
    "# # Define endpoints\n",
    "# NUM_QA_MODELS = 12\n",
    "# deployments = [\n",
    "#     serve.deployment(ZeroCopyQAModel, f\"qa{model_num}\",\n",
    "#                      ray_actor_options={\"num_cpus\": 0.1})\n",
    "#     for model_num in range(NUM_QA_MODELS)\n",
    "# ]\n",
    "\n",
    "# for d in deployments:\n",
    "#     d.deploy(_blocking=False)\n",
    "\n",
    "# # Wait a moment so log output doesn't go to the next cell's output\n",
    "# time.sleep(1.)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afa7e0f34d224467fd24b0cfa9c212efa127bdf53fe1c4e3ddf54198f34a39e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
