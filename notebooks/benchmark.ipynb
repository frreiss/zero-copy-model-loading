{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8270a16d",
   "metadata": {},
   "source": [
    "# benchmark.ipynb\n",
    "\n",
    "This notebook contains the text and code for the next blog post in the zero-copy model series, \n",
    "title TBD.\n",
    "\n",
    "The first post explained how to load PyTorch models for inference extremely fast by leveraging the Plasma object store's ability to load numeric data directly from shared memory.\n",
    "\n",
    "In this post, we talk in more concrete terms about how to use this zero-copy model loading for model serving. We put together a simple model serving system, then set up a microbenchmark that simulates a heavy-tailed traffic pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbf7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import urllib\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import zerocopy\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69fefd",
   "metadata": {},
   "source": [
    "# Title of new blog post goes here\n",
    "\n",
    "*Recap of previous blog post goes here.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13eccd9",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "The end-to-end scenario for our benchmark involves supporting an AI chatbot.\n",
    "The chatbot's conversational AI runs off of a conversation tree (**TODO:** What's the best term for this tree?). Some of the nodes of this tree invoke question answering models.\n",
    "\n",
    "Our benchmark will cover the model serving portion of the chatbot's backend. This \n",
    "model serving layer runs question answering (QA) models on behalf of the \n",
    "chatbot's conversational AI. The chatbot's conversation tree leads to 4 very different\n",
    "question answering scenarios, and each scenario has its own dedicated QA\n",
    "model. Because the chatbot speaks 3 different languages, there are three versions of\n",
    "each model deployed: one for each language. So the model serving layer runs a total of\n",
    "12 models to cover the 4 question types and 3 languages.\n",
    "\n",
    "> **TODO:** Cartoon block diagram of the end-to-end scenario. \n",
    "> Diagram should show a user interacting with a chatbot. The chatbot runs off of a conversation tree. \n",
    "> Some of the nodes of the conversation tree have question answering models hanging off of them.\n",
    "\n",
    "For our question answering models, we'll use 12 copies of `deepset/roberta-base-squad2`,\n",
    "the most popular question answering model on the [Huggingface model marketplace](https://huggingface.co/models).\n",
    "Here's some code to load that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37dae247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load with standard method: 5.94 s ± 310 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# Strip out this timing code for the blog version.\n",
    "print(\"Time to load with standard method: \", end=\"\")\n",
    "%timeit -r3 transformers.pipeline(\"question-answering\", model=model_name)\n",
    "qa = transformers.pipeline(\"question-answering\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cdcae",
   "metadata": {},
   "source": [
    "The performance of this model isn't very sensitive to the specific question and context provided,\n",
    "so we define a single set of inputs and outputs for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa96aec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278851065464551e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_input = {\n",
    "    \"question\": \"What is 1 + 1?\",\n",
    "    \"context\": \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "result = qa(qa_input)\n",
    "qa_answer = result[\"answer\"]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494b1aa",
   "metadata": {},
   "source": [
    "## Baseline results\n",
    "\n",
    "Let's start with a baseline implementation of model serving for this model. This baseline implementation emulates running each QA model in a separate container. The server has 12 CPUs, so each container gets 1 CPU. We implement this baseline configuration with a pool of Ray actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec60e717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,484\tINFO checkpoint_path.py:15 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,488\tINFO http_state.py:75 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:OlGhmW:SERVE_PROXY_ACTOR-node:192.168.0.238-0' on node 'node:192.168.0.238-0' listening on '127.0.0.1:8000'\n",
      "2021-10-21 15:04:43,731\tINFO api.py:455 -- Started Serve instance in namespace 'fff95715-1ff4-4e14-911c-a881e348da3b'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fb6a87277f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8a6ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 15:04:43,748\tINFO api.py:243 -- Updating deployment 'qa0'. component=serve deployment=qa0\n",
      "2021-10-21 15:04:43,756\tINFO api.py:243 -- Updating deployment 'qa1'. component=serve deployment=qa1\n",
      "2021-10-21 15:04:43,767\tINFO api.py:243 -- Updating deployment 'qa2'. component=serve deployment=qa2\n",
      "2021-10-21 15:04:43,781\tINFO api.py:243 -- Updating deployment 'qa3'. component=serve deployment=qa3\n",
      "2021-10-21 15:04:43,797\tINFO api.py:243 -- Updating deployment 'qa4'. component=serve deployment=qa4\n",
      "2021-10-21 15:04:43,816\tINFO api.py:243 -- Updating deployment 'qa5'. component=serve deployment=qa5\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,818\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa0'. component=serve deployment=qa0\n",
      "\u001b[2m\u001b[36m(pid=9013)\u001b[0m INFO:     Started server process [9013]\n",
      "2021-10-21 15:04:43,907\tINFO api.py:243 -- Updating deployment 'qa6'. component=serve deployment=qa6\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,829\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa1'. component=serve deployment=qa1\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,839\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa2'. component=serve deployment=qa2\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,849\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa3'. component=serve deployment=qa3\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,860\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa4'. component=serve deployment=qa4\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,870\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa5'. component=serve deployment=qa5\n",
      "2021-10-21 15:04:43,948\tINFO api.py:243 -- Updating deployment 'qa7'. component=serve deployment=qa7\n",
      "2021-10-21 15:04:43,988\tINFO api.py:243 -- Updating deployment 'qa8'. component=serve deployment=qa8\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:43,992\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa6'. component=serve deployment=qa6\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:44,006\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa7'. component=serve deployment=qa7\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:44,021\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa8'. component=serve deployment=qa8\n",
      "2021-10-21 15:04:44,073\tINFO api.py:243 -- Updating deployment 'qa9'. component=serve deployment=qa9\n",
      "2021-10-21 15:04:44,119\tINFO api.py:243 -- Updating deployment 'qa10'. component=serve deployment=qa10\n",
      "2021-10-21 15:04:44,169\tINFO api.py:243 -- Updating deployment 'qa11'. component=serve deployment=qa11\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:44,174\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa9'. component=serve deployment=qa9\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:44,190\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa10'. component=serve deployment=qa10\n",
      "\u001b[2m\u001b[36m(pid=9016)\u001b[0m 2021-10-21 15:04:44,207\tINFO backend_state.py:896 -- Adding 1 replicas to deployment 'qa11'. component=serve deployment=qa11\n"
     ]
    }
   ],
   "source": [
    "class QAModel:\n",
    "    def __init__(self):\n",
    "        self._qa = transformers.pipeline(\"question-answering\",\n",
    "                                         model=model_name)\n",
    "\n",
    "    def __call__(self, request: starlette.requests.Request):\n",
    "        # Pull model inputs from URL query parameters.\n",
    "        # A production version of this code would sanitize these strings.\n",
    "        model_input = {\n",
    "            \"question\": request.query_params[\"question\"],\n",
    "            \"context\": request.query_params[\"context\"]\n",
    "        }\n",
    "        return self._qa(model_input)\n",
    "\n",
    "\n",
    "# Define endpoints\n",
    "NUM_QA_MODELS = 12\n",
    "deployments = [\n",
    "    serve.deployment(QAModel, f\"qa{model_num}\")\n",
    "    for model_num in range(NUM_QA_MODELS)\n",
    "]\n",
    "\n",
    "for d in deployments:\n",
    "    d.deploy(_blocking=False)\n",
    "\n",
    "# Wait a moment so log output doesn't go to the next cell's output\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22fcdd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278851065464551e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out the deployment.\n",
    "# This web service call blocks until the asychronous deployment has completed.\n",
    "params = urllib.parse.urlencode(qa_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/qa0?{params}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea4da8",
   "metadata": {},
   "source": [
    "Let's wrap this model web service in a callback function that calls the model, retrieves the result, and\n",
    "returns elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079ad698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.381 seconds elapsed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_model(model_num: int, question: str, context: str, expected_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Callack function that calls the model deployment, retrieves and \n",
    "    validates the result, and returns elapsed time.\n",
    "\n",
    "    :param model_num: Index of the model to call\n",
    "    :param question: The `question` argument to pass to the QA model\n",
    "    :param context: The `context` argument to pass to the QA model\n",
    "    :param expected_answer: The answer that the model should return\n",
    "\n",
    "    :returns: Tuple of start and end times of the web service call\n",
    "    \"\"\"\n",
    "    # For now, use the same input every time\n",
    "    params = urllib.parse.urlencode({\"question\": question, \"context\": context})\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = requests.get(f\"http://127.0.0.1:8000/qa{model_num}?{params}\").json()\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Do some basic validation\n",
    "    if result[\"answer\"] != expected_answer:\n",
    "        raise ValueError(f\"Unexpected result: {result}\")\n",
    "\n",
    "    return (start_time, end_time)\n",
    "\n",
    "\n",
    "times = call_model(0, qa_input[\"question\"], qa_input[\"context\"], qa_answer)\n",
    "f\"{times[1] - times[0]:1.3f} seconds elapsed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925870a",
   "metadata": {},
   "source": [
    "Now we can define a simple benchmark.\n",
    "\n",
    "Our benchmark generates a trace of requests, then plays back the trace and measures the \n",
    "latency of each request. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92a1f5-3624-4f22-8a95-0b9d9c2f32d6",
   "metadata": {},
   "source": [
    "The request rate changes each second, with the rate of a particular 1-second window drawn from the Poisson\n",
    "distribution. Here's the code to generate the start times for the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110ace1f-9c9d-4b55-a50b-9c20d2d9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_start_times(requests_per_sec: float, num_sec: int,\n",
    "                    seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a trace of inference request start times. Divides the trace\n",
    "    into 1-second intervals. Each interval gets a number of requests drawn\n",
    "    from a Poissson distribution. These requests are evenly spread through the\n",
    "    interval.\n",
    "\n",
    "    :param requests_per_sec: Average requests per second overall\n",
    "    :param num_sec: Number of seconds of trace to generate\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Numpy array of timestamps (starting from 0) for the requests\n",
    "     in the trace\n",
    "    \"\"\"\n",
    "    trace = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Compute the number of requests in each 1-second window.\n",
    "    req_per_window = rng.poisson(requests_per_sec, size=num_sec)\n",
    "\n",
    "    for window_num in range(num_sec):\n",
    "        num_requests = req_per_window[window_num]\n",
    "        if num_requests > 0:\n",
    "            request_interval = 1.0 / num_requests\n",
    "            for i in range(num_requests):\n",
    "                trace.append(window_num + request_interval * i)\n",
    "\n",
    "    return np.array(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c681f9-e06e-4da8-a6e8-08ae5e54fa85",
   "metadata": {},
   "source": [
    "Each request goes to a randomly-selected model. The choice of models is\n",
    "weighted according to a truncated Poisson distribution. Here's the code to generate\n",
    "the list of model IDs for the requests in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "451f1281-6c46-45e9-b397-5d8898717e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_ids(lambda_: float, num_models: int, num_points: int,\n",
    "                  seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw model IDs at random from a truncated Poisson distribution.\n",
    "\n",
    "    :param lambda_: Primary parameter of the distribution, which also happens to \n",
    "     be the mean value of the (untruncated) distribution.\n",
    "    :num_models: Number of models. This function will truncate the Poisson \n",
    "     distribution such that only values < num_models will be returned.\n",
    "    :param num_points: Number of random model IDs to return.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: Randomly generated model IDs for a series of requests, as a\n",
    "     1-dimensional array.\n",
    "    \"\"\"\n",
    "    # Draw numbers from a truncated Poisson distribution.\n",
    "    # Start with a non-truncated distribution, then resample for\n",
    "    # any values that went over the limit. \n",
    "    rng = np.random.default_rng(seed)\n",
    "    result = rng.poisson(lambda_, size=num_points)\n",
    "    while np.any(result >= num_models):\n",
    "        new_values = rng.poisson(lambda_, size=np.sum(result >= num_models))\n",
    "        result[result >= num_models] = new_values\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61453629-426e-4b09-8430-4b2c11b71a64",
   "metadata": {},
   "source": [
    "The benchmark itself generates and then plays back the trace, measuring the end-to-end latency of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa55ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>model_num</th>\n",
       "      <th>desired_start</th>\n",
       "      <th>actual_start</th>\n",
       "      <th>end</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.400686</td>\n",
       "      <td>0.399788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.069783</td>\n",
       "      <td>0.791132</td>\n",
       "      <td>0.721349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.134029</td>\n",
       "      <td>0.532247</td>\n",
       "      <td>0.398218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.202648</td>\n",
       "      <td>0.614423</td>\n",
       "      <td>0.411775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.267616</td>\n",
       "      <td>1.302230</td>\n",
       "      <td>1.034614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>4.616411</td>\n",
       "      <td>17.607030</td>\n",
       "      <td>12.990619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4.692308</td>\n",
       "      <td>4.692817</td>\n",
       "      <td>17.607163</td>\n",
       "      <td>12.914346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>4.769231</td>\n",
       "      <td>4.769844</td>\n",
       "      <td>17.607320</td>\n",
       "      <td>12.837476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4.846154</td>\n",
       "      <td>4.851322</td>\n",
       "      <td>17.607443</td>\n",
       "      <td>12.756121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>4.927259</td>\n",
       "      <td>17.607525</td>\n",
       "      <td>12.680266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    request_id  model_num  desired_start  actual_start        end    latency\n",
       "0            0          1       0.000000      0.000898   0.400686   0.399788\n",
       "1            1          1       0.066667      0.069783   0.791132   0.721349\n",
       "2            2          0       0.133333      0.134029   0.532247   0.398218\n",
       "3            3          2       0.200000      0.202648   0.614423   0.411775\n",
       "4            4          0       0.266667      0.267616   1.302230   1.034614\n",
       "..         ...        ...            ...           ...        ...        ...\n",
       "57          57          0       4.615385      4.616411  17.607030  12.990619\n",
       "58          58          0       4.692308      4.692817  17.607163  12.914346\n",
       "59          59          0       4.769231      4.769844  17.607320  12.837476\n",
       "60          60          0       4.846154      4.851322  17.607443  12.756121\n",
       "61          61          0       4.923077      4.927259  17.607525  12.680266\n",
       "\n",
       "[62 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_benchmark(model_callback: Callable, requests_per_sec: float, \n",
    "                  num_sec: int, model_lambda: float = 0.3,\n",
    "                  seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A simple benchmark in Python.\n",
    "\n",
    "    Sends a stream of requests to multiple models, with the rate varying\n",
    "    according to a Poisson distribution and division of traffic among models\n",
    "    following a truncated Poisson distribution.\n",
    "\n",
    "    :param model_callback: Thread-safe callback function that makes a \n",
    "     single request and returns elapsed time. Should have the signature\n",
    "     `f(model_num: int, question: str, context: str, expected_answer: str)`\n",
    "    :param request_per_sec: Mean of the Poisson distribution that determines\n",
    "     the number of requests in each 1-second window.\n",
    "    :param num_sec: Seconds of traffic to generate at the requested rate.\n",
    "     The actual session will extend past this window until all open requests\n",
    "     have finished.\n",
    "    :param model_lambda: Primary parameter of the truncated Poisson\n",
    "     distribution used to split requests among models. Approximately \n",
    "     equal to the mean of the distribution. The default value of 0.3 sends\n",
    "     70% of traffic to model 0.\n",
    "    :param seed: Seed for the random number generator\n",
    "\n",
    "    :returns: DataFrame of benchmark results at per-request granularity\n",
    "    \"\"\"\n",
    "    # Preallocate the trace as a set of lists.\n",
    "    benchmark_start_time = time.time()\n",
    "    desired_start_times = (\n",
    "        gen_start_times(requests_per_sec, num_sec, seed)\n",
    "        + benchmark_start_time)\n",
    "    num_requests = desired_start_times.shape[0]\n",
    "    model_nums = gen_model_ids(model_lambda, NUM_QA_MODELS, num_requests,\n",
    "                               seed)\n",
    "    actual_start_times = [None] * num_requests\n",
    "    end_times = [None] * num_requests\n",
    "\n",
    "    # Because some notebook servers (i.e. VSCode) don't play well with\n",
    "    # asyncio, we use threads to manage concurrent requests.\n",
    "    thread_pool = concurrent.futures.ThreadPoolExecutor(1000)\n",
    "\n",
    "    # Map from request object to request number\n",
    "    active_requests = {}  # type: Dict[concurrent.futures.Future, int]\n",
    "\n",
    "    # Main event loop: Spawn background requests, get their responses.\n",
    "    request_num = 0\n",
    "    while request_num < num_requests or len(active_requests) > 0:\n",
    "        sec_to_next = (\n",
    "            1.0 if request_num >= num_requests\n",
    "            else desired_start_times[request_num] - time.time()\n",
    "        )\n",
    "        if sec_to_next <= 0:\n",
    "            # Time to send the next request\n",
    "            model_num = model_nums[request_num]\n",
    "            future = thread_pool.submit(\n",
    "                model_callback, model_num,\n",
    "                qa_input[\"question\"], qa_input[\"context\"], qa_answer)\n",
    "            active_requests[future] = request_num\n",
    "            request_num += 1\n",
    "        else:\n",
    "            # Block until it's time to send the next request or a previous\n",
    "            # request is done.\n",
    "            ready_set, _ = concurrent.futures.wait(\n",
    "                list(active_requests.keys()), \n",
    "                timeout=sec_to_next)\n",
    "\n",
    "            # Record timings from any open requests that have completed.\n",
    "            for future in ready_set:\n",
    "                request_id = active_requests.pop(future)\n",
    "                start_time, end_time = future.result()\n",
    "                actual_start_times[request_id] = start_time\n",
    "                end_times[request_id] = end_time\n",
    "\n",
    "    # Collate results as a DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        \"request_id\": range(num_requests),\n",
    "        \"model_num\": model_nums, \n",
    "        \"desired_start\": desired_start_times, \n",
    "        \"actual_start\": actual_start_times, \n",
    "        \"end\": end_times\n",
    "    })\n",
    "\n",
    "    # Make all times relative to start of the trace\n",
    "    for key in (\"desired_start\", \"actual_start\", \"end\"):\n",
    "        result[key] -= benchmark_start_time\n",
    "    result[\"latency\"] = result[\"end\"] - result[\"actual_start\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Quick test run\n",
    "run_benchmark(call_model, 12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5047d",
   "metadata": {},
   "source": [
    "Let's run the benchmark with our baseline model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f53994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running at 2 requests/sec.\n",
      "Running at 3 requests/sec.\n",
      "Running at 4 requests/sec.\n",
      "Running at 5 requests/sec.\n",
      "Running at 6 requests/sec.\n",
      "Running at 8 requests/sec.\n",
      "Running at 10 requests/sec.\n"
     ]
    }
   ],
   "source": [
    "# Run the benchmark at multiple different request rates\n",
    "REQUEST_RATES = (2, 3, 4, 5, 6, 8, 10, 12, 14)\n",
    "RUNNING_TIME_SEC = 60\n",
    "to_concat = []\n",
    "for request_rate in REQUEST_RATES:\n",
    "    print(f\"Running at {request_rate} requests/sec.\")\n",
    "    times = run_benchmark(call_model, request_rate, RUNNING_TIME_SEC)\n",
    "    times.insert(0, \"request_rate\", request_rate)\n",
    "    to_concat.append(times)\n",
    "\n",
    "results = pd.concat(to_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96396d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results[\"request_rate\"] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41097a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = results.groupby(\"request_rate\").aggregate({\"latency\": [\"mean\", \"median\", \"max\"]})\n",
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9496843",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(agg_results.index, agg_results[\"latency\", \"mean\"])\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Average Latency (sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d457420",
   "metadata": {},
   "source": [
    "## Using zero-copy model loading\n",
    "\n",
    "Now let's redo this baseline using zero-copy model loading.\n",
    "First we'll need to convert the model into a format that can be loaded without copying\n",
    "data. The model is actually a pipeline of multiple operations, but the RoBERTa model\n",
    "at its center is orders of magnitude larger and more CPU-intensive than everything else, so we'll only apply zero-copy loading to that part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6565396",
   "metadata": {},
   "source": [
    "## Introducing `zerocopy`\n",
    "\n",
    "We've created a Python package, `zerocopy`, with the model rewrite code from our previous post (TODO: Publish the package to PyPI).\n",
    "\n",
    "To use that package, you'll need to install it with `pip`, then import it into your script.\n",
    "\n",
    "```python\n",
    "import zerocopy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this code to the `zerocopy` library.\n",
    "@ray.remote\n",
    "def call_model_zero_copy(model_ref: ray.ObjectRef, args, kwargs) -> Any:\n",
    "    \"\"\"\n",
    "    Ray task that uses zero-copy model loading to reconstitute a model\n",
    "    from Plasma, then invokes the model's ``__call__()`` method.\n",
    "\n",
    "    :param model_ref: Object reference to a tuple of model skeleton\n",
    "     and model weights, as returned by :func:`extract_tensors`\n",
    "    :param args: Ordered arguments to pass to the model's :func:`__call__`\n",
    "     method\n",
    "    :param kwargs: Keyword arguments to pass to the model's :func:`__call__`\n",
    "     method\n",
    "\n",
    "    :returns: Return value from the model's :func:`__call__` method\n",
    "    \"\"\"\n",
    "    # Suppress PyTorch warnings about immutable tensors\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    model_skeleton, model_weights = model_ref\n",
    "    zerocopy.replace_tensors(model_skeleton, model_weights)\n",
    "    with torch.no_grad():\n",
    "        return model_skeleton(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7941cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model directly\n",
    "inputs = qa.tokenizer(qa_input[\"question\"], qa_input[\"context\"], return_tensors=\"pt\")\n",
    "qa.model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model via `call_model`. Results should be the same as the previous cell.\n",
    "model_ref = ray.put(zerocopy.extract_tensors(qa.model))\n",
    "ray.get(call_model_zero_copy.remote(model_ref, [], inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae691c",
   "metadata": {},
   "source": [
    "The time to invoke the model once via `call_model_zero_copy()` is almost the same as running the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2416e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare timings\n",
    "print(\"       Time to run locally: \", end=\"\")\n",
    "%timeit qa.model(**inputs)\n",
    "print(\"Time to run with zero-copy: \", end=\"\")\n",
    "%timeit ray.get(call_model_zero_copy.remote(model_ref, [], inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccc91c",
   "metadata": {},
   "source": [
    "If we run inference multiple times, `call_model_zero_copy()` can send those inference requests to separate Ray tasks that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local(num_repeats: int):\n",
    "    for _ in range(num_repeats):\n",
    "        qa.model(**inputs)\n",
    "\n",
    "\n",
    "def run_zero_copy(num_repeats: int):\n",
    "    futures = [call_model_zero_copy.remote(model_ref, [], inputs) for _ in range(num_repeats)]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "NUM_REPEATS = 100\n",
    "print(f\"       Time to run {NUM_REPEATS} times locally: \", end=\"\")\n",
    "%timeit -r 3 run_local(NUM_REPEATS)\n",
    "print(f\"Time to run {NUM_REPEATS} times with zero-copy: \", end=\"\")\n",
    "%timeit -r 3 run_zero_copy(NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43922b",
   "metadata": {},
   "source": [
    "Now let's define a Ray Serve endpoint that runs the model preprocessing code locally and farms out model inference \n",
    "to Ray tasks that use zero-copy model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b57958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroCopyQAModel:\n",
    "    def __init__(self):\n",
    "        # TODO: Move this rewrite to the `zerocopy` library.\n",
    "        # Load the entire pipeline, then copy the model portion to Plasma.\n",
    "        self._qa = transformers.pipeline(\"question-answering\", model=model_name)\n",
    "        model_ref = ray.put(zerocopy.extract_tensors(self._qa.model))\n",
    "\n",
    "        # Replace the pipeline's model with a callback that farms out work to\n",
    "        # Ray tasks.\n",
    "        class _ModelCallback:\n",
    "            def __call__(self, *args, **kwargs):\n",
    "                return ray.get(call_model_zero_copy.remote(model_ref, args, kwargs))\n",
    "        self._qa.model = _ModelCallback()\n",
    "\n",
    "        # Use a threadpool because the model is called from pre/postprocessing code\n",
    "        # that is not asyncio-aware\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        # Pull model inputs from URL query parameters.\n",
    "        # A production version of this code would sanitize these strings.\n",
    "        model_input = {\n",
    "            \"question\": request.query_params[\"question\"],\n",
    "            \"context\": request.query_params[\"context\"]\n",
    "        }\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "            self._threadpool, lambda: self._qa(model_input))\n",
    "        return result\n",
    "\n",
    "    def __del__(self):  # Ray Serve needs this callback\n",
    "        pass\n",
    "\n",
    "\n",
    "# Define endpoints\n",
    "NUM_QA_MODELS = 12\n",
    "deployments = [\n",
    "    serve.deployment(ZeroCopyQAModel, f\"qa{model_num}\",\n",
    "                     ray_actor_options={\"num_cpus\": 0.1})\n",
    "    for model_num in range(NUM_QA_MODELS)\n",
    "]\n",
    "\n",
    "for d in deployments:\n",
    "    d.deploy(_blocking=False)\n",
    "\n",
    "# Wait a moment so log output doesn't go to the next cell's output\n",
    "time.sleep(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the new deployment.\n",
    "# This web service call blocks until the asychronous deployment has completed.\n",
    "params = urllib.parse.urlencode(qa_input)\n",
    "requests.get(f\"http://127.0.0.1:8000/qa0?{params}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c04338",
   "metadata": {},
   "source": [
    "We've deployed these models to the same URLs, so the benchmark code from before should work without\n",
    "any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test run\n",
    "run_benchmark(call_model, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark at multiple different request rates\n",
    "to_concat = []\n",
    "for request_rate in REQUEST_RATES:\n",
    "    print(f\"Running at {request_rate} requests/sec.\")\n",
    "    times = run_benchmark(call_model, request_rate, RUNNING_TIME_SEC)\n",
    "    times.insert(0, \"request_rate\", request_rate)\n",
    "    to_concat.append(times)\n",
    "\n",
    "results_zerocopy = pd.concat(to_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f129d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results_zerocopy = results_zerocopy.groupby(\"request_rate\").aggregate({\n",
    "    \"latency\": [\"mean\", \"median\", \"max\"]})\n",
    "agg_results_zerocopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two sets of results against each other.\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(agg_results.index, agg_results[\"latency\", \"mean\"],\n",
    "            label=\"baseline\")\n",
    "plt.scatter(agg_results_zerocopy.index, \n",
    "            agg_results_zerocopy[\"latency\", \"mean\"],\n",
    "            label=\"zero-copy\")\n",
    "plt.xlabel(\"Average Requests per Second\")\n",
    "plt.ylabel(\"Average Latency (sec)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac27d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code defined an actor\n",
    "\n",
    "# class ModelCallback:\n",
    "#     def __init__(self, model_ref: ray.ObjectRef):\n",
    "#         self._model_ref = model_ref\n",
    "\n",
    "#     def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "#         return ray.get(call_model.remote(self._model_ref, args, kwargs))\n",
    "\n",
    "# @ray.remote\n",
    "# class QAModelZeroCopyActor:\n",
    "#     def __init__(self):\n",
    "#         self._qa = transformers.pipeline(\"question-answering\", model=model_name)\n",
    "#         self._model_ref = ray.put(zerocopy.extract_tensors(self._qa.model))\n",
    "#         self._qa.model = ModelCallback(self._model_ref)\n",
    "\n",
    "#     def run_inference(self, input_: Dict[str, str]) -> Dict[str, Any]:\n",
    "#         return self._qa(input_)\n",
    "\n",
    "# zero_copy_actors = [QAModelZeroCopyActor.options(max_concurrency=8).remote() \n",
    "#                     for _ in range(NUM_QA_MODELS)]\n",
    "# ray.get(zero_copy_actors[0].run_inference.remote(qa_input))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afa7e0f34d224467fd24b0cfa9c212efa127bdf53fe1c4e3ddf54198f34a39e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
